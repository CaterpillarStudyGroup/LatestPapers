RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping


生成模型的最新进展已彻底革新了视频合成与编辑领域。然而，高质量多样化数据集的稀缺性，依然严重阻碍着视频条件化机器人学习的进展，限制了跨平台的泛化能力。在本研究中，我们致力于解决一个关键挑战：将一段视频中的机器人手臂替换为另一段视频中的手臂——这是跨具身学习(crossembodiment learning)的关键一步。与以往依赖相同环境设置下成对视频演示的方法不同，我们提出的框架 **RoboSwap** 能够在来自不同环境的无配对数据上运行，从而显著缓解数据收集的需求。

RoboSwap 引入了一种新颖的视频编辑流程，融合了生成对抗网络(GANs)和扩散模型(diffusion models)各自的优势。具体而言，我们首先将机器人手臂从背景中分割出来，然后训练一个基于无配对数据的 GAN 模型，用于将一种机器人手臂转换(翻译)成另一种。转换后的手臂与原始视频背景进行融合，并经过扩散模型的精细化处理，以增强视频的连贯性、运动真实感以及与物体交互的自然度。GAN 和扩散模型阶段采用分阶段独立训练。

我们的实验表明，在三个基准测试上，RoboSwap 在结构连贯性和运动一致性方面均优于最先进的视频和图像编辑模型，从而为在机器人学习中生成可靠的跨具身数据提供了一个强大的解决方案。
