PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model

音频驱动的人体动画技术在人机交互领域得到广泛应用，而扩散模型的出现进一步推动了该技术的发展。当前大多数方法依赖多阶段生成和中间表示，导致推理时间较长，并在特定前景区域和音频-动作一致性方面存在生成质量问题。这些缺陷主要源于缺乏局部细粒度监督指导。为解决上述挑战，我们提出了PAHA——首个端到端的扩散模型驱动的音频驱动上半身人体动画框架。我们引入两个核心方法：部件感知重加权(PAR)和部件一致性增强(PCE)。PAR基于姿态置信度分数动态调整区域训练损失权重，有效提升视觉质量；PCE通过构建并训练基于扩散模型的区域视听分类器，改善动作与伴随语音的一致性。我们进一步为前述分类器设计了两种创新推理指导方法——序列指导(SG)和差分指导(DG)，分别实现效率与质量的平衡。此外，我们构建了首个公开的中文新闻主播语音数据集CNAS，以推动该领域的研究与验证。大量实验结果和用户研究表明，PAHA在音频-动作对齐和视频相关评估指标上显著优于现有方法。代码和CNAS数据集将在论文录用后开源。   