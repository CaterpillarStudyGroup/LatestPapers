M3G: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human Motion Synthesis

在虚拟化身创建中，从音频生成包含面部、身体、手部和整体运动的全身人体姿态是一项重要但具有挑战性的任务。以往的系统主要关注于逐帧对人体姿态进行标记化处理，并通过输入音频预测每帧的标记。然而，我们发现一个现象：一个完整的富有表现力的人体姿态所需的帧数(定义为粒度)在不同姿态模式中存在差异。由于现有系统的姿态标记采用固定粒度，它们无法有效建模这些多样化的姿态模式。

为解决这一问题，我们提出了一种名为多粒度姿态生成器(Multi-Granular Gesture Generator, M3G)的创新框架，用于音频驱动的全身姿态生成。在M3G中，我们首先提出新型多粒度向量量化变分自编码器(Multi-Granular VQ-VAE, MGVQ-VAE)，用于从不同时间粒度对运动模式进行标记化处理并重建运动序列。随后，我们设计了多粒度标记预测器，能够从音频中提取多粒度信息并预测相应的运动标记。最终，M3G通过MGVQ-VAE从预测标记中重建人体姿态。主客观实验结果表明，我们提出的M3G框架在生成自然且富有表现力的全身人体姿态方面优于现有最先进方法。