Move as You Say, Interact as You Can:Language-guided Human Motion Generation with Scene Affordance

尽管文本到动作合成技术取得了显著进展，但在3D环境中生成语言引导的人体动作仍面临重大挑战。这些挑战主要源于两个核心问题：(i)目前缺乏能够联合建模自然语言、3D场景和人体动作的强效生成模型；(ii)生成模型对数据的高度需求与当前缺乏全面、高质量的语言-场景-动作数据集之间的矛盾。为解决这些问题，我们提出了一种创新的两阶段框架，通过引入场景功能（scene affordance）作为中间表征，有效桥接了3D场景基础与条件动作生成。该框架包含两个核心组件：用于预测显式功能图的功能扩散模型（ADM），以及用于生成合理人体动作的功能到动作扩散模型（AMDM）。通过场景功能图的中间转换，我们的方法成功解决了在多模态条件信号下生成人体动作的难题，尤其在缺乏大量语言-场景-动作配对数据的有限训练条件下表现优异。大量实验表明，我们的方法在HumanML3D和HUMANISE等基准测试中持续优于所有基线模型。此外，我们通过专门构建的评估集验证了模型的卓越泛化能力，该评估集包含模型训练时未接触过的新描述和新场景。