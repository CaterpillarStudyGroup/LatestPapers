Synthesizing Diverse Human Motions in 3D Indoor Scenes

我们提出了一种在三维室内场景中生成虚拟人类的新方法，这些虚拟人能够在环境中自主导航并以逼真的方式与物体互动。现有方法依赖于包含捕捉人体动作和对应交互三维场景的高质量训练序列，然而这类交互数据获取成本高昂、采集困难，且难以覆盖复杂室内环境中所有合理的人-场景交互可能性。为解决这些挑战，我们提出了一种基于强化学习的方法，通过习得的运动控制策略驱动虚拟人在三维场景中实现自主导航和逼真的物体交互。该运动控制策略采用隐式运动动作空间，这些空间对应真实的运动基元，并通过强大的生成式运动模型从大规模动作捕捉数据中学习获得。

针对三维环境中的导航问题，我们设计了一种场景感知策略，其创新性的状态和奖励机制能有效规避碰撞。结合基于导航网格的路径规划算法生成中间航点，我们的方法能够合成虚拟人在三维室内场景中多样化避障导航的运动轨迹。为实现细粒度的人-物体交互，我们采用基于标记点的身体表征精心设计交互目标指引，并利用基于有向距离场（SDF）的特征编码人-场景邻近关系。即使面对物体形状、朝向、初始身体位置和姿态各异的分布外测试场景，我们的方法仍能合成逼真多样的人-物体交互（如坐在椅子上后起身）。实验结果表明，该方法在运动自然性和多样性方面均优于现有最先进的人-场景交互合成方法。相关代码、模型及演示视频已开源发布于：<https://zkf1997.github.io/DIMOS>。