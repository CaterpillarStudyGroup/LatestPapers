Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose Interaction   


**视频虚拟试穿技术**旨在将特定服装无缝贴合到视频人物的身体上。其核心挑战在于保持服装视觉真实性的同时，动态适配人物的姿态与体型。现有方法主要聚焦于基于图像的虚拟试穿，若直接应用于视频常导致时序不一致问题。当前多数视频虚拟试穿方案通过引入时序模块缓解此问题，但仍忽视人物与服装间关键的时空姿态交互关系。   

有效的视频姿态交互需同时兼顾两方面：单帧内人物与服装姿态的空间对齐，以及整个视频中人体姿态的时序动态变化。基于此动机，我们提出**动态姿态交互扩散模型（DPIDM）** 这一新型框架，利用扩散模型深入研究视频虚拟试穿中的动态姿态交互。   

技术实现上，DPIDM创新性地：   
1. 采用**基于骨骼的姿态适配器**，将同步化处理的人物/服装姿态输入去噪网络    
2. 设计**分层注意力模块**，通过姿态感知的：    
   - 空间注意力机制建模帧内人-衣姿态交互    
   - 时序注意力机制捕捉跨帧的长期人体姿态动态    
3. 引入**时序正则化注意力损失函数**，在连续帧间增强时序一致性    

在VITON-HD、VVT和ViViD数据集上的大量实验证明DPIDM的优越性。特别值得注意的是，在VVT数据集上DPIDM达到0.506的VFID分数，较当前最优方法GPD-VVTO实现60.5%的性能提升。   
