Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for Human Motion Generation


大规模语言模型的最新进展，尤其在自然语言理解与推理方面的突破，为文本驱动动作生成开辟了新可能。尽管现有方法在语义对齐与动作合成领域取得显著进展，但其依赖的端到端映射策略往往难以捕捉深层语言结构与逻辑推理。这导致生成的动作常缺乏可控性、连贯性与多样性。   

为突破这些局限，我们提出Motion-R1——一种融合思维链机制的统一动作-语言建模框架。该框架通过将复杂文本指令显式分解为逻辑化动作路径，为动作生成提供高层语义指导，显著增强模型对多步骤、长时程及组合式复杂指令的解析与执行能力。   

在模型训练中，我们采用为大模型设计的强化学习算法"群体相对策略优化"，通过动作质量反馈联合优化推理链与动作合成。在多个基准数据集上的实验表明：Motion-R1在需要细致语义理解与长期时序连贯性的场景中，性能达到或超越现有最优方法。代码、模型及数据将全面公开。  