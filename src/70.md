DiffLocks: Generating 3D Hair from a Single Image using Diffusion Models

我们致力于解决从单张图像生成三维头发几何结构的任务，这一任务由于发型多样性及缺乏成对的图像-三维头发数据而极具挑战性。先前的方法主要依赖合成数据进行训练，并通过使用低维中间表征(如引导发丝和头皮层级嵌入)应对数据量不足的问题，但这些表征需要后处理步骤进行解码、上采样和真实性增强。这些方法无法重建细节丰富的发型，在处理卷发时效果欠佳，或仅能处理有限数量的发型。

为突破这些限制，我们提出了DiffLocks这一创新框架，能够直接从单张图像实现多种复杂发型的细节重建。首先，我们通过自动化流程创建了迄今为止最大的合成头发数据集(包含4万种发型)，解决了三维头发数据匮乏的问题。其次，我们利用该合成数据集训练了一个图像条件化的扩散变换器模型，使其能够通过单张正面图像生成精确的三维发丝。得益于预训练的图像主干网络，我们的方法虽然仅使用合成数据训练，却能泛化至真实场景图像。该扩散模型预测的头皮纹理图中，任意位置均包含单根发丝的潜在编码。这些编码可直接解码为三维发丝，无需任何后处理技术。

通过直接表征单根发丝而非引导发丝，变换器能够建模复杂发型的精细空间结构。基于此，DiffLocks首次实现了从单张图像重建高度卷曲发型(如非洲式发型)的能力。相关数据和代码已发布于<https://radualexandru.github.io/difflocks/>。

