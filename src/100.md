Video-GPT via Next Clip Diffusion

GPT在自然语言处理领域展现了非凡成就。然而，语言序列难以充分描述视觉世界中的时空细节，而视频序列恰好擅长捕捉此类信息。基于此观察，本文提出了一种简洁的Video-GPT框架，将视频视为描述视觉世界的新型"语言"。受GPT中下一词元预测的启发，我们针对视频GPT的预训练提出了新颖的下一片段扩散范式。与现有方法不同，这种独特范式通过基于历史干净片段对噪声片段进行自回归去噪，使Video-GPT能同时处理短期生成与长期预测任务。大量实验表明，我们的Video-GPT在视频预测任务上达到了最先进性能(Physics-IQ基准测试：Video-GPT 34.97 vs. Kling 23.64 vs. Wan 20.89)，这正是实现世界建模的关键指标。此外，该模型能良好适配视频生成与理解领域的6个主流任务，展现出优异的下游泛化能力。项目主页请访问<https://Video-GPT.github.io>。   