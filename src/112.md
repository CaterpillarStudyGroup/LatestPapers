MEgoHand: Multimodal Egocentric Hand-Object Interaction Motion Generation


**以自我为中心的手-物体运动生成**对于沉浸式AR/VR和机器人模仿至关重要，但由于不稳定的视角、自我遮挡、透视畸变以及嘈杂的自身运动，这仍然是一项挑战。现有方法依赖于预定义的3D物体先验知识，限制了对新物体的泛化能力（这限制了它们对新物体的泛化能力）。同时，近期的多模态方法存在以下问题：从抽象文本提示生成结果模糊不清、用于建模3D手-物体关联的流程过于复杂、以及在开环预测中误差不断累积。   

我们提出了**MEgoHand**，一个多模态框架，能够从以自我为中心的RGB图像、文本和初始手部姿态合成物理合理的手-物体交互动作。MEgoHand引入了一个**双层架构**：高层“大脑”（cerebrum）利用**视觉语言模型（VLM）**从视觉-文本上下文中推断运动先验知识，并使用**单目深度估计器**进行与物体无关的空间推理；而低层则基于**DiT（扩散变换器）的流匹配策略**生成细粒度轨迹，并采用**时序正交滤波**来增强稳定性。    

为了解决数据集不一致的问题，我们设计了一个**数据集构建范式**，包含一个**逆向MANO重定向网络**和一个**虚拟RGB-D渲染器**，构建了一个包含335万帧RGB-D图像、2.4万次交互和1200个物体的统一数据集。在**五个域内数据集和两个跨域数据集**上进行的大量实验证明了MEgoHand的有效性，实现了手腕平移误差（降低86.9%）和关节旋转误差（降低34.1%）的大幅减少，突显了其精确建模细粒度手部关节结构以及鲁棒泛化的能力。    