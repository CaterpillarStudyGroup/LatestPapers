ELGAR: Expressive Cello Performance Motion Generation for Audio Rendition

乐器演奏艺术是人类创造力与情感的生动体现。然而，生成乐器演奏动作是一项极具挑战性的任务，因其不仅需要捕捉复杂肢体运动，还需重构演奏者与乐器之间相互作用的动态关系。现有研究主要集中于局部肢体运动的建模，为此我们提出ELGAR(基于音频的富有表现力大提琴演奏动作生成系统)，这是一种基于扩散模型的先进框架，能够仅凭音频信号即可生成全身细粒度的乐器演奏动作。为强调乐器演奏的交互本质，我们创新性地引入手部交互接触损失(HICL)和琴弓交互接触损失(BICL)，有效保证了交互过程的真实性。此外，为更好评估生成动作与音乐音频语义的契合度，我们专门针对弦乐器演奏动作生成设计了新型评价指标，包括指板接触距离、弓弦距离及运弓评分。通过大量评估与消融实验验证了所提方法的有效性。同时，我们构建了动作生成数据集SPD-GEN，该数据集整理并规范化自动作捕捉数据集SPD。实验表明，ELGAR在生成具有复杂快速交互特征的乐器演奏动作方面展现出巨大潜力，将有力推动动画制作、音乐教育、交互艺术创作等领域的进一步发展。
