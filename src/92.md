MoCLIP: Motion-Aware Fine-Tuning and Distillation of CLIP for Human Motion Generation

人体运动生成在动画、机器人学和虚拟现实等领域具有关键作用，这类任务需要模型能够有效捕捉文本描述中的运动动力学特征。现有方法通常基于对比语言-图像预训练(CLIP)文本编码器，但由于其训练数据仅限于文本-图像对，导致模型难以理解运动本身及运动生成过程中固有的时间动态和运动学结构。为此，我们提出MoCLIP —— 通过添加运动编码头并对运动序列进行对比学习与束缚损失训练而实现的CLIP微调模型。该模型通过显式整合运动感知表征，在保持与现有CLIP流程兼容性的同时显著提升了运动保真度，并能无缝集成到各类基于CLIP的方法中。实验表明，MoCLIP在保持竞争力FID分数的同时，显著提高了Top-1、Top-2和Top-3准确率，从而实现了更优的文本-运动对齐效果。这些成果印证了MoCLIP的多功能性和有效性，使其成为增强运动生成任务的强健框架。  

