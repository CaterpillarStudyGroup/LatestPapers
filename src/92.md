MoCLIP: Motion-Aware Fine-Tuning and Distillation of CLIP for Human Motion Generation

人体运动生成对于动画、机器人学和虚拟现实等领域至关重要，这要求模型能够有效捕捉文本描述中的运动动态。现有方法通常依赖基于对比语言-图像预训练(CLIP)的文本编码器，但其在文本-图像对上的训练限制了模型对运动及运动生成所固有的时序和运动学结构的理解。本研究提出MoCLIP —— 一种经过微调的CLIP模型，通过添加运动编码头，采用对比学习和约束损失在运动序列上进行训练。通过显式整合运动感知表征，MoCLIP在保持与现有CLIP框架兼容性的同时增强了运动保真度，并能无缝集成到各类基于CLIP的方法中。实验表明，MoCLIP在保持具有竞争力的FID分数基础上，显著提升了Top-1、Top-2和Top-3准确率，从而改善了文本到运动的对齐效果。这些成果彰显了MoCLIP的通用性和有效性，确立了其作为增强运动生成的稳健框架地位。
 

