Efficient Listener: Dyadic Facial Motion Synthesis via Action Diffusion

在双向对话中生成逼真的听者面部动作仍然具有挑战性，这主要源于高维动作空间和时序依赖性的要求。现有方法通常考虑提取3D可变形模型(3DMM)系数并在3DMM空间进行建模。然而，这种做法使得3DMM的计算速度成为性能瓶颈，难以实现实时交互响应。为解决这一问题，我们提出了面部动作扩散方法(Facial Action Diffusion, FAD)，该方法将图像生成领域的扩散方法引入到面部动作生成中，以实现高效的面部动作生成。我们进一步构建了专为接收说话者视觉和听觉信息的双模态输入而设计的高效听者网络(Efficient Listener Network, ELNet)。结合FAD和ELNet，所提出的方法能有效学习听者面部运动表征，在性能上超越了现有最优方法的同时，将计算时间减少了99%。