ReactDance: Progressive-Granular Representation for Long-Term Coherent Reactive Dance Generation

反应式舞蹈生成(Reactive Dance Generation, RDG)通过结合引导舞者动作和音乐输入来生成跟随者动作，同时确保空间协调性和时间连贯性。然而现有方法过度强调全局约束和优化，忽视了细粒度空间交互、局部时间上下文等局部信息。为此，我们提出了ReactDance——一个基于扩散模型的新型框架，能够实现具有长期连贯性和多尺度可控性的高保真RDG。针对双人舞合成中存在的交互保真度、同步性和时间一致性等挑战，我们提出两大核心创新：(1)组残差有限标量量化(GRFSQ)，这种多尺度解耦动作表征技术能够从粗粒度的身体律动到细粒度的关节动力学全面捕捉交互语义；(2)分块局部上下文(BLC)，该采样策略通过局部块因果掩码和周期性位置编码，有效消除长序列生成中的误差累积。基于解耦的多尺度GRFSQ表征，我们构建了配备层解耦无分类器指导(LDCFG)的扩散模型，实现跨尺度运动语义的精细化控制。在标准基准上的大量实验表明，ReactDance超越了现有方法，达到了最先进的性能水平。  

