M4V: Multi-Modal Mamba for Text-to-Video Generation


文本到视频生成技术显著丰富了内容创作，并具备发展为强大世界模拟器的潜力。然而，对广阔时空空间进行建模在计算上仍然要求很高，尤其是在使用 Transformer 架构时——其序列处理存在二次方复杂度，因此限制了实际应用。    

近期在线性时间序列建模方面的进展，特别是 Mamba 架构，提供了一种更高效的替代方案。然而，其朴素的设计限制了其在多模态和时空视频生成任务中的直接适用性。   

为了应对这些挑战，我们提出了 **M4V**，一个用于文本到视频生成的**多模态 Mamba 框架**。具体来说，我们设计了一个**多模态扩散 Mamba (MM-DiM) 模块**，通过**多模态令牌重组设计**，能够无缝整合多模态信息并进行时空建模。   

因此，在生成 768×1280 分辨率视频时，M4V 中的 Mamba 模块相比基于注意力机制的方案，将浮点运算量 (FLOPs) 降低了 45%。此外，为了缓解长上下文自回归生成过程中出现的视觉质量下降问题，我们引入了一种**奖励学习策略**，进一步提升了逐帧的视觉真实感。   

在文本到视频基准测试上的大量实验表明，M4V 能够在显著降低计算成本的同时生成高质量视频。代码和模型将在 <https://huangjch526.github.io/M4V_project> 公开提供。   