AlignHuman: Improving Motion and Fidelity via Timestep-Segment Preference Optimization for Audio-Driven Human Animation

在扩散模型（Diffusion Models）的推动下，人类视频生成与动画任务近期取得了显著进展。然而，由于**自然动作（motion naturalness）**与**视觉保真度（visual fidelity）**之间的权衡问题，生成富有表现力且逼真的人类动画仍然具有挑战性。   

为了解决这一问题，我们提出了 **AlignHuman** 框架。该框架将**偏好优化（Preference Optimization）**作为一种训练后精调技术，并结合**分治训练策略（divide-and-conquer training strategy）**，以协同优化这两个相互竞争的目标。   

我们的核心洞察源于对跨时间步去噪过程的分析：(1) **早期去噪时间步（early denoising timesteps）**主要控制运动动态（motion dynamics），而 (2) **保真度和人体结构（fidelity and human structure）**即使在跳过早期步骤的情况下，也能由**后期时间步（later timesteps）**有效管理。   

基于这一观察，我们提出了**分时段偏好优化（Timestep-segment Preference Optimization, TPO）**，并引入了两个专门的**LoRA模块（Low-Rank Adaptation）**作为专家对齐模块。每个 LoRA 模块专注于其对应时间步区间内的特定优化维度（运动自然度或保真度）。这些 LoRA 模块使用各自对应的偏好数据进行训练，并在推理过程中在相应的时间步区间内激活，分别用于增强运动自然度和视觉保真度。   

大量实验表明，AlignHuman 显著提升了多个强基线模型的性能，并在推理时减少了**去噪步数（Number of Function Evaluations, NFEs）**，实现了 **3.3× 的加速**（从 100 NFEs 减少到 30 NFEs），同时对生成质量的影响微乎其微。    

项目主页：<https://alignhuman.github.io/>   
