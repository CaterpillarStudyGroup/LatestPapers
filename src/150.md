HOSIG: Full-Body Human-Object-Scene Interaction Generation with Hierarchical Scene Perception


在计算机图形学与动画领域，如何生成高保真、涉及动态物体和静态场景的全身人体交互，仍然是一个关键挑战。现有的人-物交互方法常常忽略场景上下文，导致不合理的穿模现象；而人-场景交互方法则难以协调精细的操作与长距离导航。为克服这些局限，我们提出了 HOSIG —— 一种通过**分层场景感知**合成全身交互的新型框架。   

我们的方法将任务解耦为三个关键组件：   
1.  **场景感知抓取姿态生成器**：通过整合局部几何约束，确保生成无碰撞的全身姿态，并实现精确的手-物接触；   
2.  **启发式导航算法**：利用压缩的二维平面图(2D floor maps)和双组件空间推理，在复杂室内环境中自主规划避障路径；   
3.  **场景引导的运动扩散模型**：通过融入空间锚点和双空间无分类器引导(dual-space classifier-free guidance)，生成具有轨迹控制、手指级精度的全身运动。   

在 TRUMANS 数据集上进行的大量实验表明，其性能优于当前最先进(state-of-the-art)的方法。值得注意的是，我们的框架通过自回归生成支持无限长度的运动，并且只需极少的人工干预。这项工作弥合了场景感知导航与灵巧物体操控之间的关键鸿沟，推动了具身交互合成的前沿发展。代码将在论文发表后开源。项目主页：<http://yw0208.github.io/hosig>   