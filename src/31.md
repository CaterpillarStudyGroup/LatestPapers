CoCoDiff: Diversifying Skeleton Action Features via Coarse-Fine Text-Co-Guided Latent Diffusion

在动作识别任务中，特征多样性对提升模型泛化能力和性能至关重要。现有方法通常通过在样本空间中扩充训练数据来增强特征多样性，但这种方式往往效率低下且存在语义不一致问题。为克服这些局限，我们提出了一种新颖的粗细粒度文本协同引导扩散模型(CoCoDiff)。该模型通过扩散过程和多粒度文本引导，在潜在空间中生成多样化且语义一致的特征表示。具体而言，我们的方法将骨骼序列提取的时空特征输入潜在扩散模型，生成多样化的动作表征。同时，我们创新性地引入粗细粒度文本协同引导策略，利用大语言模型(LLMs)提供的文本信息，确保生成特征与原始输入的语义一致性。值得注意的是，CoCoDiff在训练过程中作为即插即用辅助模块运行，无需增加推理阶段的计算开销。大量实验表明，CoCoDiff在基于骨骼数据的动作识别基准测试(包括NTU RGB+D、NTU RGB+D 120和Kinetics-Skeleton)中取得了最先进的性能表现。