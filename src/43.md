Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion

从人类语音生成手势在虚拟化身动画领域取得了巨大进展。现有方法虽然能够合成单人独白场景下的协同手势，却忽视了双人交互对话场景下同步手势建模的实用性。此外，缺乏高质量的同步伴随语音手势数据集也限制了这一问题的解决。为实现这一目标，我们首先构建了大规模同步伴随语音手势数据集GES-Inter，包含超过700万帧多样化的双人交互姿势序列。在此基础上，我们提出新型框架Co³Gesture，能够生成包含双人交互动作的连贯同步伴随语音手势。考虑到对话双方身体动态的非对称性，我们的框架基于两个以分离说话者音频为条件的协作生成分支构建。具体而言，为增强对话过程中人体姿势与对应说话者语音的协调性，我们设计了时序交互模块（TIM）。该模块能够有效建模双说话者手势序列间的时序关联表征作为交互指导，并将其融入同步手势生成。此外，我们开发了互注意力机制，从整体上增强交互同步动作的依赖关系学习，从而实现生动连贯的手势生成。大量实验表明，我们的方法在新构建的GES-Inter数据集上优于当前最先进模型。数据集与源代码已发布于<\href{https://mattie-e.github.io/Co3/}{\textit{https://mattie-e.github.io/Co3/}}。>

