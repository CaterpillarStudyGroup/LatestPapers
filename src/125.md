Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals


近期视频生成模型的进展激发了对能够模拟逼真环境的世界模型的兴趣。虽然导航功能已被充分探索，但模仿真实世界力量的、具有物理意义的交互在很大程度上仍未得到充分研究。    

在这项工作中，我们研究了将物理力作为视频生成的控制信号，并提出了“力提示”。这使得用户能够通过局部点力（例如戳植物）和全局风力场（例如风吹动布料）与图像进行交互。我们证明，通过利用原始预训练模型中的视觉和运动先验，这些力提示能够使视频对物理控制信号做出逼真的响应，并且在推理过程中无需使用任何3D资源或物理模拟器。    

力提示的主要挑战在于难以获得高质量配对的力-视频训练数据。这在现实世界中是由于获取力信号的困难，而在合成数据中则是由于物理模拟器在视觉质量和领域多样性方面的局限性。    

**我们的关键发现是：当视频生成模型经过调整以适应Blender合成视频中的物理力条件时，即使仅使用少量物体的有限演示样本，它们也能表现出卓越的泛化能力。我们的方法可以生成模拟各种几何形状、场景和材质受力的视频。**    

**我们还试图理解这种泛化能力的来源，并通过消融实验揭示了两个关键要素：视觉多样性以及在训练过程中使用特定的文本关键词。**   

**我们的方法仅在约1.5万个训练样本上，使用四块A100 GPU训练一天，就在力遵循度和物理真实性方面超越了现有方法，使世界模型更接近真实世界的物理交互。**

**我们在项目主页上发布了所有数据集、代码、权重和交互式视频演示。**   