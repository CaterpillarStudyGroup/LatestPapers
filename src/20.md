Joint Optimization of Neural Radiance Fields and Continuous Camera Motion from a Monocular Video

神经辐射场(Neural Radiance Fields, NeRF)已展现出其在三维几何表征方面的卓越能力，但其训练过程依赖于预先精确计算的相机位姿。为了降低这一限制，现有方法通常通过联合优化相机位姿与NeRF来实现，但往往需要良好的位姿初始化或深度先验。然而，由于这些方法将每个相机映射至统一的世界坐标系，在面对大幅旋转等挑战性场景时表现受限。本文提出了一种创新方法，通过将连续相机运动建模为时间依赖的角速度与线速度，从而消除对先验条件的依赖。我们首先通过学习相机间的相对运动(通过速度积分实现)，随后通过聚合这些相对运动至视频某一时间步定义的世界坐标系中，最终获得相机位姿。具体而言，我们通过时间依赖的NeRF模型学习精确的连续相机运动——该模型通过在每个时间步利用相邻帧进行训练，以捕捉局部场景几何与运动特征。学习到的运动信息进一步用于微调NeRF，使其能够表征完整场景几何结构。在Co3D和Scannet数据集上的实验表明，相较于现有最优方法，我们的方法在相机位姿估计、深度估计精度方面表现更优，同时在新视角合成任务中达到可比性能。代码已开源：<https://github.com/HoangChuongNguyen/cope-nerf>   