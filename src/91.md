Towards Robust and Controllable Text-to-Motion via Masked Autoregressive Diffusion

从文本描述生成3D人体运动仍然面临挑战，这源于人体运动本身的多样性和复杂性。现有方法虽然在训练数据分布范围内表现优异，但面对分布外运动时往往效果不佳，限制了其在真实场景中的应用。基于VQVAE的方法难以通过离散标记忠实地表征新运动，这阻碍了其对未见过数据的泛化能力。而基于扩散模型的方法虽然处理连续表示，但通常缺乏对单帧的细粒度控制。为应对这些挑战，我们提出了鲁棒的运动生成框架MoMADiff，通过将掩码建模与扩散过程相结合，利用帧级连续表示来生成运动。该模型支持用户灵活指定关键帧，能够精确控制运动合成的空间和时间维度。MoMADiff在新型文本到运动数据集上展现出强大的泛化能力，能够以稀疏关键帧作为运动提示。通过在两个保留测试集和两个标准基准上的大量实验表明，我们的方法在运动质量、指令保真度和关键帧遵循度方面持续优于当前最先进的模型。

