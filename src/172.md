Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos


**铰接物体**在日常生活中十分普遍。理解它们的**运动学结构**并对其进行重建，在**具身人工智能**和**机器人学**领域具有众多应用。然而，当前的方法需要精心采集的数据进行训练或推断，这阻碍了对铰接物体进行实用、可扩展且泛化性强的重建。   
  
我们专注于如何利用**手持摄像机随手拍摄的一段RGBD视频**来重建一个铰接物体。通过智能手机，可以轻松地大规模获取人与铰接物体交互的随手拍摄视频。然而，这种场景极具挑战性：物体和摄像机同时移动，并且当人与物体交互时会产生**显著的遮挡**。   

为应对这些挑战，我们提出了一个**由粗到精的框架**，该框架能够从动态的RGBD视频中推断出物体的**关节参数**并**分割其可移动部件**。为了在这种新场景下评估我们的方法，我们构建了一个**20倍**于现有规模的大型合成数据集，包含**784个视频**、**284个物体**，涵盖**11个类别**。我们将我们的方法与同样以视频作为输入的现有方法进行了比较。实验表明，我们的方法能够从动态RGBD视频中重建不同类别的合成及真实铰接物体，**性能显著优于**现有方法。   
