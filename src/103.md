Vid2World: Crafting Video Diffusion Models to Interactive World Models

世界模型能够根据历史观察和动作序列预测状态转移，在提升序列决策的数据效率方面展现出巨大潜力。然而，现有世界模型通常需要大量领域特定训练，且生成的预测结果保真度低、粒度粗糙，限制了其在复杂环境中的应用。相比之下，基于海量互联网数据集训练的视频扩散模型，已展现出生成高质量视频的卓越能力，能够捕捉多样化的真实世界动态特征。本文提出Vid2World，这是一种将预训练视频扩散模型迁移应用于交互式世界模型的通用方法。为弥合模型差异，Vid2World通过对预训练视频扩散模型进行架构改造和训练目标优化，实现自回归生成能力，完成模型的因果化重构。此外，该方法创新性地引入因果动作引导机制，显著提升了交互式世界模型的动作可控性。在机器人操作和游戏模拟领域的广泛实验表明，本方法为将高性能视频扩散模型转化为交互式世界模型提供了可扩展且高效的解决方案。    