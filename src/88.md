
EWMBench: Evaluating Scene, Motion, and Semantic Quality in Embodied World Models

创造性人工智能的最新进展使得基于文本指令生成高保真图像和视频成为可能。在这些技术突破的基础上，文本到视频扩散模型已发展为具身世界模型(Embodied World Models, EWMs)，能够通过语言指令生成物理合理的场景，有效桥接具身人工智能应用中的视觉与行动。本研究针对如何超越通用感知指标来评估具身世界模型这一关键挑战，旨在确保生成结果具有物理基础且行为一致。我们提出了具身世界模型基准(EWMBench)，该专用评估框架围绕三个核心维度构建：视觉场景一致性、运动正确性和语义对齐性。该方法采用精心构建的数据集(涵盖多样化场景与运动模式)和全面的多维评估工具包，系统评估并对比不同模型。该基准不仅揭示了现有视频生成模型在满足具身任务特殊需求方面的局限性，更为领域未来发展提供了关键洞见。完整数据集与评估工具已开源发布于<https://github.com/AgibotTech/EWMBench>。   


