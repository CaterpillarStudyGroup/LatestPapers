MAMMA: Markerless & Automatic Multi-Person Motion Action Capture


我们提出 **MAMMA**，一种无标记动作捕捉流程，能够准确地从双人交互序列的多视角视频中恢复 SMPL-X 参数。传统的动作捕捉系统依赖于物理标记点。虽然它们精度很高，但其对专用硬件、手动标记点放置以及大量后期处理的要求，使得它们成本高昂且耗时。近期的基于学习的方法试图克服这些限制，但大多数方法主要面向单人捕捉、依赖于稀疏关键点，或在处理遮挡和物理交互时存在困难。   

在这项工作中，我们介绍了一种方法，它**基于分割掩码（segmentation masks）预测密集的 2D 表面标志点（landmarks）**，从而即使在严重遮挡的情况下也能实现针对特定人物的对应关系估计。我们采用了一种新颖的架构，利用**可学习的查询（learnable queries）** 来对应每个标志点。我们证明，我们的方法能够处理复杂的人与人交互，并且比现有方法具有更高的精度。   

为了训练我们的网络，我们构建了一个**大规模合成的多视角数据集**，该数据集结合了来自不同来源的人体动作，包括极端姿势、手部动作和近距离交互。我们的数据集产生了具有丰富身体接触和遮挡的高可变性合成序列，并包含了带有密集 2D 标志点的 SMPL-X 真实值（ground-truth）标注。最终成果是一个无需标记点即可捕捉人体动作的系统。   

与商业化的基于标记点的动作捕捉解决方案相比，我们的方法无需大量手动清理，就能提供具有竞争力的重建质量。最后，针对密集标志点预测和无标记动作捕捉领域缺乏通用基准的问题，我们基于真实多视角序列构建了**两种评估设置**。我们将开源我们的**数据集、基准、方法、训练代码和预训练模型权重**，供研究使用。   