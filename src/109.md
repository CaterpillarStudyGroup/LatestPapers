AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled Whole-Body Audio-Driven Avatars

全身音频驱动虚拟形象姿势与表情生成是创建逼真数字人、增强交互式虚拟代理能力的关键技术，在虚拟现实、数字娱乐和远程通信领域具有广泛应用。现有方法通常分别生成音频驱动的面部表情和肢体动作，这种方式存在显著缺陷：缺乏面部表情与肢体动作之间的协调性，导致生成的动画不够自然流畅。为解决这一问题，我们提出AsynFusion框架，该创新性方法通过扩散变换器(DiT)实现表情与动作的和谐合成。该方法基于双分支DiT架构，可并行生成面部表情和肢体动作。在模型内部，我们设计了协同同步模块以促进两种模态间的双向特征交互，并提出异步LCM采样策略在保持高质量输出的同时降低计算开销。大量实验表明，AsynFusion在实时同步全身动画生成方面达到最先进水平，在定量评估和定性评估中均显著优于现有方法。   