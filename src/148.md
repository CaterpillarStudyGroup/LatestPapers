UMA: Ultra-detailed Human Avatars via Multi-level Surface Alignment


基于多视角视频学习具有生动动态效果与照片级真实感的可动画着装人体模型，是计算机图形学与视觉领域的重要基础研究课题。得益于隐式表示技术的最新进展，通过将隐式表示绑定至可驱动的人体模板网格，当前可动画化身的建模质量已达到前所未有的水平。然而，此类方法通常难以保留最高级别的细节特征，在虚拟摄像机拉近视角以及进行4K及以上分辨率渲染时尤为明显。  

我们认为，这一局限源于表面追踪的不准确性——具体表现为角色几何体与真实表面间的深度失准和表面漂移问题，迫使细节外观模型不得不补偿几何误差。为解决该问题，我们提出一种潜在变形模型，并利用基础性2D视频点追踪器的引导信号来监督可动画角色的三维形变。相比可微分渲染，点追踪器对光照变化和表面差异具有更强的鲁棒性，且更不易陷入局部最优解。  

为克服2D点追踪器随时间累积的漂移问题及其缺乏三维空间感知的缺陷，我们提出级联式训练策略：通过将点追踪轨迹锚定至渲染的虚拟化身，生成具有一致性的3D点追踪轨迹，最终在顶点和纹理元素(texel)级别监督我们的虚拟化身模型。    

为验证方法的有效性，我们构建了一个包含五段多视角视频序列的新型数据集：每段时长超10分钟，使用40台标定过的6K分辨率摄像机采集，拍摄对象身着具有挑战性纹理图案与褶皱变形的服装。实验证明，我们的方法在渲染质量与几何精度上显著超越了现有最先进技术。  
