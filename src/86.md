Real2Render2Real: Scaling Robot Data Without Dynamics Simulation or Robot Hardware

扩展机器人学习需要大量多样化数据集。然而主流的数据收集范式——人类遥操作——仍然成本高昂，并受限于人工劳动力和实体机器人设备的获取。我们提出了Real2Render2Real(R2R2R)这一创新方法，无需依赖物体动力学模拟或机器人硬件遥操作即可生成训练数据。该方法输入仅需通过智能手机扫描一个或多个物体的三维模型，以及单段人类演示视频。R2R2R通过重建精细的物体三维几何形态与表面特征，追踪六自由度物体运动轨迹，可渲染生成数千个高视觉保真度、与机器人平台无关的演示数据。该框架采用3D高斯溅射技术(3DGS)实现刚体和铰接物体的灵活资产生成与轨迹合成，并将这些表征转化为网格模型以兼容IsaacLab等可扩展渲染引擎(但关闭碰撞模型)。由R2R2R生成的机器人演示数据可直接应用于依赖机器人本体感知状态与视觉观测的模型，如视觉-语言-动作模型(VLA)和模仿学习策略。物理实验表明，使用单个人类演示生成的R2R2R数据进行训练的模型，其性能可媲美基于150个人类遥操作演示数据训练的模型。项目主页：https://real2render2real.com