UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes

复杂场景中的人体运动合成是一个基础性挑战，其要求整合静态环境、可移动物体、自然语言提示和空间路径点等多模态信息，这超越了传统文本驱动运动生成任务的范畴。现有的基于语言条件的运动模型由于运动标记化的局限性(导致信息丢失且无法捕捉三维人体运动的连续上下文依赖性)，在场景感知运动生成方面常常面临困难。为解决这些问题，我们提出了统一运动语言模型UniHM，该模型利用基于扩散的生成方法实现场景感知人体运动合成。UniHM是首个支持复杂三维场景中文本驱动运动生成与文本驱动人物-物体交互(HOI)的统一框架。本研究的核心创新包含三个方面：(1)融合连续六自由度运动与离散局部运动标记的混合运动表征，提升运动真实感；(2)创新的无查找量化变分自编码器(LFQ-VAE)，在重建精度和生成性能上均超越传统VQ-VAE；(3)增强版Lingo数据集(HumanML3D注释增强版)，为场景特定运动学习提供更强化监督。实验结果表明，UniHM在OMOMO基准测试中实现了与当前最优方法相当的文本驱动HOI合成性能，同时在HumanML3D数据集上获得了具有竞争力的通用文本条件运动生成效果。   