HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation


为应对人-物交互(HOI)视频生成中的关键限制——特别是对精选运动数据的依赖、对新物体/场景的泛化能力有限以及可访问性受限——我们推出了**混元视频-HOMA (HunyuanVideo-HOMA)**，一个**弱约束的多模态驱动框架**。   

HunyuanVideo-HOMA 通过**稀疏解耦的运动引导**增强可控性，并减少对精确输入的依赖。它将外观和运动信号编码到**多模态扩散Transformer (MMDiT)** 的双重输入空间中，并在共享的上下文空间内融合这些信号，以合成时序一致且物理合理的交互。  

为了优化训练，我们集成了两个关键组件：   
1.  **参数空间HOI适配器**：该适配器从预训练的MMDiT权重初始化，在保留先验知识的同时实现高效适配。   
2.  **面部交叉注意力适配器**：用于实现解剖结构准确的音频驱动唇形同步。   

大量实验证实了该框架在弱监督条件下，于交互自然度和泛化能力方面达到了**最先进(state-of-the-art)的性能**。   

最后，HunyuanVideo-HOMA 展示了其在文本条件生成和交互式物体操控方面的**多功能性**，并得到了一个**用户友好型演示界面**的支持。   

项目页面位于：<https://anonymous.4open.science/w/homa-page-0FBE/>   