MMGT: Motion Mask Guided Two-Stage Network for Co-Speech Gesture Video Generation


**伴随语音的手势视频生成**(Co-Speech Gesture Video Generation)旨在根据音频驱动的静态图像生成生动逼真的说话视频。这一任务极具挑战性，因为身体不同部位在**运动幅度、音频相关性和细节特征**方面存在巨大差异。仅依赖音频作为控制信号往往难以捕捉视频中的大幅手势动作，导致更明显的**伪影和失真**。   

现有方法通常通过引入**额外的先验信息**来解决此问题，但这可能会限制任务的实际应用。具体而言，我们提出了一种**运动掩码引导的两阶段网络(MMGT)**。该网络利用音频，以及从音频信号生成的**运动掩码**和**运动特征**，共同驱动生成同步的语音-手势视频。    

*   **第一阶段**：**空间掩码引导的音频姿态生成(SMGA)网络** 从音频生成高质量的姿态视频和运动掩码，有效捕捉面部和手势等关键区域的大幅度运动。
*   **第二阶段**：我们将**运动掩码分层音频注意力机制(MM-HAA)** 集成到**稳定扩散视频生成模型**中，克服了传统方法在**细粒度运动生成**和**区域特定细节控制**方面的局限性。这保证了生成具有精确纹理和运动细节的**高质量上半身视频**。    

评估结果表明，该方法在**视频质量、唇形同步和手势自然度**方面均有提升。模型和代码开源地址：<https://github.com/SIA-IDE/MMGT>。    