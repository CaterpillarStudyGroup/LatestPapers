LMP: Leveraging Motion Prior in Zero-Shot Video Generation with Diffusion Transformer

近年来，大规模预训练扩散变换器模型在视频生成领域取得了显著进展。尽管当前DiT模型能够生成高清、高帧率且高度多样化的视频，但对视频内容的细粒度控制仍存在不足。仅通过提示语控制视频中主体的运动具有挑战性，尤其是在描述复杂运动时。此外，现有方法在图像到视频生成中无法有效控制运动，因为参考图像中的主体与参考视频中的主体在初始位置、尺寸和形状方面往往存在差异。为此，我们提出了利用运动先验(Leveraging Motion Prior, LMP)的零样本视频生成框架。该框架通过利用预训练扩散变换器的强大生成能力，使生成的视频在文本到视频和图像到视频生成中都能参考用户提供的运动视频。具体而言，我们首先引入前景-背景解耦模块来区分参考视频中的运动主体和背景，避免对目标视频生成产生干扰。设计了重加权运动迁移模块，使目标视频能够参考源视频的运动特征。为避免参考视频主体对目标视频的干扰，我们提出外观分离模块来抑制参考主体在目标视频中的外观表现。我们为DAVIS数据集标注了详细的提示语用于实验，并设计了评估指标验证方法的有效性。大量实验表明，我们的方法在生成质量、提示-视频一致性及控制能力方面均达到了最先进水平。项目主页详见<https://vpx-ecnu.github.io/LMP-Website/>。

