TesserAct: Learning 4D Embodied World Models

本文提出了一种有效的学习新型4D具身世界模型的方法。该模型能够预测3D场景随具身智能体动作产生的动态演变，同时保持时空一致性。我们提出通过RGB-DN(RGB、深度与法线)视频训练来学习4D世界模型，这种方法不仅通过将形状细节、配置状态和时间变化纳入预测而超越了传统2D模型，还能有效学习具身智能体的精确逆动态模型。具体而言，我们首先利用现成模型扩展现有机器人操作视频数据集，补充深度和法线信息；然后在此标注数据集上对视频生成模型进行微调，使其能够联合预测每帧的RGB-DN信息；最后提出一种算法，直接将生成的RGB、深度和法线视频转换为高质量的4D世界场景。该方法在具身场景的4D场景预测中确保了时间与空间一致性，实现了具身环境的新视角合成，并支持策略学习——实验表明由此获得的策略性能显著优于基于传统视频世界模型的方案。