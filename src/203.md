Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos


由于可变形物体多样的物理特性以及难以从有限的视觉信息中估计其状态，建模其动力学具有挑战性。我们通过一个**神经动力学框架**来解决这些挑战，该框架在混合表示中结合了**物体粒子 (object particles)** 和**空间网格 (spatial grids)**。我们的**粒子-网格模型 (particle-grid model)** 既能捕捉全局形状和运动信息，又能预测密集的粒子运动，从而能够建模具有不同形状和材料的物体。粒子代表物体形状，而空间网格则将三维空间离散化，以确保空间连续性并提高学习效率。结合用于视觉渲染的**高斯泼溅 (Gaussian Splatting)** 技术，我们的框架实现了**完全基于学习的可变形物体数字孪生模型 (fully learning-based digital twin)**，并生成**动作条件的三维视频 (3D action-conditioned videos)**。   

通过实验，我们证明了我们的模型能够从机器人-物体交互的**稀疏视角RGB-D记录 (sparse-view RGB-D recordings)** 中学习各种物体（如绳索、布料、毛绒玩具和纸袋）的动力学，同时还能在**类别层面 (category level)** 上泛化到未见过的实例。我们的方法优于最先进的**基于学习的模拟器 (learning-based simulators)** 和**基于物理的模拟器 (physics-based simulators)**，尤其是在**相机视角有限 (limited camera views)** 的场景下。此外，我们展示了所学模型在**基于模型的规划 (model-based planning)** 中的实用性，使其能够在一系列任务中实现**目标条件的物体操作 (goal-conditioned object manipulation)**。    

项目页面位于：<https://kywind.github.io/pgnd>   