HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting


**3D人体生成**是计算机视觉和图形学领域中一个具有广泛应用的重要问题。尽管在生成式人工智能(如扩散模型)和渲染方法(如神经辐射场或高斯溅射)方面取得了最新进展，但根据文本提示精确控制3D人体的生成仍然是一个未解决的挑战。现有方法在细节精细度、手部和面部精准渲染、人体真实感以及外观可控性方面存在困难。人体图像数据在多样性、真实性和标注方面的不足也仍然是个挑战，阻碍了基础性3D人体模型的发展。    

我们提出了一种**弱监督流程**来尝试应对这些挑战。在第一步中，我们使用**最先进的图像扩散模型**生成了一个具有可控属性(如外貌、种族、性别等)的**高真实感人像数据集**。接下来，我们提出了一种**高效的映射方法**，使用基于**Transformer的架构**将图像特征映射到**3D点云**。最后，我们通过训练一个**点云扩散模型**来形成闭环，该模型以用于生成原始样本的相同**文本提示**为条件。    

与现有最先进方法相比，我们展示了**数量级的3D人体生成速度提升**，同时显著改善了**文本提示对齐度、真实感和渲染质量**。我们将**开放代码和数据集**。   