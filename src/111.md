MAGIC: Motion-Aware Generative Inference via Confidence-Guided LLM


**静态3D生成的最新进展**加剧了对物理一致性**动态3D内容**的需求。然而，现有的视频生成模型，包括基于扩散的方法，通常优先考虑**视觉真实感**，却忽视了**物理合理性**，导致生成的对象动态效果**不符合物理规律**。  

先前实现**物理感知动态生成**的方法通常依赖于**大规模标注数据集**或**广泛的模型微调**，这带来了巨大的计算和数据收集负担，并限制了其在**不同场景下的可扩展性**。   

为了应对这些挑战，我们提出了**MAGIC**框架——一个**免训练**的、用于**单图像物理属性推断**与**动态生成**的框架。该框架将**预训练的图生视频扩散模型**与**基于大语言模型（LLM）的迭代推理**相结合。MAGIC能够从单张静态图像生成**运动丰富的视频**，并通过一个**置信度驱动的大语言模型反馈循环**弥合**视觉与物理之间的差距**，该循环能够自适应地引导扩散模型生成**符合物理规律的运动**。  

为了将**视觉动态转化为可控的物理行为**，我们进一步引入了一个**可微分物质点法（MPM）模拟器**。该模拟器直接操作于从单张图像重建出的**3D高斯模型**之上，无需任何监督或模型调整，即可产生**基于物理原理、可直接用于仿真的输出结果**。   

实验表明，MAGIC在**推断准确性**上超越了现有的物理感知生成方法，并且在**时间一致性**方面也优于最先进的视频扩散模型。   