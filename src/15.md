ActionArt: Advancing Multimodal Large Models for Fine-Grained Human-Centric Video Understanding

对视频中人类动作和姿态的细粒度理解对于以人为中心的人工智能应用至关重要。本研究提出ActionArt数据集——一个旨在推动以人为中心的多模态理解研究的细粒度视频描述数据集。我们的数据集包含数千个涵盖广泛人类动作、人-物交互和多样化场景的视频片段，每个视频均配有详细标注，精确记录了每个肢体动作的细微变化。我们设计了八项子任务来评估现有大型多模态模型在不同维度的细粒度理解能力。实验结果表明，虽然当前主流大型多模态模型在多项任务上表现优异，但在细粒度理解层面仍存在明显不足。我们分析这种局限主要源于高质量标注数据的稀缺性，这类数据的标注既成本高昂又难以进行规模化人工标注。鉴于人工标注的高成本和规模化难题，我们提出通过代理任务来增强模型在时空维度上的感知能力。这些代理任务经过精心设计，可以通过现有MLLMs自动生成的数据进行驱动，从而降低对昂贵人工标注的依赖。实验结果显示，所提出的代理任务显著缩小了与人工标注细粒度数据训练的性能差距。