Model See Model Do: Speech-Driven Facial Animation with Style Control

语音驱动的三维面部动画在虚拟化身、游戏和数字内容创作等应用中发挥着关键作用。尽管现有方法在实现精确唇形同步和生成基础情感表情方面取得了显著进展，但它们往往难以捕捉并有效传递细腻的表演风格。我们提出了一种新颖的基于示例的生成框架，通过将隐扩散模型与参考风格片段进行条件约束，可生成高表现力且时序连贯的面部动画。为解决精确遵循风格参考的挑战，我们引入了一种称为"风格基底"的创新条件调节机制：该机制从参考片段中提取关键姿势，以叠加方式引导扩散生成过程适配目标风格，同时保持唇形同步质量。这种方法使模型能够捕捉细微的风格线索，同时确保生成动画与输入语音高度契合。通过定性、定量和感知评估的全面验证，我们的方法在忠实复现目标风格的同时，实现了跨多种语音场景的卓越唇形同步效果。