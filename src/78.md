CyberHost: A One-stage Diffusion Framework for Audio-driven Talking Body Generation

基于扩散的视频生成技术已取得显著进展，推动了人体动画研究的蓬勃发展。尽管通过多种模态驱动人像动画已实现突破，但目前人体动画解决方案仍主要聚焦于视频驱动方法，使得音频驱动的说话身体生成研究相对较少。本文提出CyberHost——一种单阶段音频驱动的说话身体生成框架，有效解决了半身动画中常见的合成退化问题，包括手部完整性、身份一致性和动作自然性。CyberHost的核心设计体现在两个创新点：首先，区域注意力模块(RAM)通过维护一组可学习的隐式身份无关潜在特征，并将其与特定身份局部视觉特征相结合，显著提升了关键区域(如手部)的合成质量；其次，人体先验引导条件通过引入更多人体结构先验知识，有效降低了生成动作模式的不确定性，从而提升了生成视频的稳定性。据我们所知，CyberHost是首个能够实现人体零样本视频生成的单阶段音频驱动扩散模型。大量实验表明，CyberHost在定量评估和定性分析方面均优于现有方法。此外，该框架可扩展应用于视频驱动及音视频混合驱动场景，同样能获得令人满意的生成效果。

