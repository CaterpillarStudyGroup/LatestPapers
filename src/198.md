GAF: Gaussian Action Field as a Dvnamic World Model for Robotic Mlanipulation

**准确的动作推断对于基于视觉的机器人操作至关重要。** 现有方法通常遵循两种范式：一种是**视觉到动作 (V-A)** 范式，直接从视觉输入预测动作；另一种是**视觉到三维再到动作 (V-3D-A)** 范式，利用中间的三维表示。然而，由于操作场景的复杂性和动态性，这些方法常常难以实现动作的精准推断。   

本文提出了一种 **V-4D-A 框架**，该框架通过**高斯动作场 (Gaussian Action Field, GAF)** 实现了从运动感知的四维表示中进行直接的动作推理。GAF 通过引入可学习的运动属性扩展了三维高斯泼溅 (3D Gaussian Splatting, 3DGS)，能够同时建模动态场景和操作动作。   

为了学习时变场景几何和动作感知的机器人运动，GAF 支持三种关键查询类型：重建当前场景、预测未来帧，以及通过机器人运动估计初始动作。此外，GAF 生成的高质量当前帧和未来帧，有助于通过 **GAF 引导的扩散模型** 来优化操作动作。   

大量实验表明，该方法取得了显著提升：在重建质量方面，GAF 实现了 **+11.5385 dB PSNR** 的提升和 **-0.5574 LPIPS** 的降低；同时，在机器人操作任务的平均成功率上，相比最先进方法提升了 **10.33%**。    

项目页面：<http://chaiying1.github.io/GAF.github.io/project_page/>   