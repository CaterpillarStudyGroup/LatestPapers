LatentSync: Taming Audio-Conditioned Latent Diffusion Models for Lip Sync with SyncNet Supervision

端到端音频条件化潜在扩散模型(LDMs)已被广泛应用于音频驱动的人像动画生成，其在生成逼真高清说话视频方面展现出显著优势。然而，直接将音频条件化LDMs应用于唇部同步任务时，其同步精度表现欠佳。通过深入分析，我们发现其根本原因在于"捷径学习问题"——模型主要学习视觉特征间的捷径关联，而忽略了关键的视听相关性。为解决这一问题，我们探索了多种将SyncNet监督信号融入音频条件化LDMs的方法，以显式强化视听相关性的学习。由于SyncNet的性能直接影响监督模型的唇部同步精度，训练一个充分收敛的SyncNet变得至关重要。我们首次开展了系统性实证研究，揭示了影响SyncNet收敛的关键因素。基于分析结果，我们提出了具有稳定收敛架构设计的StableSyncNet。在HDTF测试集上，StableSyncNet的准确率从91%显著提升至94%。此外，我们提出创新的时间表征对齐机制(TREPA)以增强生成视频的时序一致性。实验结果表明，在HDTF和VoxCeleb2数据集的多项评估指标上，我们的方法均超越了当前最先进的唇部同步技术。