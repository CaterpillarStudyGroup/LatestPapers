Eye2Eye: A Simple Approach for Monocular-to-Stereo Video Synthesis

沉浸式视觉体验的日益普及，推动了对立体3D视频生成技术的关注。尽管视频合成领域已取得重大进展，但由于3D视频数据的相对稀缺，创建3D视频仍面临挑战。我们提出了一种将文本到视频生成器转化为视频到立体生成器的简单方法。给定输入视频，我们的框架能自动生成视点偏移后的视频帧，从而实现引人入胜的3D效果。现有及同期方法通常采用多阶段处理流程：首先估计视频视差或深度，随后对视频进行形变处理以生成第二视角，最后对暴露区域进行修复。当场景涉及镜面反射表面或透明物体时，这种方法的固有缺陷就会显现。在此类情况下，单层视差估计无法满足需求，导致形变过程中出现伪影和像素位移错误。我们的研究通过直接合成新视点突破了这些限制，完全规避了中间处理步骤。这一突破的实现得益于对预训练视频模型在几何结构、物体材质、光学特性和语义理解等方面先验知识的有效利用，而无需依赖外部几何模型或人为解耦合成过程中的几何要素。我们在包含多种物体材质与复杂组合的真实场景中验证了该方法的优势。视频演示请访问：https://video-eye2eye.github.io
