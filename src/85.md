TexTailor: Customized Text-aligned Texturing via Effective Resampling

我们提出了TexTailor，这是一种从文本描述生成一致性物体纹理的新方法。现有的文本到纹理合成方法利用深度感知扩散模型，通过预定义的多视角逐步生成图像并合成纹理。然而，这些方法会导致跨视角纹理属性逐渐偏移，主要原因在于：(1) 扩散过程中每个视角对先前合成纹理的信息整合不足；(2) 纹理合成过程的自回归特性。此外，预定义的相机位置选择未考虑物体几何形状，限制了不同视角合成纹理信息的有效利用，最终影响整体纹理一致性。   

TexTailor通过以下创新解决这些问题：(1) 采用重采样方案，在扩散过程中反复整合来自先前合成纹理的信息；(2) 基于这些重采样纹理对深度感知扩散模型进行微调。在此过程中，我们发现仅使用少量训练图像会限制模型生成与条件对齐的高保真图像的能力，因此提出了性能保持损失函数来缓解该问题。同时，我们通过基于物体几何形状自适应调整相机位置，改进了视角一致性纹理的合成。在Objaverse数据集子集和ShapeNet汽车数据集上的实验表明，TexTailor在合成视角一致性纹理方面优于现有最先进方法。

