RL from Physical Feedback: Aligning Large Motion Models with Humanoid Control

本文聚焦于机器人学中的一个关键挑战：如何将文本驱动的人类动作转化为人形机器人可执行的动作，从而实现高效、低成本的新行为学习。虽然现有的文本到动作生成方法能够实现语言与动作的语义对齐，但它们生成的动常常在运动学或物理上不可行，不适合实际部署。为了弥合仿真与现实之间的鸿沟，我们提出了**基于物理反馈的强化学习（Reinforcement Learning from Physical Feedback, RLPF）**——一个将物理感知的运动评估与文本条件化动作生成相结合的全新框架。RLPF利用一个动作跟踪策略在物理仿真器中评估动作的可行性，并据此生成奖励信号用于微调动作生成器。此外，RLPF引入了一个对齐验证模块，以确保生成的动始终忠实于文本指令的语义。这种联合优化保证了动作在物理上的合理性和对指令语义的对齐性。大量实验表明，RLPF在生成物理可行的动作方面显著优于基线方法，同时保持了与文本指令的语义对应，从而成功实现了在真人形机器人上的部署。     