Think Before You Diffuse: LLMs-Guided Physics-Aware Video Generation

最近的视频扩散模型在生成视觉效果出色的结果方面展现出了强大的能力，但在生成视频中合成正确的物理效果仍然具有挑战性。现实世界运动的复杂性、交互作用和动力学特性给从数据中学习物理规律带来了巨大困难。    

在本工作中，我们提出了 **DiffPhy**，这是一个通用框架，通过对预训练的视频扩散模型进行微调，实现物理准确且逼真的视频生成。我们的方法利用大型语言模型(LLMs)从文本提示中显式推理出全面的**物理上下文(physical context)**，并用其指导生成过程。   

为了将物理上下文整合到扩散模型中，我们利用**多模态大型语言模型(MLLM)**作为**监督信号(supervisory signal)**，并引入了一套新颖的训练目标。这些目标共同作用，确保生成的视频在物理上正确，且与输入文本保持语义一致性。   

我们还建立了一个包含多样化物理动作和事件的**高质量物理视频数据集**，以促进有效的微调。在公开基准上进行的大量实验表明，DiffPhy 能够在各种物理相关场景中产生最先进(state-of-the-art)的结果。   

我们的项目页面位于：<https://bwgzk-keke.github.io/DiffPhy/>   