Locality Sensitive Avatars From Video

我们提出了一种基于神经辐射场(NeRF)的局部敏感人体表征网络(locality-sensitive avatar)，用于从单目视频中学习人体运动。该方法通过非线性映射将视频各帧对齐到规范空间，并将该映射分解为骨骼刚性运动与非刚性形变两部分。我们的核心创新在于采用图神经网络(GNN)建模非刚性形变部分，使姿态信息仅作用于相邻身体部位，从而保留细节特征。相比传统基于整体形状坐标空间的规范表征方法，这种局部敏感的运动建模能同时还原逼真的形体轮廓和生动的细粒度细节。我们在ZJU-MoCap、SynWild、ActorsHQ、MVHumanNet等数据集及多种户外视频上进行评估。实验表明，通过对规范特征空间施加局部敏感形变，我们首次实现了在新视角合成、新姿态动画和三维形状重建三个任务上同时达到最优性能。项目代码已开源：<https://github.com/ChunjinSong/lsavatar>。    
