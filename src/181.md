Controllable Expressive 3D Facial Animation via Diffusion in a Unified Multimodal Space


音频驱动的情绪化3D面部动画面临两大关键挑战：(1) 依赖单模态控制信号（视频、文本或情绪标签），未能综合利用其互补优势实现全面情绪操控；(2) 基于确定性回归的映射方法限制了情感表达和非语言行为的随机性本质，导致合成动画表现力受限。为应对这些挑战，我们提出基于扩散模型的可控表达性3D面部动画框架。该方案包含两大创新点：(1) 以FLAME模型为核心的多模态情绪绑定策略，通过对比学习对齐文本、音频及情绪标签等多源模态，实现灵活的多信号情绪控制；(2) 搭载内容感知注意力与情绪引导层的注意力潜扩散模型，在保持时序连贯性与自然面部动态的同时增强运动多样性。大量实验表明，本方法在多数指标上超越现有方案，情绪相似度提升21.6%，且能保持符合生理规律的面部动态。

项目主页：<https://kangweiiliu.github.io/Control_3D_Animation>