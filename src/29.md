HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation

扩散模型的快速发展为VR和AR技术的应用革新带来曙光，这类技术通常需要场景级4D资产来支撑用户体验。然而，现有扩散模型主要聚焦于静态3D场景或物体级动态建模，限制了其提供真正沉浸式体验的能力。为解决这一问题，我们提出HoloTime框架：通过整合视频扩散模型实现单提示词或参考图像生成全景视频，并开发360度4D场景重建方法，将生成的全景视频无缝转化为4D资产，为用户创造完全沉浸的4D体验。具体而言，为驯化视频扩散模型生成高保真全景视频，我们构建了首个适用于下游4D场景重建任务的全景视频数据集360World。基于此数据集，我们提出全景动画生成器 —— 一种两阶段图像到视频扩散模型，可将全景图像转化为高质量全景视频。随后开发的全景时空重建技术，通过时空深度估计方法将生成视频转化为4D点云，进而优化整体4D高斯溅射表征，重建时空一致的4D场景。通过与现有方法的对比实验验证，本方法在全景视频生成与4D场景重建方面均展现显著优势，证明其能创建更具吸引力和真实感的沉浸式环境，从而提升VR/AR应用中的用户体验。