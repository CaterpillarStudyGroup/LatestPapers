TSTMotion: Training-free Scene-awarenText-to-motion Generation

文本驱动动作生成技术近期引发了广泛的研究关注，其核心目标是在空白背景下生成人体运动序列。然而，真实场景中的人类动作往往发生在多样化的三维环境中，这推动了面向场景感知的文本驱动动作生成方法的探索。但现有场景感知方法通常依赖大规模三维场景中的真实动作序列，由于数据采集成本高昂，这一需求面临实际挑战。为应对这一难题，我们首次提出了一种**免训练的**场景感知文本驱动动作生成框架**TSTMotion**，该框架通过高效赋能预训练的空白背景动作生成模型，赋予其场景感知能力。具体而言，基于给定的三维场景和文本描述，我们协同运用基础模型进行场景感知动作引导信号的推理、预测和验证。随后，通过对空白背景动作生成器进行两处针对性改进，将动作引导信号融入生成流程，最终输出符合场景语义的文本驱动动作序列。大量实验验证了所提框架的有效性和泛化能力。代码已在\href{https://tstmotion.github.io/}{项目主页}开源。