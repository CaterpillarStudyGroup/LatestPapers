TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans


由大型语言模型(LLM)驱动的数字人，近期引发了一系列关于伴随语音手势生成系统的研究。然而，现有方法在实时合成和长文本理解方面存在困难。   

本文介绍了**基于Transformer的丰富动作匹配(TRiMM)**，这是一种用于实时3D手势生成的新型多模态框架。我们的方法包含三个模块：   
1. **跨模态注意力机制**：实现语音与手势之间的精确时间对齐；   
2. **带滑动窗口机制的长上下文自回归模型**：用于有效的序列建模；   
3. **大规模手势匹配系统**：构建原子动作库并支持实时检索。   

此外，我们开发了一个在虚幻引擎(Unreal Engine)中实现的轻量级实验管线。我们的方法在消费级显卡(Geforce RTX3060)上实现了**120 FPS的实时推理**，并保持**每句0.15秒的延迟**。   

在ZEGGS和BEAT数据集上进行的大量主观和客观评估表明，我们的模型性能优于当前最先进的方法。TRiMM在保证手势质量的同时，显著提升了伴随语音手势的生成速度，使得由LLM驱动的数字人能够实时响应语音并合成对应的手势动作。   

我们的代码开源地址：<https://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matching>    