DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation


**可控视频生成（CVG）** 技术发展迅速，然而当需要在嘈杂的控制信号下让多个角色移动、互动并交换位置时，现有系统却表现不佳。我们推出了 **DanceTogether** 来解决这一不足，它是首个端到端的扩散框架，能够将**单张参考图像**加上**独立的姿态掩膜流**转化为**长时、逼真的视频**，同时**严格保持每个角色的身份特征**。   

我们提出了一种新颖的 **MaskPoseAdapter**，它在每一步去噪过程中，通过将鲁棒的跟踪掩膜与语义丰富但含噪的姿态热图相融合，从而绑定“谁”（身份）和“如何”（动作），有效消除了困扰逐帧处理流程的**身份漂移**和**外观渗色**问题。   

为了进行大规模训练和评估，我们引入了：   
(i) **PairFS-4K**：包含26小时的双人滑冰视频素材，涵盖超过7,000个不同身份；   
(ii) **HumanRob-300**：一小时的类人机器人互动数据集，用于快速跨领域迁移；   
(iii) **TogetherVideoBench**：一个包含三个测试轨道的基准平台，核心是 **DanceTogEval-100** 测试集，涵盖舞蹈、拳击、摔跤、瑜伽和花样滑冰。   

在 **TogetherVideoBench** 上，**DanceTogether** 的性能显著优于现有技术。此外，我们证明只需**一小时的微调训练**，即可生成令人信服的人机互动视频，突显了其在具身人工智能（embodied-AI）和人机交互（HRI）任务上强大的泛化能力。大量的消融实验证实，**持续的身份-动作绑定**是实现这些提升的关键。   

总而言之，我们的模型、数据集和基准平台共同将CVG从**单主体编排**提升到了**组合可控的多主体互动**水平，为数字制作、模拟仿真和具身智能开辟了新途径。我们的视频演示和代码已开放：<https://DanceTog.github.io/>。   
