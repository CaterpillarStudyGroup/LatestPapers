ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models

目前基于扩散模型的文本到视频生成方法仅限于生成单镜头的短视频片段，缺乏生成带有离散转场的多镜头视频的能力（即同一角色在相同或不同背景下执行不同活动的场景）。为了突破这一限制，我们提出了一个包含数据集收集流程和视频扩散模型架构扩展的框架，以实现文本到多镜头视频生成。我们的方法能够生成将所有镜头视为单一视频的多镜头视频，通过对所有镜头所有帧的全局注意力机制确保角色与背景的一致性，同时允许用户通过镜头特定条件控制镜头的数量、时长及内容。这一效果通过以下创新实现：(1) 在文本到视频模型中引入转场标记(transition token)，用于控制新镜头的起始帧；(2) 采用局部注意力掩码策略(local attention masking strategy)，既控制转场标记的作用范围，又支持镜头特定提示。为了获取训练数据，我们设计了一个新颖的数据收集流程，能够从现有单镜头视频数据集构建多镜头视频数据集。大量实验表明，仅需对预训练文本到视频模型进行数千次迭代的微调，模型即可具备生成具备镜头级控制能力的多镜头视频，其性能显著超越基线方法。更多技术细节请访问项目网站：<https://shotadapter.github.io/>

