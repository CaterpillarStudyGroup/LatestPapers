FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video Generation


文本到视频扩散模型在建模运动、物理和动态交互等时间层面方面的能力存在众所周知的局限性。现有方法通过重新训练模型或引入外部条件信号来强制时间一致性，以解决这一限制。   

在本研究中，我们探讨是否可以直接从预训练模型的预测中提取有意义的时间表征，而无需任何额外训练或辅助输入。我们提出了 **FlowMo**，一种新颖的无训练引导方法，它仅利用模型自身在每个扩散步骤中的预测来增强运动连贯性。   

FlowMo首先通过测量对应于连续帧的潜在表示之间的距离，推导出一个**外观去偏的时间表征**。这突显了模型预测的隐含时间结构。然后，它通过测量时间维度上的**块级方差**来估计运动连贯性，并在采样过程中动态地引导模型减少这种方差。   

在多个文本到视频模型上进行的大量实验表明，FlowMo 在不牺牲视觉质量或提示对齐的前提下，显著提高了运动连贯性，为增强预训练视频扩散模型的时间保真度提供了一种有效的即插即用解决方案。    
