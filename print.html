<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Latest Paper</title>
        <meta name="robots" content="noindex" />
        <!-- Custom HTML head -->
        <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
        <meta name="description" content="The note of Latest Articles">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">
        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">
        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->
        <link rel="stylesheet" href="theme/pagetoc.css">
        <!-- MathJax -->
        <script async type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    </head>
    <body>
        <!-- Provide site root to javascript -->
        <script type="text/javascript">
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script type="text/javascript">
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script type="text/javascript">
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script type="text/javascript">
            var html = document.querySelector('html');
            var sidebar = 'hidden';
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="index.html">Introduction</a></li><li class="chapter-item expanded affix "><li class="part-title">Abstract</li><li class="chapter-item expanded "><a href="*.html"><strong aria-hidden="true">1.</strong> </a></li><li class="chapter-item expanded "><a href="205.html"><strong aria-hidden="true">2.</strong> Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D Rewards</a></li><li class="chapter-item expanded "><a href="204.html"><strong aria-hidden="true">3.</strong> Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material</a></li><li class="chapter-item expanded "><a href="203.html"><strong aria-hidden="true">4.</strong> Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos</a></li><li class="chapter-item expanded "><a href="202.html"><strong aria-hidden="true">5.</strong> HOIDiNi: Human-Object Interaction through Diffusion Noise Optimization</a></li><li class="chapter-item expanded "><a href="201.html"><strong aria-hidden="true">6.</strong> GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis for Unseen Objects</a></li><li class="chapter-item expanded "><a href="200.html"><strong aria-hidden="true">7.</strong> VideoMAR: Autoregressive Video Generatio with Continuous Tokens</a></li><li class="chapter-item expanded "><a href="199.html"><strong aria-hidden="true">8.</strong> SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting</a></li><li class="chapter-item expanded "><a href="198.html"><strong aria-hidden="true">9.</strong> GAF: Gaussian Action Field as a Dvnamic World Model for Robotic Mlanipulation</a></li><li class="chapter-item expanded "><a href="197.html"><strong aria-hidden="true">10.</strong> GMT: General Motion Tracking for Humanoid Whole-Body Control</a></li><li class="chapter-item expanded "><a href="196.html"><strong aria-hidden="true">11.</strong> Toward Rich Video Human-Motion2D Generation</a></li><li class="chapter-item expanded "><a href="195.html"><strong aria-hidden="true">12.</strong> PF-LHM: 3D Animatable Avatar Reconstruction from Pose-free Articulated Human Images</a></li><li class="chapter-item expanded "><a href="194.html"><strong aria-hidden="true">13.</strong> MAMMA: Markerless &amp; Automatic Multi-Person Motion Action Capture</a></li><li class="chapter-item expanded "><a href="193.html"><strong aria-hidden="true">14.</strong> iDiT-HOI: Inpainting-based Hand Object Interaction Reenactment via Video Diffusion Transformer</a></li><li class="chapter-item expanded "><a href="192.html"><strong aria-hidden="true">15.</strong> RL from Physical Feedback: Aligning Large Motion Models with Humanoid Control</a></li><li class="chapter-item expanded "><a href="191.html"><strong aria-hidden="true">16.</strong> KungfuBot: Physics-Based Humanoid Whole-Body Control for Learning Highly-Dynamic Skills</a></li><li class="chapter-item expanded "><a href="190.html"><strong aria-hidden="true">17.</strong> Enter: Graduated Realism: A Pedagogical Framework for AI-Powered Avatars in Virtual Reality Teacher Training</a></li><li class="chapter-item expanded "><a href="189.html"><strong aria-hidden="true">18.</strong> Auto-Connect: Connectivity-Preserving RigFormer with Direct Preference Optimization</a></li><li class="chapter-item expanded "><a href="188.html"><strong aria-hidden="true">19.</strong> SignAligner: Harmonizing Complementary Pose Modalities for Coherent Sign Language Generation</a></li><li class="chapter-item expanded "><a href="187.html"><strong aria-hidden="true">20.</strong> Aligned Novel View Image and Geometry Synthesis via Cross-modal Attention Instillation</a></li><li class="chapter-item expanded "><a href="186.html"><strong aria-hidden="true">21.</strong> AlignHuman: Improving Motion and Fidelity via Timestep-Segment Preference Optimization for Audio-Driven Human Animation</a></li><li class="chapter-item expanded "><a href="185.html"><strong aria-hidden="true">22.</strong> DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers</a></li><li class="chapter-item expanded "><a href="184.html"><strong aria-hidden="true">23.</strong> M4V: Multi-Modal Mamba for Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="183.html"><strong aria-hidden="true">24.</strong> Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for Human Motion Generation</a></li><li class="chapter-item expanded "><a href="182.html"><strong aria-hidden="true">25.</strong> Rethinking Generative Human Video Coding with Implicit Motion Transformation</a></li><li class="chapter-item expanded "><a href="181.html"><strong aria-hidden="true">26.</strong> Controllable Expressive 3D Facial Animation via Diffusion in a Unified Multimodal Space</a></li><li class="chapter-item expanded "><a href="180.html"><strong aria-hidden="true">27.</strong> PlayerOne: Egocentric World Simulator</a></li><li class="chapter-item expanded "><a href="179.html"><strong aria-hidden="true">28.</strong> HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene</a></li><li class="chapter-item expanded "><a href="178.html"><strong aria-hidden="true">29.</strong> AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation</a></li><li class="chapter-item expanded "><a href="177.html"><strong aria-hidden="true">30.</strong> Self-Supervised Multi-Part Articulated Objects Modeling via Deformable Gaussian Splatting and Progressive Primitive Segmentation</a></li><li class="chapter-item expanded "><a href="176.html"><strong aria-hidden="true">31.</strong> SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach</a></li><li class="chapter-item expanded "><a href="175.html"><strong aria-hidden="true">32.</strong> RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping</a></li><li class="chapter-item expanded "><a href="174.html"><strong aria-hidden="true">33.</strong> StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams</a></li><li class="chapter-item expanded "><a href="173.html"><strong aria-hidden="true">34.</strong> Orientation Matters: Making 3D Generative Models Orientation-Aligned</a></li><li class="chapter-item expanded "><a href="172.html"><strong aria-hidden="true">35.</strong> Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos</a></li><li class="chapter-item expanded "><a href="171.html"><strong aria-hidden="true">36.</strong> HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation</a></li><li class="chapter-item expanded "><a href="170.html"><strong aria-hidden="true">37.</strong> PIG: Physically-based Multi-Material Interaction with 3D Gaussians</a></li><li class="chapter-item expanded "><a href="169.html"><strong aria-hidden="true">38.</strong> Drive Any Mesh: 4D Latent Diffusion for Mesh Deformation from Video</a></li><li class="chapter-item expanded "><a href="168.html"><strong aria-hidden="true">39.</strong> NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D Generation</a></li><li class="chapter-item expanded "><a href="167.html"><strong aria-hidden="true">40.</strong> PhysiInter: Integrating Physical Mapping for High-Fidelity Human Interaction Generation</a></li><li class="chapter-item expanded "><a href="166.html"><strong aria-hidden="true">41.</strong> Parametric Gaussian Human Model: Generalizable Prior for Efficient and Realistic Human Avatar Modeling</a></li><li class="chapter-item expanded "><a href="165.html"><strong aria-hidden="true">42.</strong> Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting </a></li><li class="chapter-item expanded "><a href="164.html"><strong aria-hidden="true">43.</strong> Follow-Your-Creation: Empowering 4D Creation through Video Inpainting</a></li><li class="chapter-item expanded "><a href="163.html"><strong aria-hidden="true">44.</strong> Realizing Text-Driven Motion Generation on NAO Robot: A Reinforcement Learning-Optimized Control Pipeline</a></li><li class="chapter-item expanded "><a href="162.html"><strong aria-hidden="true">45.</strong> Follow-Your-Motion: Video Motion Transfer via Efficient Spatial-Temporal Decoupled Finetuning</a></li><li class="chapter-item expanded "><a href="161.html"><strong aria-hidden="true">46.</strong> FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity</a></li><li class="chapter-item expanded "><a href="160.html"><strong aria-hidden="true">47.</strong> POMP: Physics-consistent Motion Generative Model through Phase Manifolds</a></li><li class="chapter-item expanded "><a href="159.html"><strong aria-hidden="true">48.</strong> SinGS: Animatable Single-Image Human Gaussian Splats with Kinematic Priors</a></li><li class="chapter-item expanded "><a href="158.html"><strong aria-hidden="true">49.</strong> SplArt: Articulation Estimation and Part-Level Reconstruction with 3D Gaussian Splatting</a></li><li class="chapter-item expanded "><a href="157.html"><strong aria-hidden="true">50.</strong> HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting</a></li><li class="chapter-item expanded "><a href="156.html"><strong aria-hidden="true">51.</strong> Multimodal Generative AI with Autoregressive LLMs for Human Motion Understanding and Generation: A Way Forward</a></li><li class="chapter-item expanded "><a href="155.html"><strong aria-hidden="true">52.</strong> LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering</a></li><li class="chapter-item expanded "><a href="154.html"><strong aria-hidden="true">53.</strong> EnliveningGS: Active Locomotion of 3DGS</a></li><li class="chapter-item expanded "><a href="153.html"><strong aria-hidden="true">54.</strong> AniMo: Species-Aware Model for Text-Driven Animal Motion Generation</a></li><li class="chapter-item expanded "><a href="152.html"><strong aria-hidden="true">55.</strong> Controllable Human-centric Keyframe Interpolation with Generative Prior</a></li><li class="chapter-item expanded "><a href="151.html"><strong aria-hidden="true">56.</strong> SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios</a></li><li class="chapter-item expanded "><a href="150.html"><strong aria-hidden="true">57.</strong> HOSIG: Full-Body Human-Object-Scene Interaction Generation with Hierarchical Scene Perception</a></li><li class="chapter-item expanded "><a href="149.html"><strong aria-hidden="true">58.</strong> DiffuseSlide: Training-Free High Frame Rate Video Generation Diffusion </a></li><li class="chapter-item expanded "><a href="148.html"><strong aria-hidden="true">59.</strong> UMA: Ultra-detailed Human Avatars via Multi-level Surface Alignment</a></li><li class="chapter-item expanded "><a href="147.html"><strong aria-hidden="true">60.</strong> FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video Generation</a></li><li class="chapter-item expanded "><a href="146.html"><strong aria-hidden="true">61.</strong> TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans</a></li><li class="chapter-item expanded "><a href="145.html"><strong aria-hidden="true">62.</strong> UniGeo: Taming Video Diffusion for Unified Consistent Geometry Estimation</a></li><li class="chapter-item expanded "><a href="144.html"><strong aria-hidden="true">63.</strong> LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework</a></li><li class="chapter-item expanded "><a href="143.html"><strong aria-hidden="true">64.</strong> DreamDance: Animating Character Art via Inpainting Stable Gaussian Worlds</a></li><li class="chapter-item expanded "><a href="142.html"><strong aria-hidden="true">65.</strong> AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion</a></li><li class="chapter-item expanded "><a href="141.html"><strong aria-hidden="true">66.</strong> Generating Fit Check Videos with a Handheld Camera</a></li><li class="chapter-item expanded "><a href="140.html"><strong aria-hidden="true">67.</strong> GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion</a></li><li class="chapter-item expanded "><a href="139.html"><strong aria-hidden="true">68.</strong> MMGT: Motion Mask Guided Two-Stage Network for Co-Speech Gesture Video Generation</a></li><li class="chapter-item expanded "><a href="138.html"><strong aria-hidden="true">69.</strong> ATI: Any Trajectory Instruction for Controllable Video Generation</a></li><li class="chapter-item expanded "><a href="137.html"><strong aria-hidden="true">70.</strong> Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference Optimization and Temporal Motion Modulation</a></li><li class="chapter-item expanded "><a href="136.html"><strong aria-hidden="true">71.</strong> HyperMotion: DiT-Based Pose-Guided Human Image Animation of Complex Motions</a></li><li class="chapter-item expanded "><a href="135.html"><strong aria-hidden="true">72.</strong> LatentMove: Towards Complex Human Movement Video Generation </a></li><li class="chapter-item expanded "><a href="134.html"><strong aria-hidden="true">73.</strong> UniMoGen: Universal Motion Generation</a></li><li class="chapter-item expanded "><a href="133.html"><strong aria-hidden="true">74.</strong> Diffusion Model-based Activity Completion for AI Motion Capture from Videos</a></li><li class="chapter-item expanded "><a href="131.html"><strong aria-hidden="true">75.</strong> How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control</a></li><li class="chapter-item expanded "><a href="130.html"><strong aria-hidden="true">76.</strong> IKMo: Image-Keyframed Motion Generation with Trajectory-Pose Conditioned Motion Diffusion Model</a></li><li class="chapter-item expanded "><a href="129.html"><strong aria-hidden="true">77.</strong> AniCrafter: Customizing Realistic Human-Centric Animation via Avatar-Background Conditioning in Video Diffusion Models</a></li><li class="chapter-item expanded "><a href="128.html"><strong aria-hidden="true">78.</strong> ParticleGS: Particle-Based Dynamics Modeling of 3D Gaussians for Prior-free Motion Extrapolation</a></li><li class="chapter-item expanded "><a href="127.html"><strong aria-hidden="true">79.</strong> MotionPro: A Precise Motion Controller for Image-to-Video Generation</a></li><li class="chapter-item expanded "><a href="126.html"><strong aria-hidden="true">80.</strong> DIPO: Dual-State Images Controlled Articulated Object Generation Powered by Diverse Data</a></li><li class="chapter-item expanded "><a href="125.html"><strong aria-hidden="true">81.</strong> Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals</a></li><li class="chapter-item expanded "><a href="124.html"><strong aria-hidden="true">82.</strong> PAMD: Plausibility-Aware Motion Diffusion Model for Long Dance Generation</a></li><li class="chapter-item expanded "><a href="123.html"><strong aria-hidden="true">83.</strong> Absolute Coordinates Make Motion Generation Easy</a></li><li class="chapter-item expanded "><a href="122.html"><strong aria-hidden="true">84.</strong> Flow Matching for Geometric Trajectory Simulation</a></li><li class="chapter-item expanded "><a href="121.html"><strong aria-hidden="true">85.</strong> Pose Splatter: A 3D Gaussian Splatting Model for Quantifying Animal Pose and Appearance</a></li><li class="chapter-item expanded "><a href="120.html"><strong aria-hidden="true">86.</strong> CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesis</a></li><li class="chapter-item expanded "><a href="119.html"><strong aria-hidden="true">87.</strong> Multi-Person Interaction Generation from Two-Person Motion Priors</a></li><li class="chapter-item expanded "><a href="118.html"><strong aria-hidden="true">88.</strong> DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation</a></li><li class="chapter-item expanded "><a href="117.html"><strong aria-hidden="true">89.</strong> Temporal Differential Fields for 4D Motion Modeling via Image-to-Video Synthesis</a></li><li class="chapter-item expanded "><a href="116.html"><strong aria-hidden="true">90.</strong> WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions</a></li><li class="chapter-item expanded "><a href="115.html"><strong aria-hidden="true">91.</strong> Motion Matters: Compact Gaussian Streaming for Free-Viewpoint Video Reconstruction</a></li><li class="chapter-item expanded "><a href="114.html"><strong aria-hidden="true">92.</strong> Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose Interaction</a></li><li class="chapter-item expanded "><a href="113.html"><strong aria-hidden="true">93.</strong> SHaDe: Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane Deformation and Latent Diffusion</a></li><li class="chapter-item expanded "><a href="112.html"><strong aria-hidden="true">94.</strong> MEgoHand: Multimodal Egocentric Hand-Object Interaction Motion Generation</a></li><li class="chapter-item expanded "><a href="111.html"><strong aria-hidden="true">95.</strong> MAGIC: Motion-Aware Generative Inference via Confidence-Guided LLM</a></li><li class="chapter-item expanded "><a href="108.html"><strong aria-hidden="true">96.</strong> GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream Generation</a></li><li class="chapter-item expanded "><a href="107.html"><strong aria-hidden="true">97.</strong> EVA: Expressive Virtual Avatars from Multi-view Videos</a></li><li class="chapter-item expanded "><a href="105.html"><strong aria-hidden="true">98.</strong> Large-Scale Multi-Character Interaction Synthesis</a></li><li class="chapter-item expanded "><a href="104.html"><strong aria-hidden="true">99.</strong> Hunyuan-Game: Industrial-grade Intelligent Game Creation Model</a></li><li class="chapter-item expanded "><a href="103.html"><strong aria-hidden="true">100.</strong> Vid2World: Crafting Video Diffusion Models to Interactive World Models</a></li><li class="chapter-item expanded "><a href="102.html"><strong aria-hidden="true">101.</strong> MGStream: Motion-aware 3D Gaussian for Streamable Dynamic Scene Reconstruction</a></li><li class="chapter-item expanded "><a href="101.html"><strong aria-hidden="true">102.</strong> LMP: Leveraging Motion Prior in Zero-Shot Video Generation with Diffusion Transformer</a></li><li class="chapter-item expanded "><a href="100.html"><strong aria-hidden="true">103.</strong> Video-GPT via Next Clip Diffusion</a></li><li class="chapter-item expanded "><a href="99.html"><strong aria-hidden="true">104.</strong> MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos</a></li><li class="chapter-item expanded "><a href="98.html"><strong aria-hidden="true">105.</strong> RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers</a></li><li class="chapter-item expanded "><a href="97.html"><strong aria-hidden="true">106.</strong> UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes</a></li><li class="chapter-item expanded "><a href="95.html"><strong aria-hidden="true">107.</strong> PoseBench3D: A Cross-Dataset Analysis Framework for 3D Human Pose Estimation</a></li><li class="chapter-item expanded "><a href="94.html"><strong aria-hidden="true">108.</strong> Robust Photo-Realistic Hand Gesture Generation: from Single View to Multiple View</a></li><li class="chapter-item expanded "><a href="93.html"><strong aria-hidden="true">109.</strong> Infinigen-Sim: Procedural Generation of Articulated Simulation Assets</a></li><li class="chapter-item expanded "><a href="90.html"><strong aria-hidden="true">110.</strong> Locality Sensitive Avatars From Video</a></li><li class="chapter-item expanded "><a href="89.html"><strong aria-hidden="true">111.</strong> Dyadic Mamba: Long-term Dyadic Human Motion Synthesis</a></li><li class="chapter-item expanded "><a href="86.html"><strong aria-hidden="true">112.</strong> MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation</a></li><li class="chapter-item expanded "><a href="85.html"><strong aria-hidden="true">113.</strong> TexTailor: Customized Text-aligned Texturing via Effective Resampling</a></li><li class="chapter-item expanded "><a href="82.html"><strong aria-hidden="true">114.</strong> CameraCtrl: Enabling Camera Control for Video Diffusion Models</a></li><li class="chapter-item expanded "><a href="81.html"><strong aria-hidden="true">115.</strong> ACT-R: Adaptive Camera Trajectories for 3D Reconstruction from Single Image</a></li><li class="chapter-item expanded "><a href="80.html"><strong aria-hidden="true">116.</strong> M3G: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human Motion Synthesis</a></li><li class="chapter-item expanded "><a href="79.html"><strong aria-hidden="true">117.</strong> SplineGS: Learning Smooth Trajectories in Gaussian Splatting for Dynamic Scene Reconstruction</a></li><li class="chapter-item expanded "><a href="78.html"><strong aria-hidden="true">118.</strong> CyberHost: A One-stage Diffusion Framework for Audio-driven Talking Body Generation</a></li><li class="chapter-item expanded "><a href="76.html"><strong aria-hidden="true">119.</strong> Sketch2Anim: Towards Transferring Sketch Storyboards into 3D Animation</a></li><li class="chapter-item expanded "><a href="75.html"><strong aria-hidden="true">120.</strong> Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured 3D Assets</a></li><li class="chapter-item expanded "><a href="74.html"><strong aria-hidden="true">121.</strong> Link to the Past: Temporal Propagation for Fast 3D Human Reconstruction from Monocular Video</a></li><li class="chapter-item expanded "><a href="73.html"><strong aria-hidden="true">122.</strong> ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models</a></li><li class="chapter-item expanded "><a href="72.html"><strong aria-hidden="true">123.</strong> Human Motion Prediction via Test-domain-aware Adaptation with Easily-available Human Motions Estimated from Videos</a></li><li class="chapter-item expanded "><a href="71.html"><strong aria-hidden="true">124.</strong> MAGE:A Multi-stage Avatar Generator with Sparse Observations</a></li><li class="chapter-item expanded "><a href="70.html"><strong aria-hidden="true">125.</strong> DiffLocks: Generating 3D Hair from a Single Image using Diffusion Models</a></li><li class="chapter-item expanded "><a href="69.html"><strong aria-hidden="true">126.</strong> TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head Modeling</a></li><li class="chapter-item expanded "><a href="68.html"><strong aria-hidden="true">127.</strong> Anymate: A Dataset and Baselines for Learning 3D Object Rigging</a></li><li class="chapter-item expanded "><a href="67.html"><strong aria-hidden="true">128.</strong> ReactDance: Progressive-Granular Representation for Long-Term Coherent Reactive Dance Generation</a></li><li class="chapter-item expanded "><a href="66.html"><strong aria-hidden="true">129.</strong> Apply Hierarchical-Chain-of-Generation to Complex Attributes Text-to-3D Generation</a></li><li class="chapter-item expanded "><a href="65.html"><strong aria-hidden="true">130.</strong> Humanise: Language-conditioned human motion generation in 3d scenes</a></li><li class="chapter-item expanded "><a href="64.html"><strong aria-hidden="true">131.</strong> Synthesizing Diverse Human Motions in 3D Indoor Scenes</a></li><li class="chapter-item expanded "><a href="63.html"><strong aria-hidden="true">132.</strong> Move as You Say, Interact as You Can:Language-guided Human Motion Generation with Scene Affordance</a></li><li class="chapter-item expanded "><a href="62.html"><strong aria-hidden="true">133.</strong> 3D Scene Generation: A Survey</a></li><li class="chapter-item expanded "><a href="61.html"><strong aria-hidden="true">134.</strong> SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with Video Diffusion and Data Augmentation</a></li><li class="chapter-item expanded "><a href="59.html"><strong aria-hidden="true">135.</strong> ReAlign: Bilingual Text-to-Motion Generation via Step-Aware Reward-Guided Alignment</a></li><li class="chapter-item expanded "><a href="58.html"><strong aria-hidden="true">136.</strong> MeshGen: Generating PBR Textured Mesh with Render-Enhanced Auto-Encoder and Generative Data Augmentation</a></li><li class="chapter-item expanded "><a href="57.html"><strong aria-hidden="true">137.</strong> TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization</a></li><li class="chapter-item expanded "><a href="56.html"><strong aria-hidden="true">138.</strong> ELGAR: Expressive Cello Performance Motion Generation for Audio Rendition</a></li><li class="chapter-item expanded "><a href="55.html"><strong aria-hidden="true">139.</strong> PARC: Physics-based Augmentation with Reinforcement Learning for Character Controllers</a></li><li class="chapter-item expanded "><a href="54.html"><strong aria-hidden="true">140.</strong> Person-In-Situ: Scene-Consistent Human Image Insertion with Occlusion-Aware Pose Control</a></li><li class="chapter-item expanded "><a href="53.html"><strong aria-hidden="true">141.</strong> PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with Auto-Regressive Transformer</a></li><li class="chapter-item expanded "><a href="52.html"><strong aria-hidden="true">142.</strong> Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting</a></li><li class="chapter-item expanded "><a href="51.html"><strong aria-hidden="true">143.</strong> FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios</a></li><li class="chapter-item expanded "><a href="50.html"><strong aria-hidden="true">144.</strong> Real-Time Person Image Synthesis Using a Flow Matching Model</a></li><li class="chapter-item expanded "><a href="49.html"><strong aria-hidden="true">145.</strong> Polar Coordinate-Based 2D Pose Prior with Neural Distance Field</a></li><li class="chapter-item expanded "><a href="48.html"><strong aria-hidden="true">146.</strong> PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model</a></li><li class="chapter-item expanded "><a href="47.html"><strong aria-hidden="true">147.</strong> GUAVA: Generalizable Upper Body 3D Gaussian Avatar</a></li><li class="chapter-item expanded "><a href="46.html"><strong aria-hidden="true">148.</strong> DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization</a></li><li class="chapter-item expanded "><a href="45.html"><strong aria-hidden="true">149.</strong> SignSplat: Rendering Sign Language via Gaussian Splatting</a></li><li class="chapter-item expanded "><a href="44.html"><strong aria-hidden="true">150.</strong> MVHumanNet++: A Large-scale Dataset of Multi-view Daily Dressing Human Captures with Richer Annotations for 3D Human Digitization </a></li><li class="chapter-item expanded "><a href="43.html"><strong aria-hidden="true">151.</strong> Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion</a></li><li class="chapter-item expanded "><a href="42.html"><strong aria-hidden="true">152.</strong> Efficient 3D Full-Body Motion Generation from Sparse Tracking Inputs with Temporal Windows</a></li><li class="chapter-item expanded "><a href="41.html"><strong aria-hidden="true">153.</strong> Model See Model Do: Speech-Driven Facial Animation with Style Control</a></li><li class="chapter-item expanded "><a href="40.html"><strong aria-hidden="true">154.</strong> 3D Human Pose Estimation via Spatial Graph Order Attention and Temporal Body Aware Transformer</a></li><li class="chapter-item expanded "><a href="36.html"><strong aria-hidden="true">155.</strong> JointDiT: Enhancing RGB-Depth Joint Modeling with Diffusion Transformers</a></li><li class="chapter-item expanded "><a href="35.html"><strong aria-hidden="true">156.</strong> Eye2Eye: A Simple Approach for Monocular-to-Stereo Video Synthesis</a></li><li class="chapter-item expanded "><a href="34.html"><strong aria-hidden="true">157.</strong> T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation</a></li><li class="chapter-item expanded "><a href="33.html"><strong aria-hidden="true">158.</strong> Direct Motion Models for Assessing Generated Videos</a></li><li class="chapter-item expanded "><a href="32.html"><strong aria-hidden="true">159.</strong> Real-Time Animatable 2DGS-Avatars with Detail Enhancement from Monocular Videos</a></li><li class="chapter-item expanded "><a href="31.html"><strong aria-hidden="true">160.</strong> CoCoDiff: Diversifying Skeleton Action Features via Coarse-Fine Text-Co-Guided Latent Diffusion</a></li><li class="chapter-item expanded "><a href="30.html"><strong aria-hidden="true">161.</strong> Common3D: Self-Supervised Learning of 3D Morphable Models for Common Objects in Neural Feature Space</a></li><li class="chapter-item expanded "><a href="29.html"><strong aria-hidden="true">162.</strong> HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation</a></li><li class="chapter-item expanded "><a href="28.html"><strong aria-hidden="true">163.</strong> MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance</a></li><li class="chapter-item expanded "><a href="27.html"><strong aria-hidden="true">164.</strong> ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction</a></li><li class="chapter-item expanded "><a href="25.html"><strong aria-hidden="true">165.</strong> Efficient Listener: Dyadic Facial Motion Synthesis via Action Diffusion</a></li><li class="chapter-item expanded "><a href="24.html"><strong aria-hidden="true">166.</strong> Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting</a></li><li class="chapter-item expanded "><a href="23.html"><strong aria-hidden="true">167.</strong> SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from Gameplay Recordings</a></li><li class="chapter-item expanded "><a href="22.html"><strong aria-hidden="true">168.</strong> TesserAct: Learning 4D Embodied World Models</a></li><li class="chapter-item expanded "><a href="21.html"><strong aria-hidden="true">169.</strong> EfficientHuman: Efficient Training and Reconstruction of Moving Human using Articulated 2D Gaussian</a></li><li class="chapter-item expanded "><a href="20.html"><strong aria-hidden="true">170.</strong> Joint Optimization of Neural Radiance Fields and Continuous Camera Motion from a Monocular Video</a></li><li class="chapter-item expanded "><a href="19.html"><strong aria-hidden="true">171.</strong> Sketch2Anim: Towards Transferring Sketch Storyboards into 3D Animation</a></li><li class="chapter-item expanded "><a href="18.html"><strong aria-hidden="true">172.</strong> HumMorph: Generalized Dynamic Human Neural Fields from Few Views</a></li><li class="chapter-item expanded "><a href="17.html"><strong aria-hidden="true">173.</strong> AnimateAnywhere: Rouse the Background in Human Image Animation</a></li><li class="chapter-item expanded "><a href="16.html"><strong aria-hidden="true">174.</strong> Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions</a></li><li class="chapter-item expanded "><a href="15.html"><strong aria-hidden="true">175.</strong> ActionArt: Advancing Multimodal Large Models for Fine-Grained Human-Centric Video Understanding</a></li><li class="chapter-item expanded "><a href="14.html"><strong aria-hidden="true">176.</strong> SmallGS: Gaussian Splatting-based Camera Pose Estimation for Small-Baseline Videos</a></li><li class="chapter-item expanded "><a href="13.html"><strong aria-hidden="true">177.</strong> STP4D: Spatio-Temporal-Prompt Consistent Modeling for Text-to-4D Gaussian Splatting</a></li><li class="chapter-item expanded "><a href="12.html"><strong aria-hidden="true">178.</strong> Unify3D: An Augmented Holistic End-to-end Monocular 3D Human Reconstruction via Anatomy Shaping and Twins Negotiating</a></li><li class="chapter-item expanded "><a href="11.html"><strong aria-hidden="true">179.</strong> SSD-Poser: Avatar Pose Estimation with State Space Duality from Sparse Observations</a></li><li class="chapter-item expanded "><a href="10.html"><strong aria-hidden="true">180.</strong> PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation</a></li><li class="chapter-item expanded "><a href="9.html"><strong aria-hidden="true">181.</strong> Gaussian Splatting is an Effective Data Generator for 3D Object Detection</a></li><li class="chapter-item expanded "><a href="8.html"><strong aria-hidden="true">182.</strong> HUG: Hierarchical Urban Gaussian Splatting with Block-Based Reconstruction</a></li><li class="chapter-item expanded "><a href="7.html"><strong aria-hidden="true">183.</strong> Physically Consistent Humanoid Loco-Manipulation using Latent Diffusion Models</a></li><li class="chapter-item expanded "><a href="6.html"><strong aria-hidden="true">184.</strong> PMG: Progressive Motion Generation via Sparse Anchor Postures Curriculum Learning</a></li><li class="chapter-item expanded "><a href="5.html"><strong aria-hidden="true">185.</strong> Unveiling Hidden Vulnerabilities in Digital Human Generation via Adversarial Attacks</a></li><li class="chapter-item expanded "><a href="4.html"><strong aria-hidden="true">186.</strong> PICO: Reconstructing 3D People In Contact with Objects</a></li><li class="chapter-item expanded "><a href="3.html"><strong aria-hidden="true">187.</strong> Bolt: Clothing Virtual Characters at Scale</a></li><li class="chapter-item expanded "><a href="2.html"><strong aria-hidden="true">188.</strong> Dynamic Camera Poses and Where to Find Them</a></li><li class="chapter-item expanded "><a href="1.html"><strong aria-hidden="true">189.</strong> 3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light (default)</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Latest Paper</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/CaterpillarStudyGroup/LatestPapers" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>
                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>
                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script type="text/javascript">
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main><div class="sidetoc"><nav class="pagetoc"></nav></div>
                        <table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2025.6.18</td><td>Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D Rewards</td><td></td><td></td><td><a href="205.html">link</a></td></tr>
<tr><td></td><td>2025.6.18</td><td>Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material</td><td></td><td></td><td><a href="204.html">link</a></td></tr>
<tr><td></td><td>2025.6.18</td><td>Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos</td><td></td><td></td><td><a href="203.html">link</a></td></tr>
<tr><td></td><td>2025.6.18</td><td>HOIDiNi: Human-Object Interaction through Diffusion Noise Optimization</td><td></td><td></td><td><a href="202.html">link</a></td></tr>
<tr><td></td><td>2025.6.18</td><td>GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis for Unseen Objects</td><td></td><td></td><td><a href="201.html">link</a></td></tr>
<tr><td></td><td>2025.6.17</td><td>VideoMAR: Autoregressive Video Generatio with Continuous Tokens</td><td></td><td></td><td><a href="200.html">link</a></td></tr>
<tr><td></td><td>2025.6.17</td><td>SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting</td><td></td><td></td><td><a href="199.html">link</a></td></tr>
<tr><td></td><td>2025.6.17</td><td>GAF: Gaussian Action Field as a Dvnamic World Model for Robotic Mlanipulation</td><td></td><td></td><td><a href="198.html">link</a></td></tr>
<tr><td></td><td>2025.6.17</td><td>GMT: General Motion Tracking for Humanoid Whole-Body Control</td><td></td><td></td><td><a href="197.html">link</a></td></tr>
<tr><td></td><td>2025.6.17</td><td>Toward Rich Video Human-Motion2D Generation</td><td></td><td></td><td><a href="196.html">link</a></td></tr>
<tr><td></td><td>2025.6.16</td><td>PF-LHM: 3D Animatable Avatar Reconstruction from Pose-free Articulated Human Images</td><td></td><td></td><td><a href="195.html">link</a></td></tr>
<tr><td></td><td>2025.6.16</td><td>MAMMA: Markerless &amp; Automatic Multi-Person Motion Action Capture</td><td></td><td></td><td><a href="194.html">link</a></td></tr>
<tr><td></td><td>2025.6.15</td><td>iDiT-HOI: Inpainting-based Hand Object Interaction Reenactment via Video Diffusion Transformer</td><td></td><td></td><td><a href="193.html">link</a></td></tr>
<tr><td></td><td>2025.6.15</td><td>RL from Physical Feedback: Aligning Large Motion Models with Humanoid Control</td><td></td><td></td><td><a href="192.html">link</a></td></tr>
<tr><td></td><td>2025.6.15</td><td>KungfuBot: Physics-Based Humanoid Whole-Body Control for Learning Highly-Dynamic Skills</td><td></td><td></td><td><a href="191.html">link</a></td></tr>
<tr><td></td><td>2025.6.13</td><td>Enter: Graduated Realism: A Pedagogical Framework for AI-Powered Avatars in Virtual Reality Teacher Training</td><td></td><td></td><td><a href="190.html">link</a></td></tr>
<tr><td></td><td>2025.6.13</td><td>Auto-Connect: Connectivity-Preserving RigFormer with Direct Preference Optimization</td><td></td><td></td><td><a href="189.html">link</a></td></tr>
<tr><td></td><td>2025.6.13</td><td>SignAligner: Harmonizing Complementary Pose Modalities for Coherent Sign Language Generation</td><td></td><td></td><td><a href="188.html">link</a></td></tr>
<tr><td></td><td>2025.6.13</td><td>Aligned Novel View Image and Geometry Synthesis via Cross-modal Attention Instillation</td><td></td><td></td><td><a href="187.html">link</a></td></tr>
<tr><td></td><td>2025.6.12</td><td>DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers</td><td></td><td></td><td><a href="185.html">link</a></td></tr>
<tr><td></td><td>2025.6.12</td><td>M4V: Multi-Modal Mamba for Text-to-Video Generation</td><td></td><td></td><td><a href="184.html">link</a></td></tr>
<tr><td></td><td>2025.6.12</td><td>Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for Human Motion Generation</td><td></td><td></td><td><a href="183.html">link</a></td></tr>
<tr><td></td><td>2025.6.12</td><td>Rethinking Generative Human Video Coding with Implicit Motion Transformation</td><td></td><td></td><td><a href="182.html">link</a></td></tr>
<tr><td></td><td>2025.6.11</td><td>AlignHuman: Improving Motion and Fidelity via Timestep-Segment Preference Optimization for Audio-Driven Human Animation</td><td></td><td></td><td><a href="186.html">link</a></td></tr>
<tr><td></td><td>2025.6.11</td><td>PlayerOne: Egocentric World Simulator</td><td></td><td></td><td><a href="180.html">link</a></td></tr>
<tr><td></td><td>2025.6.11</td><td>Self-Supervised Multi-Part Articulated Objects Modeling via Deformable Gaussian Splatting and Progressive Primitive Segmentation</td><td></td><td></td><td><a href="177.html">link</a></td></tr>
<tr><td></td><td>2025.6.10</td><td>RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping</td><td></td><td></td><td><a href="175.html">link</a></td></tr>
<tr><td></td><td>2025.6.10</td><td>StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams</td><td></td><td></td><td><a href="174.html">link</a></td></tr>
<tr><td></td><td>2025.6.10</td><td>Orientation Matters: Making 3D Generative Models Orientation-Aligned</td><td></td><td></td><td><a href="173.html">link</a></td></tr>
<tr><td></td><td>2025.6.10</td><td>Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos</td><td></td><td></td><td><a href="172.html">link</a></td></tr>
<tr><td></td><td>2025.6.10</td><td><strong>HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation</strong></td><td></td><td></td><td><a href="171.html">link</a></td></tr>
<tr><td></td><td>2025.6.9</td><td>SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach</td><td></td><td></td><td><a href="176.html">link</a></td></tr>
<tr><td></td><td>2025.6.9</td><td>NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D Generation</td><td></td><td></td><td><a href="168.html">link</a></td></tr>
<tr><td></td><td>2025.6.9</td><td>PhysiInter: Integrating Physical Mapping for High-Fidelity Human Interaction Generation</td><td></td><td></td><td><a href="167.html">link</a></td></tr>
<tr><td></td><td>2025.6.7</td><td>Parametric Gaussian Human Model: Generalizable Prior for Efficient and Realistic Human Avatar Modeling</td><td></td><td></td><td><a href="166.html">link</a></td></tr>
<tr><td></td><td>2025.6.5</td><td>Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting </td><td></td><td></td><td><a href="165.html">link</a></td></tr>
<tr><td></td><td>2025.6.5</td><td>Follow-Your-Creation: Empowering 4D Creation through Video Inpainting</td><td></td><td></td><td><a href="164.html">link</a></td></tr>
<tr><td></td><td>2025.6.5</td><td>Realizing Text-Driven Motion Generation on NAO Robot: A Reinforcement Learning-Optimized Control Pipeline</td><td></td><td></td><td><a href="163.html">link</a></td></tr>
<tr><td></td><td>2025.6.5</td><td>Follow-Your-Motion: Video Motion Transfer via Efficient Spatial-Temporal Decoupled Finetuning</td><td></td><td></td><td><a href="162.html">link</a></td></tr>
<tr><td></td><td>2025.6.5</td><td><strong>FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity</strong></td><td></td><td></td><td><a href="161.html">link</a></td></tr>
<tr><td></td><td>2025.6.4</td><td>SplArt: Articulation Estimation and Part-Level Reconstruction with 3D Gaussian Splatting</td><td></td><td></td><td><a href="158.html">link</a></td></tr>
<tr><td></td><td>2025.6.4</td><td>HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting</td><td></td><td></td><td><a href="157.html">link</a></td></tr>
<tr><td></td><td>2025.6.3</td><td>Controllable Human-centric Keyframe Interpolation with Generative Prior</td><td></td><td>3D关键帧插值器</td><td><a href="152.html">link</a></td></tr>
<tr><td></td><td>2025.6.3</td><td>SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios</td><td></td><td>将<strong>视觉先验</strong>和<strong>动态约束</strong>结合在一个<strong>同步扩散过程</strong>中，以同时生成HOI视频和运动数据</td><td><a href="151.html">link</a></td></tr>
<tr><td></td><td>2025.6.2</td><td>HOSIG: Full-Body Human-Object-Scene Interaction Generation with Hierarchical Scene Perception</td><td></td><td>通过<strong>分层场景感知</strong>合成全身交互</td><td><a href="150.html">link</a></td></tr>
<tr><td></td><td>2025.6.2</td><td>UMA: Ultra-detailed Human Avatars via Multi-level Surface Alignment</td><td></td><td>基于多视角视频学习具有生动动态效果与照片级真实感的可动画着装人体模型</td><td><a href="148.html">link</a></td></tr>
<tr><td></td><td>2025.5.31</td><td>Multimodal Generative AI with Autoregressive LLMs for Human Motion Understanding and Generation: A Way Forward</td><td></td><td></td><td><a href="156.html">link</a></td></tr>
<tr><td></td><td>2025.5.30</td><td>DreamDance: Animating Character Art via Inpainting Stable Gaussian Worlds</td><td></td><td>基于精确的相机轨迹生成稳定、一致的角色与场景运动</td><td><a href="143.html">link</a></td></tr>
<tr><td></td><td>2025.5.29</td><td>Generating Fit Check Videos with a Handheld Camera</td><td></td><td>仅需手持移动设备即可实现全身视频捕捉</td><td><a href="141.html">link</a></td></tr>
<tr><td></td><td>2025.5.29</td><td>Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference Optimization and Temporal Motion Modulation</td><td></td><td>由音频和骨骼运动驱动的高度动态且具有照片级真实感的肖像动画</td><td><a href="137.html">link</a></td></tr>
<tr><td></td><td>2025.5.29</td><td>HyperMotion: DiT-Based Pose-Guided Human Image Animation of Complex Motions</td><td>高质量的人体姿态标注和精选的视频片段</td><td>数据集</td><td><a href="136.html">link</a></td></tr>
<tr><td></td><td>2025.5.27</td><td>Diffusion Model-based Activity Completion for AI Motion Capture from Videos</td><td></td><td>将AI动作捕捉应用于<strong>虚拟人</strong>领域</td><td><a href="133.html">link</a></td></tr>
<tr><td></td><td>2025.5.26</td><td>AniCrafter: Customizing Realistic Human-Centric Animation via Avatar-Background Conditioning in Video Diffusion Models</td><td></td><td>将给定角色集成并动画化到开放域的动态背景中，同时遵循给定的人体运动序列</td><td><a href="129.html">link</a></td></tr>
<tr><td></td><td>2025.5.26</td><td>ParticleGS: Particle-Based Dynamics Modeling of 3D Gaussians for Prior-free Motion Extrapolation</td><td></td><td>根据视觉观测数据建模三维高斯分布的动力学特性</td><td><a href="128.html">link</a></td></tr>
<tr><td></td><td>2025.5.26</td><td>DIPO: Dual-State Images Controlled Articulated Object Generation Powered by Diverse Data</td><td></td><td>从一对图像可控生成铰接式三维物体</td><td><a href="126.html">link</a></td></tr>
<tr><td></td><td>2025.5.24</td><td>Flow Matching for Geometric Trajectory Simulation</td><td></td><td><strong>通过融合流匹配技术和数据依赖耦合机制实现了基于物理知识的几何轨迹模拟</strong></td><td><a href="122.html">link</a></td></tr>
<tr><td></td><td>2025.5.23</td><td>How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control</td><td></td><td>大语言模型文生动作评估</td><td><a href="131.html">link</a></td></tr>
<tr><td></td><td>2025.5.23</td><td>CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesis</td><td></td><td>一种新颖的3D高斯泼溅GAN框架。它能够在无需依赖视角条件控制的情况下，实现稳定的训练和高质量、3D一致的人头像合成</td><td><a href="120.html">link</a></td></tr>
<tr><td></td><td>2025.5.23</td><td>DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation</td><td></td><td>将<strong>单张参考图像</strong>加上<strong>独立的姿态掩膜流</strong>转化为<strong>长时、逼真的视频</strong>，同时<strong>严格保持每个角色的身份特征</strong></td><td><a href="118.html">link</a></td></tr>
<tr><td></td><td>2025.5.23</td><td>WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions</td><td></td><td>将物理模拟与视频生成技术相结合，能够基于单张图像生成动作条件化的动态三维场景</td><td><a href="116.html">link</a></td></tr>
<tr><td></td><td>2025.5.22</td><td>Motion Matters: Compact Gaussian Streaming for Free-Viewpoint Video Reconstruction</td><td></td><td>利用动态场景中运动的<strong>局部性</strong>和<strong>一致性</strong>，通过<strong>关键点驱动的运动表示</strong>来建模具有<strong>物体一致性的高斯点运动</strong>。通过仅传输关键点属性，该框架提供了一种更节省存储空间的解决方案。</td><td><a href="115.html">link</a></td></tr>
<tr><td></td><td>2025.5.22</td><td>Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose Interaction</td><td></td><td>利用扩散模型深入研究视频虚拟试穿中的动态姿态交互</td><td><a href="114.html">link</a></td></tr>
<tr><td></td><td>2025.5.22</td><td>SHaDe: Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane Deformation and Latent Diffusion</td><td></td><td>一种新颖的动态3D场景重建框架</td><td><a href="113.html">link</a></td></tr>
<tr><td></td><td>2025.5.22</td><td>MAGIC: Motion-Aware Generative Inference via Confidence-Guided LLM</td><td></td><td><strong>免训练</strong>的、用于<strong>单图像物理属性推断</strong>与<strong>动态生成</strong>的框架</td><td><a href="111.html">link</a></td></tr>
<tr><td></td><td>2025.5.21</td><td>AsynFusion: Towards Asynchronous Latent Consistency Models for Decoupled Whole-Body Audio-Driven Avatars</td><td></td><td>全身音频驱动虚拟形象姿势与表情生成</td><td><a href="109.html">link</a></td></tr>
<tr><td></td><td>2025.5.21</td><td>GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream Generation</td><td></td><td>真实世界稀疏多视角RGB图像重建的数据集</td><td><a href="108.html">link</a></td></tr>
<tr><td></td><td>2025.5.21</td><td>EVA: Expressive Virtual Avatars from Multi-view Videos</td><td></td><td>一种演员专用、完全可控且表现力丰富的数字人框架，该系统在实现高保真实时渲染的同时，能够对表情、肢体动作和手势进行独立控制</td><td><a href="107.html">link</a></td></tr>
<tr><td></td><td>2025.5.20</td><td>Hunyuan-Game: Industrial-grade Intelligent Game Creation Model</td><td></td><td>专为游戏场景定制的图像生成模型和视频生成模型</td><td><a href="104.html">link</a></td></tr>
<tr><td></td><td>2025.5.20</td><td>MGStream: Motion-aware 3D Gaussian for Streamable Dynamic Scene Reconstruction</td><td></td><td>3DGS可流式传输的动态新视角合成(DNVS)</td><td><a href="102.html">link</a></td></tr>
<tr><td></td><td>2025.5.19</td><td>RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers</td><td></td><td>一种面向扩散变换器的免训练视频运动迁移方法</td><td><a href="98.html">link</a></td></tr>
<tr><td></td><td>2025.5.18</td><td>Video-GPT via Next Clip Diffusion</td><td></td><td>将视频视为描述视觉世界的新型&quot;语言&quot;</td><td><a href="100.html">link</a></td></tr>
<tr><td></td><td>2025.5.17</td><td>MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos</td><td></td><td>以零样本方式从单目视频中分析三维运动特性</td><td><a href="99.html">link</a></td></tr>
<tr><td></td><td>2025.5.16</td><td>Infinigen-Sim: Procedural Generation of Articulated Simulation Assets</td><td></td><td>Blender工具，可创建铰链资源</td><td><a href="93.html">link</a></td></tr>
<tr><td></td><td>2025.5.16</td><td>PoseBench3D: A Cross-Dataset Analysis Framework for 3D Human Pose Estimation</td><td></td><td>HPE的标准化测试环境</td><td><a href="95.html">link</a></td></tr>
<tr><td></td><td>2025.5.16</td><td>Locality Sensitive Avatars From Video</td><td></td><td>基于Nerf的HPE</td><td><a href="90.html">link</a></td></tr>
<tr><td></td><td>2025.5.15</td><td><strong>MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation</strong></td><td></td><td>直接建模原始三维运动序列(即4D运动)的人体图像动画框架</td><td><a href="86.html">link</a></td></tr>
<tr><td></td><td>2025.5.13</td><td><strong>M3G: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human Motion Synthesis</strong></td><td></td><td>个性化姿态的建模</td><td><a href="80.html">link</a></td></tr>
<tr><td></td><td>2025.5.12</td><td><strong>ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models</strong></td><td></td><td>文本到多镜头视频生成</td><td><a href="73.html">link</a></td></tr>
<tr><td></td><td>2025.5.12</td><td><strong>Human Motion Prediction via Test-domain-aware Adaptation with Easily-available Human Motions Estimated from Videos</strong></td><td></td><td>三维人体运动预测</td><td><a href="72.html">link</a></td></tr>
<tr><td></td><td>2025.5.9</td><td>MAGE:A Multi-stage Avatar Generator with Sparse Observations</td><td></td><td>从头戴式设备推断全身姿态</td><td><a href="71.html">link</a></td></tr>
<tr><td></td><td>2025.5.9</td><td>DiffLocks: Generating 3D Hair from a Single Image using Diffusion Models</td><td></td><td>单张图像重建3D头发</td><td><a href="70.html">link</a></td></tr>
<tr><td></td><td>2025.5.9</td><td>TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head Modeling</td><td></td><td>基于3DGS和稀疏体积（3D头部）重建与渲染</td><td><a href="69.html">link</a></td></tr>
<tr><td></td><td>2025.5.9</td><td>Anymate: A Dataset and Baselines for Learning 3D Object Rigging</td><td></td><td>蒙皮绑定数据集</td><td><a href="68.html">link</a></td></tr>
<tr><td></td><td>2025.5.8</td><td>SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with Video Diffusion and Data Augmentation</td><td></td><td>从单张图像创建高质量可驱动的3D人体</td><td><a href="61.html">link</a></td></tr>
<tr><td></td><td>2025.5.7</td><td>PARC: Physics-based Augmentation with Reinforcement Learning for Character Controllers</td><td></td><td>基于物理模拟增强与强化学习的角色控制框架，过机器学习与物理仿真技术迭代增强运动数据集，持续拓展地形穿越控制器的能力。</td><td><a href="55.html">link</a></td></tr>
<tr><td></td><td>2025.5.7</td><td>PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with Auto-Regressive Transformer</td><td></td><td>形状基元抽象重新定义为基元装配生成任务</td><td><a href="53.html">link</a></td></tr>
<tr><td></td><td>2025.5.7</td><td>Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting</td><td></td><td>SDS + 多视频耦合 -&gt; CDS</td><td><a href="52.html">link</a></td></tr>
<tr><td></td><td>2025.5.6</td><td>Polar Coordinate-Based 2D Pose Prior with Neural Distance Field</td><td></td><td>动捕</td><td><a href="49.html">link</a></td></tr>
<tr><td></td><td>2025.5.6</td><td>GUAVA: Generalizable Upper Body 3D Gaussian Avatar</td><td></td><td>人体模型、上半身高斯重建</td><td><a href="47.html">link</a></td></tr>
<tr><td></td><td>2025.5.4</td><td>SignSplat: Rendering Sign Language via Gaussian Splatting</td><td></td><td>3DGS，细微动作重建</td><td><a href="45.html">link</a></td></tr>
<tr><td></td><td>2025.5.3</td><td>MVHumanNet++: A Large-scale Dataset of Multi-view Daily Dressing Human Captures with Richer Annotations for 3D Human Digitization</td><td></td><td>数据集，多视角人体动作序列</td><td><a href="44.html">link</a></td></tr>
<tr><td></td><td>2025.5.2</td><td>3D Human Pose Estimation via Spatial Graph Order Attention and Temporal Body Aware Transformer</td><td></td><td>动捕</td><td><a href="40.html">link</a></td></tr>
<tr><td></td><td>2025.5.1</td><td>Direct Motion Models for Assessing Generated Videos</td><td></td><td>视频生成评价指标，物体交互，运动质量</td><td><a href="33.html">link</a></td></tr>
<tr><td></td><td>2025.5.1</td><td>Real-Time Animatable 2DGS-Avatars with Detail Enhancement from Monocular Videos</td><td></td><td>2DGS,SMPL,人体重建</td><td><a href="32.html">link</a></td></tr>
<tr><td></td><td>2025.4.30</td><td>CoCoDiff: Diversifying Skeleton Action Features via Coarse-Fine Text-Co-Guided Latent Diffusion</td><td></td><td>3D动作的特征多样性</td><td><a href="31.html">link</a></td></tr>
<tr><td></td><td>2025.4.30</td><td>Common3D: Self-Supervised Learning of 3D Morphable Models for Common Objects in Neural Feature Space</td><td></td><td>3DMM泛化到常见物体</td><td><a href="30.html">link</a></td></tr>
<tr><td></td><td>2025.4.30</td><td>HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation</td><td></td><td>4D场景生成</td><td><a href="29.html">link</a></td></tr>
<tr><td></td><td>2025.4.30</td><td>MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance</td><td></td><td>视频人脸驱动，FLAME</td><td><a href="28.html">link</a></td></tr>
<tr><td></td><td>2025.4.29</td><td>Efficient Listener: Dyadic Facial Motion Synthesis via Action Diffusion</td><td></td><td>Human Head Generation</td><td><a href="25.html">link</a></td></tr>
<tr><td></td><td>2025.4.29</td><td>Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting</td><td></td><td>生成可编辑3D数字人</td><td><a href="24.html">link</a></td></tr>
<tr><td></td><td>2025.4.29</td><td>SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from Gameplay Recordings</td><td></td><td>踢足球仿真</td><td><a href="23.html">link</a></td></tr>
<tr><td></td><td>2025.4.29</td><td>EfficientHuman: Efficient Training and Reconstruction of Moving Human using Articulated 2D Gaussian</td><td></td><td>2DGS重建运动人体</td><td><a href="21.html">link</a></td></tr>
<tr><td></td><td>2025.4.28</td><td>Joint Optimization of Neural Radiance Fields and Continuous Camera Motion from a Monocular Video</td><td></td><td>重建与相机运行联合优化</td><td><a href="20.html">link</a></td></tr>
<tr><td></td><td>2025.4.28</td><td>HumMorph: Generalized Dynamic Human Neural Fields from Few Views</td><td></td><td>动态人体自由视角渲染</td><td><a href="18.html">link</a></td></tr>
<tr><td></td><td>2025.4.27</td><td>Sketch2Anim: Towards Transferring Sketch Storyboards into 3D Animation</td><td></td><td></td><td><a href="76.html">link</a></td></tr>
<tr><td></td><td>2025.4.27</td><td>Sketch2Anim: Towards Transferring Sketch Storyboards into 3D Animation</td><td></td><td>二维故事板草图转化为三维动画</td><td><a href="19.html">link</a></td></tr>
<tr><td></td><td>2025.4.27</td><td>Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions</td><td></td><td><strong>角色动画综述</strong></td><td><a href="16.html">link</a></td></tr>
<tr><td></td><td>2025.4.25</td><td>SSD-Poser: Avatar Pose Estimation with State Space Duality from Sparse Observations</td><td></td><td>疏观测实现稳健的全身运动估计</td><td><a href="11.html">link</a></td></tr>
<tr><td></td><td>2025.4.25</td><td>STP4D: Spatio-Temporal-Prompt Consistent Modeling for Text-to-4D Gaussian Splatting</td><td></td><td><strong>文本到4D生成</strong></td><td><a href="13.html">link</a></td></tr>
<tr><td></td><td>2025.4.25</td><td>ActionArt: Advancing Multimodal Large Models for Fine-Grained Human-Centric Video Understanding</td><td></td><td>以人为中心的视频数据集</td><td><a href="15.html">link</a></td></tr>
<tr><td></td><td>2025.4.24</td><td>3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models</td><td></td><td>虚拟试衣</td><td><a href="1.html">link</a></td></tr>
<tr><td></td><td>2025.4.24</td><td>PICO: Reconstructing 3D People In Contact with Objects</td><td></td><td>人物交互3D重建</td><td><a href="4.html">link</a></td></tr>
<tr><td></td><td>2025.4.23</td><td>PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation</td><td></td><td>从视频数据中提取物理信息</td><td><a href="10.html">link</a></td></tr>
<tr><td></td><td>2025.4.14</td><td>Controllable Expressive 3D Facial Animation via Diffusion in a Unified Multimodal Space</td><td></td><td></td><td><a href="181.html">link</a></td></tr>
<tr><td></td><td>2025.4.3</td><td>TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization</td><td></td><td>人-场景交互(HSI)</td><td><a href="57.html">link</a></td></tr>
<tr><td></td><td>2025</td><td>Draganything: Motion control for anything using entity representation</td><td></td><td></td><td></td></tr>
<tr><td></td><td>2023</td><td>Synthesizing Diverse Human Motions in 3D Indoor Scenes</td><td></td><td>三维室内场景人物互动</td><td><a href="64.html">link</a></td></tr>
<tr><td></td><td>2022</td><td>Humanise: Language-conditioned human motion generation in 3d scenes</td><td></td><td>人-场景交互（HSI）数据集</td><td><a href="65.html">link</a></td></tr>
</tbody></table>
<h1 id="删除"><a class="header" href="#删除">删除</a></h1>
<h1 id="low"><a class="header" href="#low">Low</a></h1>
<table><thead><tr><th>ID</th><th>Year</th><th>Name</th><th>Note</th><th>Tags</th><th>Link</th></tr></thead><tbody>
<tr><td></td><td>2025.6.5</td><td><strong>POMP: Physics-consistent Motion Generative Model through Phase Manifolds</strong></td><td>1. 使用运动学生成动作 2. 使用动力学优化动作 3. 优化结果再映射回运动学数据 <br></td><td>物理合理，自回归，动作优化</td><td><a href="160.html">link</a></td></tr>
<tr><td></td><td>2025.6.3</td><td>LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering</td><td></td><td></td><td><a href="155.html">link</a></td></tr>
<tr><td></td><td>2025.5.30</td><td>AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion</td><td></td><td>从单张真实场景图像生成高保真可动画3D头像</td><td><a href="142.html">link</a></td></tr>
<tr><td></td><td>2025.5.30</td><td>UniGeo: Taming Video Diffusion for Unified Consistent Geometry Estimation</td><td></td><td>利用扩散模型先验知识辅助单目几何估计</td><td><a href="145.html">link</a></td></tr>
<tr><td></td><td>2025.5.30</td><td>LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework</td><td></td><td>条件化3D形状生成</td><td><a href="144.html">link</a></td></tr>
<tr><td></td><td>2025.5.20</td><td>Vid2World: Crafting Video Diffusion Models to Interactive World Models</td><td></td><td>将预训练视频扩散模型迁移应用于交互式世界模型的通用方法</td><td><a href="103.html">link</a></td></tr>
<tr><td></td><td>2025.5.16</td><td>Infinigen-Sim: Procedural Generation of Articulated Simulation Assets</td><td></td><td>Blender工具，可创建铰链资源</td><td><a href="93.html">link</a></td></tr>
<tr><td></td><td>2025.5.16</td><td>PoseBench3D: A Cross-Dataset Analysis Framework for 3D Human Pose Estimation</td><td></td><td>HPE的标准化测试环境</td><td><a href="95.html">link</a></td></tr>
<tr><td></td><td>2025.5.15</td><td>TexTailor: Customized Text-aligned Texturing via Effective Resampling</td><td></td><td>文本给3D物体加纹理</td><td><a href="85.html">link</a></td></tr>
<tr><td></td><td>2025.5.13</td><td>ACT-R: Adaptive Camera Trajectories for 3D Reconstruction from Single Image</td><td></td><td>多视角的3D重建</td><td><a href="81.html">link</a></td></tr>
<tr><td></td><td>2025.5.12</td><td>Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured 3D Assets</td><td></td><td>3D生成框架</td><td><a href="75.html">link</a></td></tr>
<tr><td></td><td>2025.5.12</td><td>Link to the Past: Temporal Propagation for Fast 3D Human Reconstruction from Monocular Video</td><td></td><td>单目视频实现快速3D穿衣人体重建</td><td><a href="74.html">link</a></td></tr>
<tr><td></td><td>2025.5.8</td><td>3D Scene Generation: A Survey</td><td></td><td></td><td><a href="62.html">link</a></td></tr>
<tr><td></td><td>2025.5.7</td><td>Apply Hierarchical-Chain-of-Generation to Complex Attributes Text-to-3D Generation</td><td></td><td>文生3D模型</td><td><a href="66.html">link</a></td></tr>
<tr><td></td><td>2025.5.7</td><td>MeshGen: Generating PBR Textured Mesh with Render-Enhanced Auto-Encoder and Generative Data Augmentation</td><td></td><td>3D重建</td><td><a href="58.html">link</a></td></tr>
<tr><td></td><td>2025.5.7</td><td>Person-In-Situ: Scene-Consistent Human Image Insertion with Occlusion-Aware Pose Control</td><td></td><td>将人体图像合成到场景图像中</td><td><a href="54.html">link</a></td></tr>
<tr><td></td><td>2025.5.3</td><td>Efficient 3D Full-Body Motion Generation from Sparse Tracking Inputs with Temporal Windows</td><td></td><td>稀疏输入，动作生成</td><td><a href="42.html">link</a></td></tr>
<tr><td></td><td>2025.5.1</td><td>JointDiT: Enhancing RGB-Depth Joint Modeling with Diffusion Transformers</td><td></td><td>RGB图像与深度图进行联合生成</td><td><a href="36.html">link</a></td></tr>
<tr><td></td><td>2025.4.29</td><td>TesserAct: Learning 4D Embodied World Models</td><td></td><td>4D场景重建</td><td><a href="22.html">link</a></td></tr>
<tr><td></td><td>2025.4.28</td><td>AnimateAnywhere: Rouse the Background in Human Image Animation</td><td></td><td>基于人体视频生成运行背景</td><td><a href="17.html">link</a></td></tr>
<tr><td></td><td>2025.4.25</td><td>Unify3D: An Augmented Holistic End-to-end Monocular 3D Human Reconstruction via Anatomy Shaping and Twins Negotiating</td><td></td><td>单目三维穿衣人体重建</td><td><a href="12.html">link</a></td></tr>
<tr><td></td><td>2025.4.24</td><td>Dynamic Camera Poses and Where to Find Them</td><td></td><td>相机位姿估计</td><td><a href="2.html">link</a></td></tr>
<tr><td></td><td>2025.4.24</td><td>Bolt: Clothing Virtual Characters at Scale</td><td></td><td>3D服装迁移适配</td><td><a href="3.html">link</a></td></tr>
<tr><td></td><td>2025.4.24</td><td>Unveiling Hidden Vulnerabilities in Digital Human Generation via Adversarial Attacks</td><td></td><td>HPS攻击对抗</td><td><a href="5.html">link</a></td></tr>
<tr><td></td><td>2025.4.23</td><td>HUG: Hierarchical Urban Gaussian Splatting with Block-Based Reconstruction</td><td></td><td>3D场景重建</td><td><a href="8.html">link</a></td></tr>
<tr><td></td><td>2025.4.23</td><td>Gaussian Splatting is an Effective Data Generator for 3D Object Detection</td><td></td><td>3D物体放入2D场景</td><td><a href="9.html">link</a></td></tr>
<tr><td></td><td>2025.4.23</td><td>Physically Consistent Humanoid Loco-Manipulation using Latent Diffusion Models</td><td></td><td>生成逼真的RGB人-物交互场景，以指导人形机器人的移动操作规划</td><td><a href="7.html">link</a></td></tr>
<tr><td></td><td>2025.4.22</td><td>SmallGS: Gaussian Splatting-based Camera Pose Estimation for Small-Baseline Videos</td><td></td><td>小基线视频的相机姿态估计</td><td><a href="14.html">link</a></td></tr>
</tbody></table>
<div style="break-before: page; page-break-before: always;"></div><h1 id=""><a class="header" href="#"></a></h1>
<div style="break-before: page; page-break-before: always;"></div><p>Nabla-R2D3: Effective and Efficient 3D Diffusion Alignment with 2D Rewards</p>
<p><strong>生成高质量、照片级真实感的3D资源（资产）仍然是3D视觉和计算机图形学领域长期存在的挑战。</strong> 尽管最先进的生成模型（如扩散模型）在3D生成方面取得了显著进展，但由于在遵循指令、符合人类偏好或生成逼真的纹理、几何结构和物理属性方面能力有限，它们通常无法达到人工设计内容的水平。本文介绍了 <strong>Nabla-R2D3</strong>，这是一个为原生3D扩散模型设计的、高效且样本高效的强化学习对齐框架，它仅利用2D奖励信号进行训练。该框架建立在最近提出的 <strong>Nabla-GFlowNet</strong> 方法之上——该方法以原理性的方式将得分函数与奖励梯度相匹配，从而实现基于奖励的微调。我们的 Nabla-R2D3 能够仅使用 2D 奖励信号就有效地适配 3D 扩散模型。大量实验表明，与那些要么难以收敛、要么遭受奖励破解问题的普通微调基线方法不同，Nabla-R2D3 在少量微调步骤内就能持续获得更高的奖励并减少先验遗忘。**     </p>
<div style="break-before: page; page-break-before: always;"></div><p>Hunyuan3D 2.1: From Images to High-Fidelity 3D Assets with Production-Ready PBR Material</p>
<p>三维AI生成内容（AIGC）是一个蓬勃发展的领域，显著加速了游戏、影视和设计行业中3D模型的创作进程。虽然多项开创性模型已彻底革新了3D生成技术，但由于3D模型在数据采集、处理与训练过程中存在高度复杂性，该领域目前仍主要面向研究人员、开发者和设计师群体开放。</p>
<p>为突破这些技术壁垒，本教程将以&quot;浑元3D 2.1&quot;系统为案例展开研究。本教程提供全面的操作指南，逐步讲解如何通过这套先进系统处理3D数据、训练3D生成模型并评估模型性能。该系统可生成高分辨率带纹理的3D资产，其核心架构包含两大组件：负责形状生成的&quot;浑元3D-DiT&quot;模块与专注纹理合成的&quot;浑元3D-Paint&quot;模块。</p>
<p>我们将深入探讨完整工作流程，涵盖数据准备、模型架构设计、训练策略制定、评估指标构建及最终部署方案。完成本教程后，您将掌握微调或开发健壮3D生成模型的实践能力，这些模型可广泛应用于游戏开发、虚拟现实及工业设计领域。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Particle-Grid Neural Dynamics for Learning Deformable Object Models from RGB-D Videos</p>
<p>由于可变形物体多样的物理特性以及难以从有限的视觉信息中估计其状态，建模其动力学具有挑战性。我们通过一个<strong>神经动力学框架</strong>来解决这些挑战，该框架在混合表示中结合了<strong>物体粒子 (object particles)</strong> 和<strong>空间网格 (spatial grids)</strong>。我们的<strong>粒子-网格模型 (particle-grid model)</strong> 既能捕捉全局形状和运动信息，又能预测密集的粒子运动，从而能够建模具有不同形状和材料的物体。粒子代表物体形状，而空间网格则将三维空间离散化，以确保空间连续性并提高学习效率。结合用于视觉渲染的<strong>高斯泼溅 (Gaussian Splatting)</strong> 技术，我们的框架实现了<strong>完全基于学习的可变形物体数字孪生模型 (fully learning-based digital twin)</strong>，并生成<strong>动作条件的三维视频 (3D action-conditioned videos)</strong>。</p>
<p>通过实验，我们证明了我们的模型能够从机器人-物体交互的<strong>稀疏视角RGB-D记录 (sparse-view RGB-D recordings)</strong> 中学习各种物体（如绳索、布料、毛绒玩具和纸袋）的动力学，同时还能在<strong>类别层面 (category level)</strong> 上泛化到未见过的实例。我们的方法优于最先进的<strong>基于学习的模拟器 (learning-based simulators)</strong> 和<strong>基于物理的模拟器 (physics-based simulators)</strong>，尤其是在<strong>相机视角有限 (limited camera views)</strong> 的场景下。此外，我们展示了所学模型在<strong>基于模型的规划 (model-based planning)</strong> 中的实用性，使其能够在一系列任务中实现<strong>目标条件的物体操作 (goal-conditioned object manipulation)</strong>。</p>
<p>项目页面位于：<a href="https://kywind.github.io/pgnd">https://kywind.github.io/pgnd</a>   </p>
<div style="break-before: page; page-break-before: always;"></div><p>HOIDiNi: Human-Object Interaction through Diffusion Noise Optimization</p>
<p>我们提出 <strong>HOIDiNi</strong>，一个用于生成逼真且合理的人-物交互（HOI）的<strong>文本驱动扩散框架</strong>。HOI 生成极具挑战性，因为它要求<strong>严格的接触精度</strong>和<strong>多样化的运动流形</strong>。现有方法往往在<strong>真实感</strong>和<strong>物理正确性</strong>之间取舍，而 HOIDiNi 通过<strong>扩散噪声优化（DNO）</strong> 直接在预训练扩散模型的<strong>噪声空间</strong>中进行优化，<strong>同时实现了两者</strong>。</p>
<p>这得以实现是因为我们观察到该问题可以分解为两个阶段：</p>
<ol>
<li><strong>以物体为中心阶段</strong>：主要做出<strong>手-物接触位置</strong>的离散选择；</li>
<li><strong>以人为中心阶段</strong>：优化<strong>全身运动</strong>以实现该蓝图。</li>
</ol>
<p>这种结构化方法能在<strong>不牺牲运动自然度</strong>的前提下，实现<strong>精确的手-物接触</strong>。仅在 <strong>GRAB 数据集</strong>上进行的<strong>定量、定性和主观评估</strong>均清晰表明，HOIDiNi 在<strong>接触精度、物理有效性和整体质量</strong>上均<strong>优于</strong>先前工作和基线方法。我们的结果表明，该框架能够<strong>仅由文本提示驱动</strong>，生成包括<strong>抓取、放置和全身协调</strong>在内的<strong>复杂、可控的交互</strong>。项目主页：<a href="https://hoidini.github.io">https://hoidini.github.io</a>。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>GenHOI: Generalizing Text-driven 4D Human-Object Interaction Synthesis for Unseen Objects</p>
<p>尽管扩散模型和大规模运动数据集推动了文本驱动的人体动作生成技术的发展，但将这些进展扩展到四维（4D）人-物交互（HOI）领域仍然充满挑战，这主要归因于大规模 4D HOI 数据集的稀缺性。在我们的研究中，我们提出了 GenHOI，一个新颖的两阶段框架，旨在实现两个关键目标：1) 对未见过的物体具有泛化能力；2) 合成高保真度的 4D HOI 序列。</p>
<p>在我们框架的初始阶段，我们采用 <strong>物体锚点网络 (Object-AnchorNet)</strong> 来为未见过的物体重建稀疏的 3D HOI 关键帧。该网络仅需从 3D HOI 数据集中学习即可，从而减少了对大规模 4D HOI 数据集的依赖。</p>
<p>随后，在第二阶段，我们引入了 <strong>接触感知扩散模型 (ContactDM)</strong>，将稀疏的 3D HOI 关键帧无缝地插值成时间一致性的密集 4D HOI 序列。为了提高生成的 4D HOI 序列质量，我们在 ContactDM 中提出了一种新颖的 <strong>接触感知编码器 (Contact-Aware Encoder)</strong> 来提取人-物接触模式，以及一种新颖的 <strong>接触感知 HOI 注意力机制 (Contact-Aware HOI Attention)</strong>，将接触信号有效地整合到扩散模型中。</p>
<p>实验结果表明，我们在公开可用的 OMOMO 和 3D-FUTURE 数据集上取得了最先进的结果，展现了对未见物体的强大泛化能力，同时实现了高保真度的 4D HOI 生成。    </p>
<div style="break-before: page; page-break-before: always;"></div><p>VideoMAR: Autoregressive Video Generatio with Continuous Tokens</p>
<p>基于掩码的自回归模型在连续空间中已展现出卓越的图像生成能力。然而，其在视频生成方面的潜力仍未得到充分探索。本文提出 <strong>VideoMAR</strong>，一个简洁高效的纯解码器自回归图像到视频模型，它使用连续标记（continuous tokens），融合了时间维度上的逐帧生成和空间维度上的掩码生成。</p>
<p>我们首先确立了时间因果性和空间双向性作为视频自回归模型的首要原则，并提出了下一帧扩散损失（next-frame diffusion loss）来整合掩码与视频生成。此外，长序列自回归建模的巨大成本和难度是一个基础但关键的问题。为此，我们提出了<strong>时间维度上由短及长的课程学习</strong>（temporal short-to-long curriculum learning）和<strong>空间维度上渐进分辨率训练</strong>（spatial progressive resolution training），并在推理时采用<strong>渐进温度策略</strong>（progressive temperature strategy）以减轻累积误差。</p>
<p>此外，VideoMAR 将语言模型的多种独特能力复现到了视频生成领域。得益于其<strong>时间维度上的 KV 缓存复用</strong>（simultaneous temporal-wise KV cache）和<strong>空间维度上的并行生成</strong>（spatial-wise parallel generation），它天然具备高效率；并且通过<strong>3D 旋转位置编码</strong>（3D rotary embeddings），它展现出空间和时间外推能力。</p>
<p>在 VBench-I2V 基准测试中，VideoMAR 超越了之前的最先进模型（Cosmos I2V），同时所需参数量（$9.3%$）、训练数据量（$0.5%$）和 GPU 资源（$0.2%$）均显著减少。</p>
<div style="break-before: page; page-break-before: always;"></div><p>SyncTalk++: High-Fidelity and Efficient Synchronized Talking Heads Synthesis Using Gaussian Splatting</p>
<p><strong>实现高度同步的真实语音驱动说话头视频合成是一项重大挑战。</strong> 一个逼真的说话头需要人物身份、唇部动作、面部表情和头部姿态的同步协调。缺乏这些同步性是导致结果不真实的一个根本性缺陷。为解决这一被视作创建逼真说话头过程中“魔鬼”的关键同步问题，我们提出了 <strong>SyncTalk++</strong>。该方案包含一个基于高斯溅射的<strong>动态肖像渲染器</strong>，以确保人物身份的一致保持；以及一个<strong>唇语同步控制器</strong>，用于对齐唇部动作与语音，并创新性地利用<strong>3D面部混合变形模型</strong>来重建精准的面部表情。为确保自然的头部运动，我们提出了<strong>头部同步稳定器</strong>，通过优化头部姿态来提升稳定性。此外，SyncTalk++ 通过整合<strong>表情生成器</strong>和<strong>躯干修复器</strong>，增强了对分布外音频的鲁棒性——前者生成与语音匹配的面部表情，后者则修复出无缝衔接的躯干区域。我们的方法在帧间视觉细节上保持了一致性与连续性，并显著提升了渲染速度和质量，最高可达<strong>每秒101帧</strong>。大量的实验和用户研究表明，SyncTalk++ 在同步性和真实感方面均优于现有最先进的方法。我们推荐观看补充视频：<a href="https://ziqiaopeng.github.io/synctalk++">https://ziqiaopeng.github.io/synctalk++</a>。</p>
<div style="break-before: page; page-break-before: always;"></div><p>GAF: Gaussian Action Field as a Dvnamic World Model for Robotic Mlanipulation</p>
<p><strong>准确的动作推断对于基于视觉的机器人操作至关重要。</strong> 现有方法通常遵循两种范式：一种是<strong>视觉到动作 (V-A)</strong> 范式，直接从视觉输入预测动作；另一种是<strong>视觉到三维再到动作 (V-3D-A)</strong> 范式，利用中间的三维表示。然而，由于操作场景的复杂性和动态性，这些方法常常难以实现动作的精准推断。</p>
<p>本文提出了一种 <strong>V-4D-A 框架</strong>，该框架通过<strong>高斯动作场 (Gaussian Action Field, GAF)</strong> 实现了从运动感知的四维表示中进行直接的动作推理。GAF 通过引入可学习的运动属性扩展了三维高斯泼溅 (3D Gaussian Splatting, 3DGS)，能够同时建模动态场景和操作动作。</p>
<p>为了学习时变场景几何和动作感知的机器人运动，GAF 支持三种关键查询类型：重建当前场景、预测未来帧，以及通过机器人运动估计初始动作。此外，GAF 生成的高质量当前帧和未来帧，有助于通过 <strong>GAF 引导的扩散模型</strong> 来优化操作动作。</p>
<p>大量实验表明，该方法取得了显著提升：在重建质量方面，GAF 实现了 <strong>+11.5385 dB PSNR</strong> 的提升和 <strong>-0.5574 LPIPS</strong> 的降低；同时，在机器人操作任务的平均成功率上，相比最先进方法提升了 <strong>10.33%</strong>。</p>
<p>项目页面：<a href="http://chaiying1.github.io/GAF.github.io/project_page/">http://chaiying1.github.io/GAF.github.io/project_page/</a>   </p>
<div style="break-before: page; page-break-before: always;"></div><p>GMT: General Motion Tracking for Humanoid Whole-Body Control</p>
<p>在现实世界中追踪通用全身运动的能力，是构建通用人形机器人的有效途径。然而，由于运动的时间与运动学多样性、策略能力限制以及上下半身协调的困难，实现这一目标颇具挑战性。为解决这些问题，我们提出了 <strong>GMT</strong>（通用运动追踪框架），这是一个通用且可扩展的运动追踪框架，它通过训练一个统一的策略，使人形机器人能够在现实世界中追踪多种多样的运动。</p>
<p>GMT 建立在两个核心组件之上：<strong>自适应采样策略</strong>和<strong>运动专家混合模型（MoE）架构</strong>。自适应采样在训练过程中自动平衡简单和困难的动作。MoE 架构则确保了对运动流形不同区域进行更好的专业化处理。我们通过在仿真和现实世界中的大量实验，验证了 GMT 的有效性。该框架使用一个统一的通用策略，在广泛的运动频谱上实现了<strong>最先进的性能</strong>。视频和更多信息请访问：<a href="https://gmt-humanoid.github.io">https://gmt-humanoid.github.io</a>。    </p>
<div style="break-before: page; page-break-before: always;"></div><p>Toward Rich Video Human-Motion2D Generation</p>
<p>生成逼真且可控的人体动作，尤其是那些涉及丰富的多角色交互的动作，由于数据稀缺以及人际互动动态建模的复杂性，仍然是一个重大挑战。为应对这些限制，我们首先引入了一个新的大规模、信息丰富的视频人体动作2D数据集（Motion2D-Video-150K），该数据集包含<strong>15万个视频序列</strong>。Motion2D-Video-150K的特点是其<strong>平衡分布</strong>了多样的单角色动作，以及<strong>关键的双角色交互动作</strong>，每个视频均配有详细的文本描述。</p>
<p>基于此数据集，我们提出了一种新颖的、基于扩散模型的丰富视频人体动作2D生成模型（<strong>RVHM2D</strong>）。RVHM2D采用了一种增强的文本条件机制，该机制利用<strong>双文本编码器（CLIP-L/B）或T5-XXL</strong>，并结合了<strong>全局和局部特征</strong>。我们设计了一种<strong>两阶段训练策略</strong>：模型首先使用标准的扩散目标进行训练，然后使用<strong>基于FID指标的奖励进行强化学习微调</strong>，以进一步提升动作的逼真度和文本对齐度。</p>
<p>大量实验表明，RVHM2D在Motion2D-Video-150K基准测试上，无论是生成单角色还是交互式双角色场景，都取得了<strong>领先的性能</strong>。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>PF-LHM: 3D Animatable Avatar Reconstruction from Pose-free Articulated Human Images</p>
<p><strong>从随手拍摄的图像重建可驱动的3D人体</strong></p>
<p>从未经标定的相机或未提供人体姿态信息的随手拍摄图像中，重建一个关节化主体的可驱动3D人体模型，是一项实用但极具挑战性的任务。这主要源于视角偏差、遮挡以及缺乏结构先验知识等问题。尽管基于优化的方法能够从单目或多视角视频中生成高保真度的结果，但它们需要精确的姿态估计和缓慢的迭代优化过程，这限制了其在无约束场景下的可扩展性。近期的前馈式方法能够实现高效的单图像重建，但在有效利用多张输入图像以减少歧义并提升重建精度方面存在困难。</p>
<p>为了应对这些挑战，我们提出了 <strong>PF-LHM</strong>，一个大型人体重建模型。它能够在一秒内从一张或多张随手拍摄的、无姿态信息的图像中生成高质量的3D虚拟化身。我们的方法引入了一种高效的<strong>编码器-解码器点-图像Transformer架构</strong>。该架构通过多模态注意力机制，融合了分层的几何点特征和多视角图像特征。融合后的特征被解码以恢复细节几何和外观，并采用3D高斯泼溅（3D Gaussian splats）技术进行表示。</p>
<p>在真实和合成数据集上进行的大量实验表明，我们的方法统一了单图和多图3D人体重建，能够在无需相机和人体姿态标注的情况下，生成高保真度且可驱动的3D人体虚拟化身。代码和模型将向公众开源。</p>
<div style="break-before: page; page-break-before: always;"></div><p>MAMMA: Markerless &amp; Automatic Multi-Person Motion Action Capture</p>
<p>我们提出 <strong>MAMMA</strong>，一种无标记动作捕捉流程，能够准确地从双人交互序列的多视角视频中恢复 SMPL-X 参数。传统的动作捕捉系统依赖于物理标记点。虽然它们精度很高，但其对专用硬件、手动标记点放置以及大量后期处理的要求，使得它们成本高昂且耗时。近期的基于学习的方法试图克服这些限制，但大多数方法主要面向单人捕捉、依赖于稀疏关键点，或在处理遮挡和物理交互时存在困难。</p>
<p>在这项工作中，我们介绍了一种方法，它<strong>基于分割掩码（segmentation masks）预测密集的 2D 表面标志点（landmarks）</strong>，从而即使在严重遮挡的情况下也能实现针对特定人物的对应关系估计。我们采用了一种新颖的架构，利用<strong>可学习的查询（learnable queries）</strong> 来对应每个标志点。我们证明，我们的方法能够处理复杂的人与人交互，并且比现有方法具有更高的精度。</p>
<p>为了训练我们的网络，我们构建了一个<strong>大规模合成的多视角数据集</strong>，该数据集结合了来自不同来源的人体动作，包括极端姿势、手部动作和近距离交互。我们的数据集产生了具有丰富身体接触和遮挡的高可变性合成序列，并包含了带有密集 2D 标志点的 SMPL-X 真实值（ground-truth）标注。最终成果是一个无需标记点即可捕捉人体动作的系统。</p>
<p>与商业化的基于标记点的动作捕捉解决方案相比，我们的方法无需大量手动清理，就能提供具有竞争力的重建质量。最后，针对密集标志点预测和无标记动作捕捉领域缺乏通用基准的问题，我们基于真实多视角序列构建了<strong>两种评估设置</strong>。我们将开源我们的<strong>数据集、基准、方法、训练代码和预训练模型权重</strong>，供研究使用。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>iDiT-HOI: Inpainting-based Hand Object Interaction Reenactment via Video Diffusion Transformer</p>
<p><strong>数字人视频生成</strong>技术在教育、电子商务等领域正日益受到关注，这主要得益于头部-身体动画和唇形同步技术的进步。然而，实现逼真的<strong>手物交互（Hand-Object Interaction, HOI）</strong>——即人手与物体之间复杂的动态关系——仍然面临诸多挑战。生成自然可信的手物交互重演非常困难，原因包括：手与物体之间的遮挡问题、物体形状和朝向的多样性、对精确物理交互的需求，以及<strong>关键的是</strong>，模型需要具备泛化到<strong>未见过的</strong>人和物体的能力。</p>
<p>本文提出了一种新颖的框架 <strong>iDiT-HOI</strong>，它能够实现<strong>野外环境下的手物交互重演生成</strong>。具体来说，我们提出了一种统一的、基于修复的令牌处理方法 <strong>Inp-TPU</strong>，该方法结合了一个<strong>两阶段视频扩散Transformer（DiT）模型</strong>。第一阶段通过将指定物体插入手部区域来生成关键帧，为后续帧提供参考。第二阶段则确保手物交互的时间连贯性和流畅性。</p>
<p>我们方法的核心贡献在于：<strong>无需引入额外参数</strong>，即可<strong>重用预训练模型的上下文感知能力</strong>，从而实现对未见物体和场景的强大泛化能力；同时，我们提出的范式<strong>天然支持生成长视频</strong>。全面的评估表明，我们的方法优于现有技术，尤其是在具有挑战性的真实世界场景中，提供了更强的真实感和更无缝的手物交互效果。    </p>
<div style="break-before: page; page-break-before: always;"></div><p>RL from Physical Feedback: Aligning Large Motion Models with Humanoid Control</p>
<p>本文聚焦于机器人学中的一个关键挑战：如何将文本驱动的人类动作转化为人形机器人可执行的动作，从而实现高效、低成本的新行为学习。虽然现有的文本到动作生成方法能够实现语言与动作的语义对齐，但它们生成的动常常在运动学或物理上不可行，不适合实际部署。为了弥合仿真与现实之间的鸿沟，我们提出了<strong>基于物理反馈的强化学习（Reinforcement Learning from Physical Feedback, RLPF）</strong>——一个将物理感知的运动评估与文本条件化动作生成相结合的全新框架。RLPF利用一个动作跟踪策略在物理仿真器中评估动作的可行性，并据此生成奖励信号用于微调动作生成器。此外，RLPF引入了一个对齐验证模块，以确保生成的动始终忠实于文本指令的语义。这种联合优化保证了动作在物理上的合理性和对指令语义的对齐性。大量实验表明，RLPF在生成物理可行的动作方面显著优于基线方法，同时保持了与文本指令的语义对应，从而成功实现了在真人形机器人上的部署。     </p>
<div style="break-before: page; page-break-before: always;"></div><p>KungfuBot: Physics-Based Humanoid Whole-Body Control for Learning Highly-Dynamic Skills</p>
<p><strong>人形机器人有望通过模仿人类行为掌握多种技能。然而，即使经过精心的奖励和课程设计，现有算法也只能追踪平滑、低速的人类动作。本文提出了一种基于物理的人形机器人控制框架，旨在通过多步骤动作处理和自适应动作跟踪，来掌握功夫和舞蹈等高动态的人类行为。在动作处理方面，我们设计了一个流程来提取、过滤、校正和重定向动作，同时最大程度地确保其符合物理约束。在动作模仿方面，我们构建了一个双层优化问题，根据当前跟踪误差动态调整跟踪精度容差，从而创建了一种自适应课程机制。我们进一步构建了一个非对称的Actor-Critic（演员-评论家）框架进行策略训练。在实验中，我们训练了全身控制策略来模仿一组高动态动作。我们的方法实现了比现有方法显著更低的跟踪误差，并成功部署在Unitree G1机器人上，展现出稳定且富有表现力的行为。项目主页为：<a href="https://kungfu-bot.github.io">https://kungfu-bot.github.io</a>。</strong>    </p>
<div style="break-before: page; page-break-before: always;"></div><p>Enter: Graduated Realism: A Pedagogical Framework for AI-Powered Avatars in Virtual Reality Teacher Training</p>
<p>虚拟现实（VR）模拟器为教师培训提供了强大的工具，然而，人工智能（AI）驱动的学生化身的整合带来了一个关键挑战：如何确定化身拟真度的最优水平以实现有效教学。本文献综述审视了VR教师培训中化身拟真度的演进历程，综合分析了其理论意义，并提出了一个新的教学框架以指导未来的设计。通过系统性综述，本文追溯了从人控化身到生成式AI原型的发展过程。应用认知负荷理论等学习理论，我们主张超拟真并非总是最优解，因为高保真化身会给新手施加过高的外在认知负荷，这一立场得到了近期实证研究的支持。当前存在一个显著鸿沟：一方面是技术追求照片级真实感的驱动力，另一方面则是教学对支架式学习的需求。为弥合这一鸿沟，我们提出了“渐进式拟真”（Graduated Realism）框架，主张让受训者从低保真化身开始，随着技能提升逐步增加行为复杂性。为实现这一框架的计算可行性，我们概述了一种新颖的“单调用架构”——Crazy Slots，它利用概率引擎和检索增强生成（RAG）数据库，无需依赖多步推理模型的延迟和高昂成本，即可生成真实、实时的响应。本综述为设计下一代AI模拟器提供了循证原则，并论证了基于教学原理的拟真度处理方式，对于创建可扩展且高效的教师教育工具至关重要。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>Auto-Connect: Connectivity-Preserving RigFormer with Direct Preference Optimization</p>
<p>我们提出<strong>Auto-Connect</strong>，一种用于自动骨骼绑定的新方法。该方法通过一种<strong>保留连接性的标记化方案</strong>，明确地维护了骨骼的连接性。与先前预测表示为两个关节的骨骼位置，或先预测点再确定连接性的方法不同，我们的方法采用特殊标记来为每个关节的子关节以及每个层级定义端点，从而有效地自动化了连接关系。通过将连接信息直接整合到预测框架中，这种方法显著提高了<strong>拓扑精度</strong>。</p>
<p>为了进一步保证高质量的拓扑结构，我们实现了一个<strong>拓扑感知的奖励函数</strong>，用于量化拓扑结构的正确性。该奖励函数在训练后阶段通过<strong>奖励引导的直接偏好优化（DPO）</strong> 被利用起来。此外，我们引入了<strong>隐式测地线特征</strong>用于<strong>潜在Top-K骨骼选择</strong>，这大幅提升了<strong>蒙皮质量</strong>。通过利用模型潜在空间内的测地线距离信息，我们的方法能够智能地确定对每个顶点最具影响力的骨骼，有效减轻了常见的<strong>蒙皮瑕疵</strong>。</p>
<p>这种<strong>保留连接性的标记化方案</strong>、<strong>奖励引导的微调</strong>和<strong>基于测地线的骨骼选择</strong>的结合，使我们的模型能够持续生成更具<strong>解剖学合理性</strong>的骨骼结构，并具备<strong>更优的变形特性</strong>。    </p>
<div style="break-before: page; page-break-before: always;"></div><p>SignAligner: Harmonizing Complementary Pose Modalities for Coherent Sign Language Generation</p>
<p><strong>手语生成</strong>的目标是基于口语生成多样化的手语表征。然而，由于手语本身的复杂性——包含精细的手势、面部表情和身体动作——实现逼真且自然的生成仍是一项重大挑战。</p>
<p>在本研究中，我们引入了 <strong>PHOENIX14T+</strong>，这是广泛使用的 RWTH-PHOENIX-Weather 2014T 数据集的一个扩展版本，新增了三种手语表征：<strong>姿势 (Pose)</strong>、<strong>Hamer</strong> 和 <strong>Smplerx</strong>。同时，我们提出了一种用于逼真手语生成的新方法——<strong>SignAligner</strong>。该方法包含三个阶段：<strong>文本驱动的姿态模态协同生成</strong>、<strong>多模态在线协作校正</strong>以及<strong>逼真的手语视频合成</strong>。</p>
<ol>
<li><strong>文本驱动的姿态模态协同生成：</strong> 通过融入文本语义信息，我们设计了一个联合手语生成器，用于同时生成姿势坐标、手势动作和身体运动。基于 Transformer 架构的文本编码器提取语义特征，而跨模态注意力机制则整合这些特征，生成多样化的手语表征，确保模态特征的准确映射并控制其多样性。</li>
<li><strong>多模态在线协作校正：</strong> 引入在线协作校正机制，利用动态损失加权策略和跨模态注意力来精炼生成的姿态模态。这促进了不同模态间信息的互补，消除了时空冲突，并确保了语义连贯性和动作一致性。</li>
<li><strong>逼真的手语视频合成：</strong> 校正后的姿态模态被输入到一个预训练的视频生成网络中，最终生成高保真的手语视频。</li>
</ol>
<p>大量实验表明，SignAligner 显著提升了生成手语视频的<strong>准确性</strong>和<strong>表现力</strong>。    </p>
<div style="break-before: page; page-break-before: always;"></div><p>Aligned Novel View Image and Geometry Synthesis via Cross-modal Attention Instillation</p>
<p>我们提出了一种基于扩散模型的框架，通过**扭曲修复方法（warping-and-inpainting methodology）<strong>实现对齐的新视角图像与几何结构生成。不同于以往需要密集姿态图像或局限于域内视角的、嵌入姿态的生成模型的方法，我们的方法</strong>利用现成的几何预测器（off-the-shelf geometry predictors）**从参考图像预测部分几何结构，并将新视角合成（novel-view synthesis）<strong>表述为图像和几何结构的修复任务（inpainting task）</strong>。</p>
<p>为确保生成的图像与几何结构之间精确对齐，我们提出了<strong>跨模态注意力蒸馏（cross-modal attention distillation）</strong>。在训练和推理过程中，将图像扩散分支（image diffusion branch）的注意力图（attention maps）<strong>注入（injected into）<strong>一个并行的几何扩散分支（geometry diffusion branch）。这种</strong>多任务方法（multi-task approach）<strong>产生了</strong>协同效应（synergistic effects）</strong>，既促进了几何鲁棒的图像合成（geometrically robust image synthesis），也实现了边界清晰的几何预测（well-defined geometry prediction）。</p>
<p>我们进一步引入了<strong>邻近网格条件（proximity-based mesh conditioning）</strong>，以整合深度和法线线索（depth and normal cues）。该方法在点云之间进行插值（interpolating between point cloud），并<strong>过滤错误预测的几何结构（filtering erroneously predicted geometry）</strong>，防止其影响生成过程。</p>
<p>实验表明，我们的方法在一系列未见场景（unseen scenes）上，针对图像和几何结构均实现了<strong>高保真的外推式视图合成（high-fidelity extrapolative view synthesis）</strong>；在插值设置（interpolation settings）下提供了具有竞争力的重建质量（competitive reconstruction quality）；并能生成几何对齐的彩色点云（geometrically aligned colored point clouds），用于全面的三维补全（comprehensive 3D completion）。</p>
<p>项目主页位于：<a href="https://cvlab-kaist.github.io/MoAI">https://cvlab-kaist.github.io/MoAI</a>。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>AlignHuman: Improving Motion and Fidelity via Timestep-Segment Preference Optimization for Audio-Driven Human Animation</p>
<p>在扩散模型（Diffusion Models）的推动下，人类视频生成与动画任务近期取得了显著进展。然而，由于**自然动作（motion naturalness）<strong>与</strong>视觉保真度（visual fidelity）**之间的权衡问题，生成富有表现力且逼真的人类动画仍然具有挑战性。</p>
<p>为了解决这一问题，我们提出了 <strong>AlignHuman</strong> 框架。该框架将<strong>偏好优化（Preference Optimization）<strong>作为一种训练后精调技术，并结合</strong>分治训练策略（divide-and-conquer training strategy）</strong>，以协同优化这两个相互竞争的目标。</p>
<p>我们的核心洞察源于对跨时间步去噪过程的分析：(1) **早期去噪时间步（early denoising timesteps）**主要控制运动动态（motion dynamics），而 (2) **保真度和人体结构（fidelity and human structure）<strong>即使在跳过早期步骤的情况下，也能由</strong>后期时间步（later timesteps）**有效管理。</p>
<p>基于这一观察，我们提出了<strong>分时段偏好优化（Timestep-segment Preference Optimization, TPO）</strong>，并引入了两个专门的**LoRA模块（Low-Rank Adaptation）**作为专家对齐模块。每个 LoRA 模块专注于其对应时间步区间内的特定优化维度（运动自然度或保真度）。这些 LoRA 模块使用各自对应的偏好数据进行训练，并在推理过程中在相应的时间步区间内激活，分别用于增强运动自然度和视觉保真度。</p>
<p>大量实验表明，AlignHuman 显著提升了多个强基线模型的性能，并在推理时减少了<strong>去噪步数（Number of Function Evaluations, NFEs）</strong>，实现了 <strong>3.3× 的加速</strong>（从 100 NFEs 减少到 30 NFEs），同时对生成质量的影响微乎其微。</p>
<p>项目主页：<a href="https://alignhuman.github.io/">https://alignhuman.github.io/</a></p>
<div style="break-before: page; page-break-before: always;"></div><p>DreamActor-H1: High-Fidelity Human-Product Demonstration Video Generation via Motion-designed Diffusion Transformers</p>
<p>在电子商务和数字营销领域，生成高保真的人-物演示视频对于有效的产品展示至关重要。然而，现有的大多数框架要么无法同时保留人物和产品的身份特征，要么缺乏对人-物空间关系的理解，导致呈现效果不真实、交互动作不自然。</p>
<p>为应对这些挑战，我们提出了一种基于扩散Transformer（DiT）的框架。我们的方法通过注入配对的人-物参考信息并利用额外的掩码交叉注意力机制，能够同时保留人物身份和产品特有的细节（如标志和纹理）。我们采用三维人体网格模板和产品边界框来提供精确的运动指导，从而实现手势与产品放置位置的直观对齐。此外，我们利用结构化文本编码引入类别级语义信息，增强了在帧间发生微小旋转变化时的三维一致性。</p>
<p>通过在采用广泛数据增强策略的混合数据集上进行训练，我们的方法在保持人物和产品身份完整性以及生成逼真的演示动作方面，均优于现有前沿技术。</p>
<p>项目页面：<a href="https://submit2025-dream.github.io/DreamActor-H1/">https://submit2025-dream.github.io/DreamActor-H1/</a></p>
<div style="break-before: page; page-break-before: always;"></div><p>M4V: Multi-Modal Mamba for Text-to-Video Generation</p>
<p>文本到视频生成技术显著丰富了内容创作，并具备发展为强大世界模拟器的潜力。然而，对广阔时空空间进行建模在计算上仍然要求很高，尤其是在使用 Transformer 架构时——其序列处理存在二次方复杂度，因此限制了实际应用。</p>
<p>近期在线性时间序列建模方面的进展，特别是 Mamba 架构，提供了一种更高效的替代方案。然而，其朴素的设计限制了其在多模态和时空视频生成任务中的直接适用性。</p>
<p>为了应对这些挑战，我们提出了 <strong>M4V</strong>，一个用于文本到视频生成的<strong>多模态 Mamba 框架</strong>。具体来说，我们设计了一个<strong>多模态扩散 Mamba (MM-DiM) 模块</strong>，通过<strong>多模态令牌重组设计</strong>，能够无缝整合多模态信息并进行时空建模。</p>
<p>因此，在生成 768×1280 分辨率视频时，M4V 中的 Mamba 模块相比基于注意力机制的方案，将浮点运算量 (FLOPs) 降低了 45%。此外，为了缓解长上下文自回归生成过程中出现的视觉质量下降问题，我们引入了一种<strong>奖励学习策略</strong>，进一步提升了逐帧的视觉真实感。</p>
<p>在文本到视频基准测试上的大量实验表明，M4V 能够在显著降低计算成本的同时生成高质量视频。代码和模型将在 <a href="https://huangjch526.github.io/M4V_project">https://huangjch526.github.io/M4V_project</a> 公开提供。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>Motion-R1: Chain-of-Thought Reasoning and Reinforcement Learning for Human Motion Generation</p>
<p>大规模语言模型的最新进展，尤其在自然语言理解与推理方面的突破，为文本驱动动作生成开辟了新可能。尽管现有方法在语义对齐与动作合成领域取得显著进展，但其依赖的端到端映射策略往往难以捕捉深层语言结构与逻辑推理。这导致生成的动作常缺乏可控性、连贯性与多样性。</p>
<p>为突破这些局限，我们提出Motion-R1——一种融合思维链机制的统一动作-语言建模框架。该框架通过将复杂文本指令显式分解为逻辑化动作路径，为动作生成提供高层语义指导，显著增强模型对多步骤、长时程及组合式复杂指令的解析与执行能力。</p>
<p>在模型训练中，我们采用为大模型设计的强化学习算法&quot;群体相对策略优化&quot;，通过动作质量反馈联合优化推理链与动作合成。在多个基准数据集上的实验表明：Motion-R1在需要细致语义理解与长期时序连贯性的场景中，性能达到或超越现有最优方法。代码、模型及数据将全面公开。  </p>
<div style="break-before: page; page-break-before: always;"></div><p>Rethinking Generative Human Video Coding with Implicit Motion Transformation</p>
<p>超越传统的基于混合模型的视频编解码器，生成式视频编解码器通过将高维信号演化为紧凑的特征表示（在编码器端实现码流精简），并开发显式运动场作为中间监督（在解码器端实现高质量重建），有望实现优异的压缩性能。这一范式在人脸视频压缩领域已取得显著成功。然而，与面部视频相比，人体视频由于其更复杂多样的运动模式带来了更大挑战，即在生成式人体视频编码（Generative Human Video Coding, GHVC）中使用显式运动指导时，重建结果可能会出现严重失真和运动不准确的问题。</p>
<p>为此，本文重点阐述了基于显式运动的方法在人体视频压缩中的局限性，并探究了借助隐式运动转换（Implicit Motion Transformation, IMT）提升GHVC性能的方法。具体而言，我们提出将复杂的人体信号表征为紧凑的视觉特征，并将这些特征转化为隐式运动指导以用于信号重建。实验结果证明了所提出的IMT范式的有效性，它能够助力GHVC实现高效压缩和高保真合成。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>Controllable Expressive 3D Facial Animation via Diffusion in a Unified Multimodal Space</p>
<p>音频驱动的情绪化3D面部动画面临两大关键挑战：(1) 依赖单模态控制信号（视频、文本或情绪标签），未能综合利用其互补优势实现全面情绪操控；(2) 基于确定性回归的映射方法限制了情感表达和非语言行为的随机性本质，导致合成动画表现力受限。为应对这些挑战，我们提出基于扩散模型的可控表达性3D面部动画框架。该方案包含两大创新点：(1) 以FLAME模型为核心的多模态情绪绑定策略，通过对比学习对齐文本、音频及情绪标签等多源模态，实现灵活的多信号情绪控制；(2) 搭载内容感知注意力与情绪引导层的注意力潜扩散模型，在保持时序连贯性与自然面部动态的同时增强运动多样性。大量实验表明，本方法在多数指标上超越现有方案，情绪相似度提升21.6%，且能保持符合生理规律的面部动态。</p>
<p>项目主页：<a href="https://kangweiiliu.github.io/Control_3D_Animation">https://kangweiiliu.github.io/Control_3D_Animation</a></p>
<div style="break-before: page; page-break-before: always;"></div><p>PlayerOne: Egocentric World Simulator</p>
<p>我们推出 <strong>PlayerOne</strong>，首个以自我为中心(第一人称视角)的逼真世界模拟器，能够在生动动态的环境中实现沉浸式且不受限制的探索。给定用户提供的第一人称场景图像，PlayerOne 能够精确构建相应的世界，并生成与由外部(第三人称)摄像头捕捉到的用户真实场景人体运动严格对齐的第一人称视角视频。</p>
<p>PlayerOne 采用<strong>由粗到精(coarse-to-fine)的流程</strong>进行训练：首先在大规模第一人称文本-视频对上进行预训练，以获得粗粒度的第一人称理解；随后使用我们自动构建的流程，从第一人称-第三人称视角同步视频数据集中提取同步运动-视频数据进行微调。</p>
<p>此外，考虑到不同身体部位的重要性差异，我们设计了一种<strong>分部位解耦的运动注入方案(part-disentangled motion injection scheme)</strong>，实现了对身体部位级运动的精确控制。另外，我们设计了一个<strong>联合重建框架(joint reconstruction framework)</strong>，该框架同时渐进式地建模4D场景和视频帧，确保了生成长视频时的场景一致性。</p>
<p>实验结果表明，PlayerOne 在精确控制多样化人体运动以及对多样场景进行世界一致性建模方面，具有<strong>卓越的泛化能力</strong>。这标志着首次尝试构建第一人称视角的现实世界模拟，并能为研究社区深入探索世界建模及其多样化应用的<strong>全新领域</strong>铺平道路。</p>
<div style="break-before: page; page-break-before: always;"></div><p>HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene</p>
<p><strong>基于单目视频的动态三维场景重建始终是三维视觉领域的核心难题。尽管三维高斯溅射(3DGS)技术在静态场景中实现了实时渲染，但其在动态场景的应用仍面临挑战——这源于学习结构化且时间一致的运动表征存在困难。现有方法通常表现为三方面局限：高斯更新冗余、运动监督不足，以及对复杂非刚性形变的建模能力薄弱。这些问题共同阻碍了连贯高效的动态重建。</strong></p>
<p><strong>为突破这些限制，我们提出HAIF-GS——一个通过稀疏锚点驱动形变实现结构化一致动态建模的统一框架。该框架首先通过锚点过滤器识别运动相关区域，抑制静态区域的冗余更新；随后利用自监督诱导流引导变形模块，通过多帧特征聚合驱动锚点运动，无需显式光流标签；为处理细粒度形变，分层锚点传播机制能依据运动复杂度提升锚点分辨率，并传播多级变换关系。在合成与真实数据集上的大量实验表明，HAIF-GS在渲染质量、时间一致性和重建效率上均显著超越现有动态3DGS方法。</strong></p>
<div style="break-before: page; page-break-before: always;"></div><p>AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation</p>
<p>以下是专业准确的学术翻译：</p>
<p><strong>4D内容生成的最新进展日益受到关注，但由于时空分布建模的复杂性及4D训练数据的稀缺性，创建高质量动态3D模型仍具挑战性。本文提出AnimateAnyMesh——首个能够高效实现任意3D网格文本驱动动画的前馈框架。该方法采用创新的DyMeshVAE架构（动态网格变分自编码器），通过解耦时空特征同时保持局部拓扑结构，有效压缩并重建动态网格序列。为实现高质量文本条件生成，我们在压缩隐空间采用基于校正流（Rectified Flow）的训练策略。此外，我们贡献了包含超400万条带文本标注的动态网格序列数据集DyMesh Dataset。实验结果表明，本方法可在数秒内生成语义准确且时序连贯的网格动画，在质量与效率上均显著超越现有方案。本工作标志着向更易用、更实用的4D内容创作迈出重要一步。所有数据、代码和模型将开源发布。</strong></p>
<div style="break-before: page; page-break-before: always;"></div><p>Self-Supervised Multi-Part Articulated Objects Modeling via Deformable Gaussian Splatting and Progressive Primitive Segmentation</p>
<p>好的，这是这段话的中文翻译：</p>
<hr />
<p><strong>关节化物体</strong>在日常生活中广泛存在，对其几何结构和运动的精确三维表征对于众多应用至关重要。然而，在缺乏人工标注的情况下，现有方法仍然难以对包含多个可动部件的物体构建统一的表征。</p>
<p>我们提出了 <strong>DeGSS</strong>，这是一个统一的框架，它将关节化物体编码为<strong>可变形三维高斯场</strong>，将几何、外观和运动嵌入到一个紧凑的表征中。每个交互状态被建模为一个共享场的平滑变形，由此产生的变形轨迹引导着一种<strong>渐进式的从粗到细的部件分割</strong>，从而识别出不同的刚性部件——<strong>全部以无监督的方式进行</strong>。</p>
<p>精炼后的场为每个部件提供了<strong>空间连续、完全解耦的描述</strong>，支持部件级的重建及其运动学关系的精确建模。为了评估泛化能力和真实感，我们扩展了合成基准数据集 <strong>PartNet-Mobility</strong>，并发布了 <strong>RS-Art</strong>——一个<strong>真实到模拟(real-to-sim)的数据集</strong>，它将 RGB 捕获数据与精确逆向工程得到的三维模型配对。</p>
<p>大量实验表明，我们的方法在<strong>精度和稳定性</strong>方面均优于现有方法。</p>
<hr />
<div style="break-before: page; page-break-before: always;"></div><p>SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach</p>
<p><strong>动作补间（Motion In-betweening）</strong> 是动画师的关键工具，使其能够精细控制每个关键帧的姿态级别细节。近期的动作补间机器学习解决方案依赖于复杂模型，这些模型要么采用骨架感知架构，要么需要多个模块和训练步骤。在本研究中，我们引入了一个简单而有效的基于 Transformer 的框架，仅使用单个 Transformer 编码器来为动作补间任务合成逼真的运动。</p>
<p>我们发现，数据建模的选择在提升补间性能方面起着重要作用。具体而言，我们证明了：</p>
<ol>
<li><strong>增加数据量</strong>可以产生等效或更优的运动过渡；</li>
<li><strong>姿态表示方式的选择</strong>对于获得高质量结果至关重要；</li>
<li><strong>引入速度特征输入</strong>能提升动画表现。</li>
</ol>
<p>这些发现挑战了“模型复杂度是动画质量首要决定因素”的假设，并为采用更<strong>以数据为中心的动作插值方法</strong>提供了见解。</p>
<p>更多视频和补充材料请访问：<a href="https://silk-paper.github.io">https://silk-paper.github.io</a>。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>RoboSwap: A GAN-driven Video Diffusion Framework For Unsupervised Robot Arm Swapping</p>
<p>生成模型的最新进展已彻底革新了视频合成与编辑领域。然而，高质量多样化数据集的稀缺性，依然严重阻碍着视频条件化机器人学习的进展，限制了跨平台的泛化能力。在本研究中，我们致力于解决一个关键挑战：将一段视频中的机器人手臂替换为另一段视频中的手臂——这是跨具身学习(crossembodiment learning)的关键一步。与以往依赖相同环境设置下成对视频演示的方法不同，我们提出的框架 <strong>RoboSwap</strong> 能够在来自不同环境的无配对数据上运行，从而显著缓解数据收集的需求。</p>
<p>RoboSwap 引入了一种新颖的视频编辑流程，融合了生成对抗网络(GANs)和扩散模型(diffusion models)各自的优势。具体而言，我们首先将机器人手臂从背景中分割出来，然后训练一个基于无配对数据的 GAN 模型，用于将一种机器人手臂转换(翻译)成另一种。转换后的手臂与原始视频背景进行融合，并经过扩散模型的精细化处理，以增强视频的连贯性、运动真实感以及与物体交互的自然度。GAN 和扩散模型阶段采用分阶段独立训练。</p>
<p>我们的实验表明，在三个基准测试上，RoboSwap 在结构连贯性和运动一致性方面均优于最先进的视频和图像编辑模型，从而为在机器人学习中生成可靠的跨具身数据提供了一个强大的解决方案。</p>
<div style="break-before: page; page-break-before: always;"></div><p>StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated Video Streams</p>
<p>从未标定视频流中对动态3D场景进行实时重建，对于众多实际应用至关重要。然而，现有方法难以同时解决三个关键挑战：1) 实时处理未标定输入；2) 精确建模动态场景演化；3)保持长期稳定性和计算效率。为此，我们提出了 <strong>StreamSplat</strong>，这是首个完全<strong>前馈</strong>的框架，能够以<strong>在线方式</strong>将任意长度的未标定视频流转化为动态的<strong>3D高斯溅射(3DGS)</strong> 表示，并能从<strong>局部时间观测</strong>中恢复场景动态。我们提出了两项关键技术创新：在用于3DGS位置预测的静态编码器中引入<strong>概率采样机制</strong>，以及在动态解码器中引入<strong>双向形变场</strong>，从而实现鲁棒且高效的动态建模。在静态和动态基准测试上进行的大量实验表明，StreamSplat 在重建质量和动态场景建模方面始终优于先前的工作，同时独特地支持对任意长度视频流的在线重建。代码和模型可在 <a href="https://github.com/nickwzk/StreamSplat">https://github.com/nickwzk/StreamSplat</a> 获取。    </p>
<div style="break-before: page; page-break-before: always;"></div><p>Orientation Matters: Making 3D Generative Models Orientation-Aligned</p>
<p><strong>人类能够凭借对规范姿态的强先验知识，从单张图像中直观地感知物体形状和朝向。</strong> 然而，由于训练数据的不一致性，现有的3D生成模型常常产生方向错位的结果，限制了它们在下游任务中的可用性。为解决这一不足，我们提出了<strong>朝向对齐的3D物体生成</strong>任务：从单张图像生成3D物体，并要求跨类别的物体保持一致的朝向。</p>
<p>为支持此任务，我们构建了 <strong>Objaverse-OA 数据集</strong>，包含涵盖 1,008 个类别的 14,832 个朝向对齐的3D模型。利用 Objaverse-OA，我们基于多视图扩散和3D变分自编码器框架，对两个代表性的3D生成模型进行了微调，使其能够生成对齐良好的物体，并能很好地泛化到各类别中未见过的物体上。</p>
<p>实验结果表明，我们的方法显著优于后处理对齐方法。此外，我们还展示了由我们这种对齐物体生成能力所实现的下游应用，包括通过<strong>合成分析法</strong>进行零样本物体朝向估计，以及基于箭头的<strong>高效物体旋转操作</strong>。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>Generalizable Articulated Object Reconstruction from Casually Captured RGBD Videos</p>
<p><strong>铰接物体</strong>在日常生活中十分普遍。理解它们的<strong>运动学结构</strong>并对其进行重建，在<strong>具身人工智能</strong>和<strong>机器人学</strong>领域具有众多应用。然而，当前的方法需要精心采集的数据进行训练或推断，这阻碍了对铰接物体进行实用、可扩展且泛化性强的重建。</p>
<p>我们专注于如何利用<strong>手持摄像机随手拍摄的一段RGBD视频</strong>来重建一个铰接物体。通过智能手机，可以轻松地大规模获取人与铰接物体交互的随手拍摄视频。然而，这种场景极具挑战性：物体和摄像机同时移动，并且当人与物体交互时会产生<strong>显著的遮挡</strong>。</p>
<p>为应对这些挑战，我们提出了一个<strong>由粗到精的框架</strong>，该框架能够从动态的RGBD视频中推断出物体的<strong>关节参数</strong>并<strong>分割其可移动部件</strong>。为了在这种新场景下评估我们的方法，我们构建了一个<strong>20倍</strong>于现有规模的大型合成数据集，包含<strong>784个视频</strong>、<strong>284个物体</strong>，涵盖<strong>11个类别</strong>。我们将我们的方法与同样以视频作为输入的现有方法进行了比较。实验表明，我们的方法能够从动态RGBD视频中重建不同类别的合成及真实铰接物体，<strong>性能显著优于</strong>现有方法。</p>
<div style="break-before: page; page-break-before: always;"></div><p>HunyuanVideo-HOMA: Generic Human-Object Interaction in Multimodal Driven Human Animation</p>
<p>为应对人-物交互(HOI)视频生成中的关键限制——特别是对精选运动数据的依赖、对新物体/场景的泛化能力有限以及可访问性受限——我们推出了<strong>混元视频-HOMA (HunyuanVideo-HOMA)</strong>，一个<strong>弱约束的多模态驱动框架</strong>。</p>
<p>HunyuanVideo-HOMA 通过<strong>稀疏解耦的运动引导</strong>增强可控性，并减少对精确输入的依赖。它将外观和运动信号编码到<strong>多模态扩散Transformer (MMDiT)</strong> 的双重输入空间中，并在共享的上下文空间内融合这些信号，以合成时序一致且物理合理的交互。</p>
<p>为了优化训练，我们集成了两个关键组件：</p>
<ol>
<li><strong>参数空间HOI适配器</strong>：该适配器从预训练的MMDiT权重初始化，在保留先验知识的同时实现高效适配。</li>
<li><strong>面部交叉注意力适配器</strong>：用于实现解剖结构准确的音频驱动唇形同步。</li>
</ol>
<p>大量实验证实了该框架在弱监督条件下，于交互自然度和泛化能力方面达到了<strong>最先进(state-of-the-art)的性能</strong>。</p>
<p>最后，HunyuanVideo-HOMA 展示了其在文本条件生成和交互式物体操控方面的<strong>多功能性</strong>，并得到了一个<strong>用户友好型演示界面</strong>的支持。</p>
<p>项目页面位于：<a href="https://anonymous.4open.science/w/homa-page-0FBE/">https://anonymous.4open.science/w/homa-page-0FBE/</a>   </p>
<div style="break-before: page; page-break-before: always;"></div><p>PIG: Physically-based Multi-Material Interaction with 3D Gaussians</p>
<p>3D高斯溅射技术在静态与动态3D场景重建领域取得了显著成功。然而，在由3D高斯基元表征的场景中，物体间的交互存在三大缺陷：三维分割精度不足、异质材质形变失准及严重渲染伪影。为应对这些挑战，我们提出PIG：基于物理的多材质3D高斯交互技术——这是一种融合三维物体分割与高精度交互仿真的创新方法。</p>
<p>首先，本方法实现了从二维像素到三维高斯基元的快速精准映射，从而达成精确的物体级三维分割。其次，我们为场景中分割后的物体赋予独特物理属性，以实现多材质耦合交互。最后，我们创新性地将约束尺度嵌入变形梯度，通过钳制高斯基元的缩放与旋转属性消除渲染伪影，达成几何保真度与视觉一致性。</p>
<p>实验结果表明，我们的方法不仅在视觉质量上显著超越现有最优方案(SOTA)，更为物理真实感场景生成领域开辟了全新研究方向与技术路径。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>Drive Any Mesh: 4D Latent Diffusion for Mesh Deformation from Video</p>
<p><strong>我们提出 DriveAnyMesh：一种基于单目视频引导的网格驱动方法。</strong> 当前的 4D 生成技术在兼容现代渲染引擎方面面临挑战。隐式方法渲染效率低下，且对基于光栅化的引擎不友好；而骨骼方法则需要大量人工干预且缺乏跨类别的泛化能力。对现有 3D 资产进行动画处理(而非从零开始创建 4D 资产)，需要对输入的 3D 结构有深入理解。</p>
<p>为应对这些挑战，我们提出了一种 4D 扩散模型，该模型对<strong>隐变量集序列</strong>进行去噪处理，这些隐变量集随后经解码，可从点云轨迹序列生成网格动画。这些隐变量集利用了<strong>基于 Transformer 架构的变分自编码器</strong>，能够同时捕捉 3D 形状和运动信息。通过采用<strong>基于时空 Transformer 的扩散模型</strong>，信息得以在多个隐变量帧之间交换，从而提升了生成结果的效率和泛化能力。</p>
<p>我们的实验结果表明，DriveAnyMesh 能够为复杂动作快速生成高质量动画，并且与现代渲染引擎兼容。该方法在游戏和电影行业都具有应用潜力。  </p>
<div style="break-before: page; page-break-before: always;"></div><p>NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D Generation</p>
<p>3D AI生成内容(AIGC)使得任何人都能越来越容易地成为3D内容创作者。尽管近期的方法利用分数蒸馏采样(Score Distillation Sampling)从预训练的图像扩散模型中蒸馏出3D物体，但它们常常受限于不足的3D先验知识，导致多视图一致性不佳。在本工作中，我们提出了NOVA3D，一个创新的单图像到3D生成框架。我们的核心洞见在于利用预训练视频扩散模型提供的强大3D先验知识，并在多视图视频微调阶段整合几何信息。为了促进颜色域与几何域之间的信息交换，我们提出了几何-时间对齐(Geometry-Temporal Alignment, GTA)注意力机制，从而提升了泛化能力和多视图一致性。此外，我们引入了<strong>解冲突几何融合</strong>算法，该算法通过解决多视图的不准确性并消除姿态对齐中的差异，提高了纹理的保真度。大量实验验证了NOVA3D相对于现有基线的优越性。    </p>
<div style="break-before: page; page-break-before: always;"></div><p>PhysiInter: Integrating Physical Mapping for High-Fidelity Human Interaction Generation</p>
<p>在动作捕捉技术和生成式人工智能进步的推动下，利用大规模动作捕捉数据集训练生成模型以合成多样化、逼真的人体运动，已成为一个前景广阔的研究方向。然而，现有的动作捕捉技术和生成模型常常忽略物理约束，导致诸如<strong>穿透</strong>、<strong>滑动</strong>和<strong>悬空</strong>等伪影。这些问题在涉及复杂交互的多人运动生成中尤为突出。</p>
<p>为了克服这些局限，我们在整个人际交互生成流程中引入了<strong>物理映射</strong>。具体而言，通过在基于物理的模拟环境中进行<strong>运动模仿</strong>，将目标运动投影到一个物理上有效的空间。由此产生的运动经过调整，既遵循现实世界的物理约束，又保留了其原有的语义含义。这种映射不仅提高了动作捕捉数据的质量，还能直接指导生成运动的后处理。</p>
<p>鉴于多人场景独特的交互性，我们提出了一个量身定制的<strong>运动表示框架</strong>。我们引入了<strong>运动一致性损失 (Motion Consistency Loss, MC)</strong> 和<strong>基于标记的交互损失 (Marker-based Interaction Loss, MI)</strong> 来提升模型性能。实验表明，我们的方法在生成人体运动质量方面取得了令人印象深刻的结果，<strong>物理保真度提升了 3%-89%</strong>。</p>
<p>项目页面：<a href="http://yw0208.github.io/physiinter">http://yw0208.github.io/physiinter</a>    </p>
<div style="break-before: page; page-break-before: always;"></div><p>Parametric Gaussian Human Model: Generalizable Prior for Efficient and Realistic Human Avatar Modeling</p>
<p><strong>逼真可动画化的人体化身</strong>是虚拟/增强现实、远程临场和数字娱乐的关键使能技术。尽管基于3D高斯溅射(3DGS)的研究近期大幅提升了渲染质量与效率，现有方法仍面临根本性挑战：包括耗时的逐主体优化流程，以及在稀疏单目输入下泛化能力不足的问题。</p>
<p>本研究提出<strong>参数化高斯人体模型(PGHM)</strong> —— 一个将人体先验知识融入3DGS的通用高效框架，可实现基于单目视频的快速高保真化身重建。PGHM包含两大核心组件：<br />
(1) <strong>UV对齐隐式身份映射图</strong>：通过可学习特征张量紧凑编码主体特定的几何与外观；<br />
(2) <strong>解耦多头U-Net</strong>：通过条件解码器分离静态、姿态相关和视角相关成分，预测高斯属性。</p>
<p>该设计在挑战性姿态和视角下仍能保持稳健的渲染质量，同时支持高效的主体适配，无需多视角捕捉或长时间优化。实验表明，PGHM相较从零优化的方法效率显著提升：仅需约20分钟即可生成视觉质量相当的个性化化身，由此证明其在现实世界单目化身创建中的实用价值。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Revisiting Depth Representations for Feed-Forward 3D Gaussian Splatting </p>
<p>深度图在前馈式 3D 高斯泼溅 (3DGS) 流程中被广泛使用，其方法是将深度图反投影为 3D 点云以进行新视角合成。这种方法具有高效训练、可利用已知相机位姿以及几何估计准确等优势。然而，物体边界处的深度不连续性常常导致点云破碎或稀疏，从而降低渲染质量——这是基于深度的表示方法的一个众所周知的局限性。为解决此问题，我们提出了 <strong>PM-Loss</strong>，这是一种基于预训练 Transformer 预测的点图（pointmap）的新型正则化损失函数。尽管点图本身的精度可能不如深度图，但它能有效地强制几何平滑性，尤其是在物体边界附近。利用改进后的深度图，我们的方法在各种架构和场景中显著提升了前馈式 3DGS 的效果，带来了一致更好的渲染结果。我们的项目页面：<a href="https://aim-uofa.github.io/PMLoss">https://aim-uofa.github.io/PMLoss</a>    </p>
<div style="break-before: page; page-break-before: always;"></div><p>Follow-Your-Creation: Empowering 4D Creation through Video Inpainting</p>
<p>我们提出了 <strong>Follow-Your-Creation（随你所创）</strong>，一个新颖的 <strong>4D 视频创作框架</strong>，能够从单一的单目视频输入中<strong>生成并编辑 4D 内容</strong>。通过利用强大的<strong>视频修复基础模型</strong>作为<strong>生成先验</strong>，我们将 4D 视频创作任务重新定义为视频修复任务，使模型能够填充由相机轨迹变化或用户编辑造成的缺失内容。</p>
<p>为实现这一目标，我们生成了<strong>复合掩码修复视频数据</strong>，以有效地对模型进行<strong>微调</strong>，使其适用于 4D 视频生成。给定输入视频及其关联的相机轨迹，我们首先执行<strong>基于深度的点云渲染</strong>，以获得指示应被补全区域的<strong>不可见掩码</strong>。同时，引入<strong>编辑掩码</strong>以指定用户定义的修改，这些掩码与不可见掩码结合，创建出<strong>复合掩码数据集</strong>。</p>
<p>在训练过程中，我们随机采样不同类型的掩码，构建多样且具有挑战性的修复场景，从而增强模型在各种 4D 编辑和生成任务中的<strong>泛化能力和鲁棒性</strong>。为处理大幅相机运动下的<strong>时间一致性</strong>，我们设计了一种<strong>自迭代调优策略</strong>：在训练过程中逐步增大视角，并在每次微调迭代后，使用当前模型生成下一阶段的训练数据。</p>
<p>此外，我们在<strong>推理阶段</strong>引入了一个<strong>时序封装模块</strong>，用于<strong>增强生成质量</strong>。我们的方法有效利用了基础模型的先验知识，且<strong>未损害其原始性能</strong>，能够生成具有<strong>一致多视角连贯性</strong>的 4D 视频。不仅如此，我们的方法还支持<strong>基于提示的内容编辑</strong>，展现出强大的灵活性，并在<strong>质量和多功能性</strong>上显著<strong>优于当前最先进的方法</strong>。    </p>
<div style="break-before: page; page-break-before: always;"></div><p>Realizing Text-Driven Motion Generation on NAO Robot: A Reinforcement Learning-Optimized Control Pipeline</p>
<p>人形机器人运动重定向技术旨在将人类运动数据转化为机器人动作以实现模仿，虽存在显著挑战但在实际应用中具有巨大潜力。传统方法依赖于通过姿态估计或动作捕捉系统采集的人类示范数据。本文提出一种文本驱动的人形机器人运动映射方法。为克服生成运动表征与人形机器人运动学约束之间的固有差异，我们设计了基于位置范数与旋转损失函数(NPR Loss)的角度信号网络。该网络生成的关节角度将作为输入，驱动基于强化学习的全身关节运动控制策略。该策略在追踪生成运动的同时，确保机器人在执行过程中保持稳定性。实验结果表明，本方法成功实现了文本驱动的人类运动向真实人形机器人NAO的迁移。    </p>
<div style="break-before: page; page-break-before: always;"></div><p>Follow-Your-Motion: Video Motion Transfer via Efficient Spatial-Temporal Decoupled Finetuning</p>
<p>近期，视频扩散变换器(video diffusion transformer)领域的突破性进展在多样化运动生成方面展现出卓越能力。在运动迁移任务上，当前方法主要采用两阶段低秩适配(Low-Rank Adaptations, LoRAs)微调来获得更好的性能。然而，现有的基于适配的运动迁移方法在应用于大型视频扩散变换器时，仍面临运动不一致性和调优效率低下的问题。由于3D注意力算子固有的时空耦合特性，简单的两阶段LoRA调优难以维持生成视频与输入视频之间的运动一致性。此外，它们需要在两个阶段都进行耗时的微调过程。</p>
<p>为了解决这些问题，我们提出了<strong>Follow-Your-Motion</strong>，一种高效的两阶段视频运动迁移框架，它通过微调强大的视频扩散变换器来合成复杂运动。具体而言：</p>
<ol>
<li><strong>时空解耦LoRA：</strong> 我们提出了一种时空解耦的低秩适配器(LoRA)，将注意力架构解耦，分别用于处理空间外观和时间运动。</li>
<li><strong>加速调优：</strong> 在第二阶段训练期间，我们设计了<strong>稀疏运动采样</strong>和<strong>自适应RoPE</strong>(Rotary Position Embedding)技术来加速调优速度。</li>
</ol>
<p>针对该领域缺乏基准测试的问题，我们引入了<strong>MotionBench</strong>，这是一个包含多样化运动的综合性基准测试集，涵盖创意摄像机运动、单物体运动、多物体运动以及复杂人体运动。我们在MotionBench上进行了广泛的评估，验证了Follow-Your-Motion的优越性。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity</p>
<p>本文旨在仅从多视角视频中建模 3D 场景的几何结构、外观及底层物理规律。现有方法通常通过将各种控制方程 (PDE) 作为物理信息神经网络 (PINN) 损失函数，或将物理模拟引入神经网络来实现。然而，这些方法往往难以学习边界处的复杂物理运动，或者需要依赖物体先验信息（如掩码或类型）。本文提出 <strong>FreeGave</strong> 方法，能够在<strong>无需任何物体先验信息</strong>的情况下学习复杂动态 3D 场景的物理规律。我们方法的关键在于引入一个物理编码 (physics code)，并辅以一个精心设计的无散度模块 (divergence-free module)，用于估计每个高斯点的速度场，而无需依赖低效的 PINN 损失函数。在三个公开数据集和一个新收集的高难度真实世界数据集上进行的大量实验表明，我们的方法在未来帧外推和运动分割任务上具有优越性能。尤为值得注意的是，我们对学习到的物理编码的研究表明，在训练中<strong>完全未使用任何人工标注</strong>的情况下，它们确实学习到了有意义的 3D 物理运动模式。我们的代码和数据可在 <a href="https://github.com/vLAR-group/FreeGave">https://github.com/vLAR-group/FreeGave</a> 获取。</p>
<div style="break-before: page; page-break-before: always;"></div><p>POMP: Physics-consistent Motion Generative Model through Phase Manifolds</p>
<p>大量关于实时运动生成的研究主要聚焦于运动学层面，这常常导致物理上失真的结果。在本文中，我们提出了POMP(&quot;<strong>P</strong>hysics-consistent Human <strong>M</strong>otion <strong>P</strong>rior through Phase Manifolds&quot;，即&quot;<strong>通过相流形实现物理一致的人体运动先验</strong>&quot;)，这是一个新颖的、基于运动学的框架。它利用相流形来对齐运动先验与物理约束，从而合成物理一致的运动。</p>
<p>POMP作为一个逐帧的自回归模型运行，包含三个核心组件：一个基于扩散的运动学模块、一个基于仿真的动力学模块和一个相位编码模块。在每一时间步，运动学模块先生成一个初始目标姿态，随后该姿态由动力学模块进行优化(通过模拟人-环境交互)。虽然物理仿真确保了遵循物理定律，但它可能会损害姿态的运动学合理性。因此，直接将仿真结果用于后续帧预测可能会导致误差累积。</p>
<p>为了解决这个问题，相位编码模块在相流形中进行语义对齐。此外，我们提出了一套在Unity引擎中实现的流程，用于生成地形图并从现有的运动捕捉(MoCap)数据中提取全身运动冲量。所收集的地形拓扑和运动冲量数据促进了POMP的训练，使其能够稳健地响应基础接触力和施加的动力学。</p>
<p>广泛的评估表明，POMP在各种情境、地形和物理交互中均表现出色。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>SinGS: Animatable Single-Image Human Gaussian Splats with Kinematic Priors</p>
<p>尽管当代单图像3D人体重建在精确估计几何形状方面取得了显著进展，但创建高质量、高效且<strong>可驱动</strong>的三维化身仍然是一个<strong>开放挑战</strong>。两个关键障碍持续存在：<strong>观测不完整</strong>和<strong>3D先验不一致</strong>。为了应对这些挑战，我们提出了SinGS，旨在实现高质量且高效的可驱动三维化身重建。</p>
<p>SinGS的核心包含两个关键组件：<strong>运动学人体扩散 (Kinematic Human Diffusion)</strong> 和<strong>几何保持的3D高斯泼溅 (Geometry-Preserving 3D Gaussian Splatting)</strong>。前者是一个基础人体模型，它在姿态空间内进行<strong>采样</strong>，生成一个高度<strong>3D一致</strong>且高质量的人体图像序列，从而推断<strong>未观测视角</strong>并提供<strong>运动学先验</strong>。后者是一个重建系统，即使在<strong>不完美先验</strong>条件下，也能重建出<strong>紧凑</strong>、<strong>高质量</strong>的3D化身。这是通过一种新颖的<strong>语义拉普拉斯正则化 (semantic Laplacian regularization)</strong> 和一种<strong>几何保持的密度控制策略 (geometry-preserving density control strategy)</strong> 实现的，该策略能够<strong>精确且紧凑地组装3D基元 (3D primitives)</strong>。</p>
<p>大量实验表明，SinGS能够实现<strong>逼真的、可驱动的人体重建</strong>，同时保持了<strong>高质量</strong>和<strong>高推理效率（高达70FPS）</strong>。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>SplArt: Articulation Estimation and Part-Level Reconstruction with 3D Gaussian Splatting</p>
<p><strong>重建日常环境中普遍存在的铰接物体</strong>对于增强现实/虚拟现实和机器人技术应用至关重要。然而，现有方法面临可扩展性限制(需要3D监督或昂贵的标注)、鲁棒性问题(易陷入局部最优)以及渲染缺陷(速度慢或缺乏照片级真实感)。我们提出了 <strong>SplArt</strong>，这是一个自监督、类别无关的框架。它利用3D高斯溅射(3DGS)技术，仅需在不同关节状态下捕获的两组带位姿的RGB图像，即可重建铰接物体并推断其运动学，从而能够对新颖视角和关节状态进行实时照片级真实感的渲染。</p>
<p>SplArt通过为每个高斯点引入一个<strong>可微分的运动参数</strong>来增强3DGS，实现了精细化的部件分割。该框架采用<strong>多阶段优化策略</strong>，逐步处理重建、部件分割和关节估计，显著提高了鲁棒性和准确性。SplArt充分利用<strong>几何自监督</strong>，无需3D标注或特定类别的先验知识，即能有效应对各种具有挑战性的场景。在既有基准和新提出的基准上进行的评估，以及使用手持RGB相机在真实场景中的应用，均证明了SplArt的<strong>领先性能</strong>和<strong>实际应用价值</strong>。代码已在 <a href="https://github.com/ripl/splart">https://github.com/ripl/splart</a> 公开。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>HuGeDiff: 3D Human Generation via Diffusion with Gaussian Splatting</p>
<p><strong>3D人体生成</strong>是计算机视觉和图形学领域中一个具有广泛应用的重要问题。尽管在生成式人工智能(如扩散模型)和渲染方法(如神经辐射场或高斯溅射)方面取得了最新进展，但根据文本提示精确控制3D人体的生成仍然是一个未解决的挑战。现有方法在细节精细度、手部和面部精准渲染、人体真实感以及外观可控性方面存在困难。人体图像数据在多样性、真实性和标注方面的不足也仍然是个挑战，阻碍了基础性3D人体模型的发展。</p>
<p>我们提出了一种<strong>弱监督流程</strong>来尝试应对这些挑战。在第一步中，我们使用<strong>最先进的图像扩散模型</strong>生成了一个具有可控属性(如外貌、种族、性别等)的<strong>高真实感人像数据集</strong>。接下来，我们提出了一种<strong>高效的映射方法</strong>，使用基于<strong>Transformer的架构</strong>将图像特征映射到<strong>3D点云</strong>。最后，我们通过训练一个<strong>点云扩散模型</strong>来形成闭环，该模型以用于生成原始样本的相同<strong>文本提示</strong>为条件。</p>
<p>与现有最先进方法相比，我们展示了<strong>数量级的3D人体生成速度提升</strong>，同时显著改善了<strong>文本提示对齐度、真实感和渲染质量</strong>。我们将<strong>开放代码和数据集</strong>。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>Multimodal Generative AI with Autoregressive LLMs for Human Motion Understanding and Generation: A Way Forward</p>
<p>本文对利用多模态生成式人工智能(GenAI)和自回归大语言模型(LLM)进行人体运动理解与生成的研究进行了深入综述，探讨了新兴方法、架构及其在推动逼真且多样化运动合成方面的潜力。本研究专注于文本与运动两种模态，探究了文本描述如何指导生成复杂、类人的运动序列。文章分析了多种生成方法(包括自回归模型、扩散模型、生成对抗网络(GANs)、变分自编码器(VAEs)以及基于Transformer的模型)在运动质量、计算效率和适应性方面的优势与局限。重点探讨了文本条件运动生成领域的最新进展，即利用文本输入来更精确地控制和优化运动输出。通过整合大语言模型(LLMs)，进一步增强了这些模型的能力，实现了指令与运动之间的语义对齐，从而提升了连贯性和情境相关性。本系统性综述强调了文本到运动(text-to-motion)的GenAI和LLM架构在医疗保健、人形机器人、游戏、动画和辅助技术等应用中的变革潜力，同时也探讨了在生成高效且逼真人运动方面持续存在的挑战。 </p>
<div style="break-before: page; page-break-before: always;"></div><p>LinkTo-Anime: A 2D Animation Optical Flow Dataset from 3D Model Rendering</p>
<p><strong>现有光流数据集主要集中于真实世界模拟或合成人体运动，但专为赛璐珞(Cel)动画角色运动设计的数据集却寥寥无几——这是一个具有独特视觉和运动特征的领域。为填补这一空白，并促进光流估计以及动漫视频生成、线稿上色等下游任务的研究，我们推出了 LinkTo-Anime 数据集。这是首个基于 3D 模型渲染生成的、专为赛璐珞动画角色运动设计的高质量数据集。</strong></p>
<p><strong>LinkTo-Anime 提供了丰富的标注信息，包括前向与后向光流、遮挡掩码以及 Mixamo 骨架。该数据集包含 395 个视频序列，总计 24,230 帧训练图像、720 帧验证图像和 4,320 帧测试图像。此外，我们还利用多种光流估计方法构建了一个全面的基准测试，以分析多个数据集中存在的不足和局限性。</strong>    </p>
<div style="break-before: page; page-break-before: always;"></div><p>EnliveningGS: Active Locomotion of 3DGS</p>
<p>本文提出了一种名为 <strong>EnliveningGS</strong> 的全新流程，它使得用 3D 高斯溅射(3DGS)表示的 3D 模型能够实现主动运动。我们的灵感来源于现实世界中生物通过<strong>压缩或拉伸</strong>其体内嵌入的肌纤维，以自然且符合物理规律的方式摆出身体姿态。EnliveningGS 旨在为 3DGS 模型复制类似的功能，从而使 3DGS 场景中的物体表现得如同活物，而非静态形状——它们在肌肉激活驱动的运动轨迹下，能够在场景中<strong>行走、跳跃和扭转</strong>。</p>
<p>尽管概念直观，但其中涉及许多需要解决的技术挑战。合成 3DGS 模型的逼真运动体现了一个<strong>非常高维度的逆向物理问题</strong>。核心挑战在于如何高效且鲁棒地建模“活化模型”与环境之间的<strong>摩擦接触</strong>，因为正是由肌肉激活引发的接触/碰撞/摩擦力共同作用，才最终生成了物体的运动。我们提出了一种<strong>混合数值方法</strong>，结合了<strong>线性互补问题(LCP)求解</strong>和<strong>惩罚法</strong>，以鲁棒地解决这个 <strong>NP 难题</strong>。</p>
<p>此外，我们的流程也解决了现有 3DGS 形变算法的局限性，并能够在模型移动时<strong>修复缺失的信息</strong>。</p>
<div style="break-before: page; page-break-before: always;"></div><p>AniMo: Species-Aware Model for Text-Driven Animal Motion Generation</p>
<p>近年来，文本驱动的运动生成技术取得了显著进展。然而，现有研究大多聚焦于人体运动，很大程度上忽视了动物丰富多样的行为。理解和合成动物运动在野生动物保护、动物生态学和生物力学等领域具有重要应用价值。</p>
<p>动物运动建模面临着独特的挑战，这源于物种的多样性、形态结构的差异以及面对相似文本描述时不同的行为模式。为应对这些挑战，我们提出了 <strong>AniMo</strong> 用于文本驱动的动物运动生成。</p>
<p>AniMo 包含两个阶段：<strong>运动标记化</strong>和<strong>文本到运动生成</strong>。</p>
<ol>
<li><strong>在运动标记化阶段</strong>，我们使用一个带有<strong>物种感知特征调制</strong>功能的<strong>关节感知时空编码器</strong>对运动进行编码，使模型能够适应不同物种的多样化骨骼结构。</li>
<li><strong>在文本到运动生成阶段</strong>，我们采用<strong>掩码建模</strong>技术联合学习文本描述与运动标记之间的映射关系。</li>
</ol>
<p>此外，我们引入了 <strong>AniMo4D</strong> 大规模数据集，该数据集包含覆盖 114 个动物物种的 78,149 个运动序列和 185,435 条文本描述。实验结果表明，AniMo 在 AniMo4D 和 AnimalML3D 数据集上均取得了卓越的性能，能够有效捕捉不同动物物种的多样化形态结构和行为模式。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Controllable Human-centric Keyframe Interpolation with Generative Prior</p>
<p><strong>现有</strong>的插帧方法利用预训练的视频扩散模型先验知识，在稀疏采样的关键帧之间生成中间帧。<strong>然而</strong>，由于缺乏三维几何指导，这些方法在处理复杂、关节式的人体运动时难以生成合理的插帧效果，并且对合成动态效果的控制能力有限。</p>
<p><strong>本文中</strong>，我们提出了 <strong>PoseFuse3D关键帧插值器(PoseFuse3D-KI)</strong>，这是一种新颖的框架，它将三维人体引导信号集成到扩散过程中，以实现<strong>可控的人体中心关键帧插值(Controllable Human-centric Keyframe Interpolation, CHKI)</strong>。为了给插帧提供丰富的空间和结构线索，我们的三维感知控制模型 <strong>PoseFuse3D</strong> 包含两个核心组件：一个<strong>新颖的 SMPL-X 编码器</strong>，负责将三维几何和形状信息转换到二维潜在条件空间；以及一个<strong>融合网络</strong>，用于将这些三维线索与二维姿态嵌入进行整合。</p>
<p><strong>为了评估</strong>，我们构建了 <strong>CHKI-Video</strong> 数据集，这是一个包含二维姿态和三维 SMPL-X 参数标注的新数据集。<strong>实验表明</strong>，PoseFuse3D-KI 在 CHKI-Video 数据集上始终优于当前最先进的基线方法，峰值信噪比(PSNR)提升了 9%，学习感知图像块相似度(LPIPS)降低了 38%。<strong>全面的消融实验</strong>证明，我们的 PoseFuse3D 模型有效提升了插帧的保真度。</p>
<div style="break-before: page; page-break-before: always;"></div><p>SViMo: Synchronized Diffusion for Video and Motion Generation in Hand-object Interaction Scenarios</p>
<p><strong>人-物交互（HOI）生成</strong>具有显著的应用潜力。然而，当前的三维HOI运动生成方法严重依赖于预定义的三维物体模型和实验室捕获的运动数据，这限制了其泛化能力。同时，HOI视频生成方法优先考虑像素级的视觉保真度，常常牺牲了物理合理性。</p>
<p>我们认识到，在现实世界中，视觉外观和运动模式遵循着相同的基本物理定律。因此，我们提出了一种新颖的框架，该框架将<strong>视觉先验</strong>和<strong>动态约束</strong>结合在一个<strong>同步扩散过程</strong>中，以同时生成HOI视频和运动数据。</p>
<p>为了整合异构的语义、外观和运动特征，我们的方法实现了<strong>三模态自适应调制</strong>以进行特征对齐，并结合<strong>三维全注意力机制</strong>来建模模态间和模态内的依赖关系。</p>
<p>此外，我们引入了一个<strong>视觉感知的三维交互扩散模型</strong>。该模型直接从同步扩散的输出中生成<strong>显式的三维交互序列</strong>，然后将其反馈回去，形成一个<strong>闭环反馈机制</strong>。这种架构消除了对预定义物体模型或显式姿态指导的依赖，同时显著增强了<strong>视频-运动一致性</strong>。</p>
<p>实验结果表明，我们的方法在生成高保真度、动态合理的HOI序列方面优于最先进的方法，并在未见过的真实世界场景中展现出卓越的泛化能力。项目页面位于：&lt;\href{https://github.com/Droliven}{https://github.com/Droliven}&gt;。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>HOSIG: Full-Body Human-Object-Scene Interaction Generation with Hierarchical Scene Perception</p>
<p>在计算机图形学与动画领域，如何生成高保真、涉及动态物体和静态场景的全身人体交互，仍然是一个关键挑战。现有的人-物交互方法常常忽略场景上下文，导致不合理的穿模现象；而人-场景交互方法则难以协调精细的操作与长距离导航。为克服这些局限，我们提出了 HOSIG —— 一种通过<strong>分层场景感知</strong>合成全身交互的新型框架。</p>
<p>我们的方法将任务解耦为三个关键组件：</p>
<ol>
<li><strong>场景感知抓取姿态生成器</strong>：通过整合局部几何约束，确保生成无碰撞的全身姿态，并实现精确的手-物接触；</li>
<li><strong>启发式导航算法</strong>：利用压缩的二维平面图(2D floor maps)和双组件空间推理，在复杂室内环境中自主规划避障路径；</li>
<li><strong>场景引导的运动扩散模型</strong>：通过融入空间锚点和双空间无分类器引导(dual-space classifier-free guidance)，生成具有轨迹控制、手指级精度的全身运动。</li>
</ol>
<p>在 TRUMANS 数据集上进行的大量实验表明，其性能优于当前最先进(state-of-the-art)的方法。值得注意的是，我们的框架通过自回归生成支持无限长度的运动，并且只需极少的人工干预。这项工作弥合了场景感知导航与灵巧物体操控之间的关键鸿沟，推动了具身交互合成的前沿发展。代码将在论文发表后开源。项目主页：<a href="http://yw0208.github.io/hosig">http://yw0208.github.io/hosig</a>   </p>
<div style="break-before: page; page-break-before: always;"></div><p>DiffuseSlide: Training-Free High Frame Rate Video Generation Diffusion </p>
<p>扩散模型的最新进展彻底革新了视频生成技术，使得创建高质量、时间一致性的视频成为可能。然而，在长序列视频尤其是快速运动场景中，由于闪烁与质量下降等问题，生成高帧率视频仍面临重大挑战。现有方法常受限于计算效率低下，且在长帧序列中难以维持视频质量。本文提出一种基于预训练扩散模型的高帧率视频生成新方法，全程无需额外训练。我们的DiffuseSlide方案构建了创新流程：通过提取低帧率视频的关键帧，应用噪声重注入与滑动窗口潜空间去噪等技术，无需微调即可生成流畅连贯的视频。大量实验表明，该方法显著提升了视频质量，增强了时间连贯性与空间保真度。该方案不仅计算高效，还能适应各类视频生成任务，特别适用于虚拟现实、电子游戏及高质量内容创作等场景。     </p>
<div style="break-before: page; page-break-before: always;"></div><p>UMA: Ultra-detailed Human Avatars via Multi-level Surface Alignment</p>
<p>基于多视角视频学习具有生动动态效果与照片级真实感的可动画着装人体模型，是计算机图形学与视觉领域的重要基础研究课题。得益于隐式表示技术的最新进展，通过将隐式表示绑定至可驱动的人体模板网格，当前可动画化身的建模质量已达到前所未有的水平。然而，此类方法通常难以保留最高级别的细节特征，在虚拟摄像机拉近视角以及进行4K及以上分辨率渲染时尤为明显。</p>
<p>我们认为，这一局限源于表面追踪的不准确性——具体表现为角色几何体与真实表面间的深度失准和表面漂移问题，迫使细节外观模型不得不补偿几何误差。为解决该问题，我们提出一种潜在变形模型，并利用基础性2D视频点追踪器的引导信号来监督可动画角色的三维形变。相比可微分渲染，点追踪器对光照变化和表面差异具有更强的鲁棒性，且更不易陷入局部最优解。</p>
<p>为克服2D点追踪器随时间累积的漂移问题及其缺乏三维空间感知的缺陷，我们提出级联式训练策略：通过将点追踪轨迹锚定至渲染的虚拟化身，生成具有一致性的3D点追踪轨迹，最终在顶点和纹理元素(texel)级别监督我们的虚拟化身模型。</p>
<p>为验证方法的有效性，我们构建了一个包含五段多视角视频序列的新型数据集：每段时长超10分钟，使用40台标定过的6K分辨率摄像机采集，拍摄对象身着具有挑战性纹理图案与褶皱变形的服装。实验证明，我们的方法在渲染质量与几何精度上显著超越了现有最先进技术。</p>
<div style="break-before: page; page-break-before: always;"></div><p>FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video Generation</p>
<p>文本到视频扩散模型在建模运动、物理和动态交互等时间层面方面的能力存在众所周知的局限性。现有方法通过重新训练模型或引入外部条件信号来强制时间一致性，以解决这一限制。</p>
<p>在本研究中，我们探讨是否可以直接从预训练模型的预测中提取有意义的时间表征，而无需任何额外训练或辅助输入。我们提出了 <strong>FlowMo</strong>，一种新颖的无训练引导方法，它仅利用模型自身在每个扩散步骤中的预测来增强运动连贯性。</p>
<p>FlowMo首先通过测量对应于连续帧的潜在表示之间的距离，推导出一个<strong>外观去偏的时间表征</strong>。这突显了模型预测的隐含时间结构。然后，它通过测量时间维度上的<strong>块级方差</strong>来估计运动连贯性，并在采样过程中动态地引导模型减少这种方差。</p>
<p>在多个文本到视频模型上进行的大量实验表明，FlowMo 在不牺牲视觉质量或提示对齐的前提下，显著提高了运动连贯性，为增强预训练视频扩散模型的时间保真度提供了一种有效的即插即用解决方案。</p>
<div style="break-before: page; page-break-before: always;"></div><p>TRiMM: Transformer-Based Rich Motion Matching for Real-Time multi-modal Interaction in Digital Humans</p>
<p>由大型语言模型(LLM)驱动的数字人，近期引发了一系列关于伴随语音手势生成系统的研究。然而，现有方法在实时合成和长文本理解方面存在困难。</p>
<p>本文介绍了<strong>基于Transformer的丰富动作匹配(TRiMM)</strong>，这是一种用于实时3D手势生成的新型多模态框架。我们的方法包含三个模块：</p>
<ol>
<li><strong>跨模态注意力机制</strong>：实现语音与手势之间的精确时间对齐；</li>
<li><strong>带滑动窗口机制的长上下文自回归模型</strong>：用于有效的序列建模；</li>
<li><strong>大规模手势匹配系统</strong>：构建原子动作库并支持实时检索。</li>
</ol>
<p>此外，我们开发了一个在虚幻引擎(Unreal Engine)中实现的轻量级实验管线。我们的方法在消费级显卡(Geforce RTX3060)上实现了<strong>120 FPS的实时推理</strong>，并保持<strong>每句0.15秒的延迟</strong>。</p>
<p>在ZEGGS和BEAT数据集上进行的大量主观和客观评估表明，我们的模型性能优于当前最先进的方法。TRiMM在保证手势质量的同时，显著提升了伴随语音手势的生成速度，使得由LLM驱动的数字人能够实时响应语音并合成对应的手势动作。</p>
<p>我们的代码开源地址：<a href="https://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matching">https://github.com/teroon/TRiMM-Transformer-Based-Rich-Motion-Matching</a>    </p>
<div style="break-before: page; page-break-before: always;"></div><p>UniGeo: Taming Video Diffusion for Unified Consistent Geometry Estimation</p>
<p>近年来，利用扩散模型先验知识辅助单目几何估计(如深度与法线预测)的方法因其强大的泛化能力受到广泛关注。然而，现有研究多聚焦于在单帧图像的相机坐标系内估计几何属性，忽视了扩散模型本身具备的帧间对应关系判定能力。本研究证明：通过合理设计与微调，视频生成模型固有的内在一致性可有效用于实现连贯的几何估计。具体而言，我们提出：1)选择与视频帧共享对应关系的全局坐标系几何属性作为预测目标；2)通过复用位置编码提出新型高效条件化方法；3)利用共享对应关系的多几何属性联合训练提升性能。该方法在视频全局几何属性预测中达到领先性能，可直接应用于重建任务。即使仅使用静态视频数据训练，本方案亦展现出向动态视频场景泛化的潜力。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>LTM3D: Bridging Token Spaces for Conditional 3D Generation with Auto-Regressive Diffusion Framework</p>
<p>我们提出LTM3D——一种用于条件化3D形状生成的隐式令牌空间建模框架，该框架融合了扩散模型与自回归(AR)模型的优势。尽管基于扩散的方法能有效建模连续隐空间，而自回归模型擅长捕捉令牌间依赖关系，但将这两种范式结合用于3D形状生成仍具挑战性。为此，LTM3D设计了<strong>条件分布建模主干网络</strong>，通过掩码自编码器与扩散模型增强令牌依赖学习。此外，我们引入<strong>前缀学习技术</strong>，在生成过程中对齐条件令牌与形状隐令牌，提升跨模态灵活性。我们进一步提出带<strong>重构引导采样</strong>的<strong>隐令牌重建模块</strong>，以降低生成形状的不确定性并增强结构保真度。该方法在令牌空间中运行，可支持多种3D表示形式(包括符号距离场、点云、网格和3D高斯溅射)。在图像与文本条件化形状生成任务上的大量实验表明：LTM3D在提示保真度和结构准确性上均超越现有方法，同时为多模态、多表示的3D生成提供了通用化框架。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>DreamDance: Animating Character Art via Inpainting Stable Gaussian Worlds</p>
<p>本文提出 DreamDance，一种新颖的角色美术动画框架，能够基于精确的相机轨迹生成稳定、一致的角色与场景运动。为实现此目标，我们将动画任务重新构建为两个基于修复的步骤：相机感知场景修复(Camera-aware Scene Inpainting)和姿态感知视频修复(Pose-aware Video Inpainting)。</p>
<p>第一步利用预训练的图像修复模型，从参考美术作品生成多视角场景图像，并优化一个稳定的大规模高斯场(Gaussian field)，从而实现基于相机轨迹的粗略背景视频渲染。然而，渲染出的视频较为粗糙且仅包含场景运动。</p>
<p>为解决此问题，第二步训练一个姿态感知视频修复模型，该模型将动态角色注入场景视频中，同时提升背景质量。具体来说，该模型是一个基于 DiT(Diffusion Transformer)的视频生成模型，采用门控策略(gating strategy)自适应地将角色的外观和姿态信息融合到基础背景视频中。</p>
<p>通过大量实验，我们证明了 DreamDance 的有效性和强泛化性(generalizability)，能够生成具有显著相机动态效果的高质量、一致性角色动画。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>AdaHuman: Animatable Detailed 3D Human Generation with Compositional Multiview Diffusion</p>
<p>现有图像转3D头像生成方法难以创建适用于实际应用场景的高细节动画级虚拟形象。我们提出<strong>AdaHuman</strong>——一种从单张真实场景图像生成高保真可动画3D头像的创新框架。该框架包含两项核心技术突破：<br />
(1) <strong>姿态条件化三维关节扩散模型</strong>：在扩散过程的每一步同步生成任意姿态下一致的多视角图像及对应的三维高斯泼溅(3DGS)重建；<br />
(2) <strong>组合式3DGS优化模块</strong>：通过图像到图像的局部精细化增强身体部位细节，并利用创新的<em>裁剪感知相机射线图</em>实现无缝集成，最终输出完整的高精度3D虚拟形象。</p>
<p>该框架能生成具有最小自遮挡的高真实度标准A姿态虚拟形象，支持配合任意输入动作进行骨骼绑定与动画驱动。在公开基准和真实场景图像上的全面评估表明，AdaHuman在头像重建与姿态重定向任务中均显著超越现有最先进方法。研究用代码与模型将公开提供。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>Generating Fit Check Videos with a Handheld Camera</p>
<p>自拍全身视频很受欢迎，但大多数现有方案需要固定安装的摄像头、精心构图的画面以及反复练习。我们提出了一种更便捷的解决方案，<strong>仅需手持移动设备即可实现全身视频捕捉</strong>。</p>
<p>我们的方法以您在镜子前拍摄的两张静态照片(正面和背面)作为输入，同时配合您手持手机时记录的一段<strong>惯性测量单元(IMU)运动参考数据</strong>，最终<strong>合成一段您执行类似目标动作的真实感视频</strong>。我们还能<strong>支持将生成的人物渲染到新场景中，并保持光照和阴影的一致性</strong>。</p>
<p>为了实现这一点，我们提出了一种<strong>基于视频扩散的新模型</strong>。具体来说，我们提出了一种<strong>免参数的帧生成策略</strong>以及一种<strong>多参考注意力机制</strong>，有效地将来自正反两面自拍照片的外观信息整合到视频扩散模型中。此外，我们引入了一种<strong>基于图像的微调策略</strong>，以增强帧的清晰度，并改善阴影和反射的生成，从而实现更逼真的<strong>人景合成效果</strong>。</p>
<div style="break-before: page; page-break-before: always;"></div><p>GeoMan: Temporally Consistent Human Geometry Estimation using Image-to-Video Diffusion</p>
<p><strong>从视频中估算精确且时序一致的3D人体几何形状是计算机视觉领域的一个具有挑战性的问题。</strong> 现有方法主要针对单张图像进行优化，通常存在时序不一致的问题，并且难以捕捉细粒度的动态细节。为了克服这些局限，我们提出了 <strong>GeoMan</strong>，这是一种新颖的架构，旨在从单目人体视频中生成精确且时序一致的深度图和法线图。</p>
<p>GeoMan 解决了两个关键挑战：<strong>高质量4D训练数据的稀缺性</strong>以及<strong>精确建模人体尺寸所需的度量深度估计</strong>。</p>
<ul>
<li><strong>针对第一个挑战(数据稀缺)</strong>：GeoMan 采用一个基于图像的模型来估算视频第一帧的深度和法线，然后用其结果作为条件输入到一个视频扩散模型中。这种设计将视频几何形状估计任务<strong>重新定义为图像到视频的生成问题</strong>。它将几何估计的重任转移给了图像模型，从而简化了视频模型的作用，使其能够专注于利用从大规模视频数据集中学习到的先验知识来生成复杂的细节。因此，GeoMan 在<strong>仅需极少量4D训练数据</strong>的情况下，显著提升了时序一致性和泛化能力。</li>
<li><strong>针对第二个挑战(精确人体尺寸估计)</strong>：我们引入了一种<strong>根节点相对深度表示法</strong>。这种方法保留了关键的人体尺度细节信息，并且更容易从单目输入中进行估计，从而克服了传统的<strong>仿射不变深度表示</strong>和<strong>度量深度表示</strong>的局限性。</li>
</ul>
<p>GeoMan 在<strong>定性和定量评估</strong>中均取得了<strong>最先进的性能</strong>，证明了其在克服视频3D人体几何形状估计中长期存在的挑战方面的有效性。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>MMGT: Motion Mask Guided Two-Stage Network for Co-Speech Gesture Video Generation</p>
<p><strong>伴随语音的手势视频生成</strong>(Co-Speech Gesture Video Generation)旨在根据音频驱动的静态图像生成生动逼真的说话视频。这一任务极具挑战性，因为身体不同部位在<strong>运动幅度、音频相关性和细节特征</strong>方面存在巨大差异。仅依赖音频作为控制信号往往难以捕捉视频中的大幅手势动作，导致更明显的<strong>伪影和失真</strong>。</p>
<p>现有方法通常通过引入<strong>额外的先验信息</strong>来解决此问题，但这可能会限制任务的实际应用。具体而言，我们提出了一种<strong>运动掩码引导的两阶段网络(MMGT)</strong>。该网络利用音频，以及从音频信号生成的<strong>运动掩码</strong>和<strong>运动特征</strong>，共同驱动生成同步的语音-手势视频。</p>
<ul>
<li><strong>第一阶段</strong>：<strong>空间掩码引导的音频姿态生成(SMGA)网络</strong> 从音频生成高质量的姿态视频和运动掩码，有效捕捉面部和手势等关键区域的大幅度运动。</li>
<li><strong>第二阶段</strong>：我们将<strong>运动掩码分层音频注意力机制(MM-HAA)</strong> 集成到<strong>稳定扩散视频生成模型</strong>中，克服了传统方法在<strong>细粒度运动生成</strong>和<strong>区域特定细节控制</strong>方面的局限性。这保证了生成具有精确纹理和运动细节的<strong>高质量上半身视频</strong>。</li>
</ul>
<p>评估结果表明，该方法在<strong>视频质量、唇形同步和手势自然度</strong>方面均有提升。模型和代码开源地址：<a href="https://github.com/SIA-IDE/MMGT">https://github.com/SIA-IDE/MMGT</a>。    </p>
<div style="break-before: page; page-break-before: always;"></div><p>ATI: Any Trajectory Instruction for Controllable Video Generation</p>
<p>我们提出了一种用于视频生成中运动控制的统一框架，该框架无缝集成了基于轨迹输入的摄像机运动、物体级平移和精细局部运动。与以往通过独立模块或针对特定任务的设计来处理这些运动类型的方法不同，我们的方法通过一个轻量级的<strong>运动注入模块</strong>，将用户定义的轨迹投影到预训练的图像到视频生成模型的<strong>隐空间</strong>中，从而提供了一体化的解决方案。</p>
<p>用户可以通过指定关键点及其运动路径来控制局部变形、整个物体运动、虚拟摄像机动态或它们的组合。注入的轨迹信号引导生成过程，产生时间一致且语义对齐的运动序列。</p>
<p>我们的框架在多种视频运动控制任务中展现出卓越性能，包括风格化运动效果(例如<strong>运动笔刷</strong>)、动态视角变化以及精确的局部运动操控。实验表明，与先前方法和商业解决方案相比，我们的方法在提供显著更好的可控性和视觉质量的同时，仍能广泛兼容各种先进的视频生成骨干模型。</p>
<p>项目主页：<a href="https://anytraj.github.io/">https://anytraj.github.io/</a>。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference Optimization and Temporal Motion Modulation</p>
<p><strong>由于需要实现精确的唇形同步、自然的面部表情以及高保真的身体运动动态，生成由音频和骨骼运动驱动的高度动态且具有照片级真实感的肖像动画仍然具有挑战性。</strong> 我们提出了一个<strong>人类偏好对齐的扩散框架</strong>，通过两项关键创新来解决这些挑战。</p>
<p><strong>首先，</strong> 我们引入了<strong>专为以人为中心的动画设计的直接偏好优化方法</strong>。该方法利用精心筛选的人类偏好数据集，使生成结果在肖像运动-视频对齐度和表情自然度等感知指标上与人类偏好保持一致。</p>
<p><strong>其次，</strong> 提出的<strong>时序运动调制机制</strong>解决了时空分辨率不匹配的问题。它通过<strong>时序通道重分布</strong>和<strong>比例特征扩展</strong>，将运动条件重塑为维度对齐的隐特征，从而在基于扩散的合成中<strong>保留高频运动细节的保真度</strong>。</p>
<p>所提出的机制与现有的基于UNet和DiT的肖像扩散方法是<strong>互补</strong>的。实验表明，相较于基线方法，该方法在<strong>唇形-音频同步性、表情生动性以及身体运动连贯性</strong>方面取得了显著提升，同时在<strong>人类偏好指标</strong>上也获得了可观的增益。</p>
<p>我们的模型和源代码可在以下网址找到：<a href="https://github.com/xyz123xyz456/hallo4">https://github.com/xyz123xyz456/hallo4</a>   </p>
<div style="break-before: page; page-break-before: always;"></div><p>HyperMotion: DiT-Based Pose-Guided Human Image Animation of Complex Motions</p>
<p><strong>扩散模型的最新进展显著提升了条件视频生成的效果，尤其是在姿态引导的人体图像动画任务上。</strong> 尽管现有方法能够在常规运动和静态场景中生成高保真度且时序一致的动画序列，但在面对包含高度动态、非标准运动的复杂人体运动(超动态运动，Hypermotion)时仍存在明显局限，并且缺乏用于评估复杂人体运动动画的高质量基准。</p>
<p>为了应对这一挑战，我们引入了 <strong>Open-HyperMotionX 数据集</strong> 和 <strong>HyperMotionX 评测基准</strong>。该数据集提供高质量的人体姿态标注和精选的视频片段，用于在复杂人体运动条件下评估和改进姿态引导的人体图像动画模型。</p>
<p>此外，我们提出了一个简洁而强大的、基于扩散Transformer (DiT) 的视频生成基线模型，并设计了<strong>空间低频增强RoPE</strong>(一种新颖的模块)。该模块通过引入可学习的频率缩放因子，选择性地增强对空间低频特征的表征能力。我们的方法显著提高了在高度动态人体运动序列中的结构稳定性和外观一致性。</p>
<p>大量实验证明了我们提出的数据集和方法在提升复杂人体运动图像动画生成质量方面的有效性。代码和数据集将公开提供。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>LatentMove: Towards Complex Human Movement Video Generation </p>
<p><strong>图像到视频(I2V)生成</strong>旨在从单张参考图像生成逼真的运动序列。尽管现有方法展现出较强的时间一致性，但在处理<strong>复杂、非重复的人体运动</strong>时往往效果不佳，导致出现“不自然变形”。为解决此问题，我们提出了<strong>LatentMove</strong>，这是一个专门为高度动态的人体动画量身定制的、<strong>基于DiT(扩散Transformer)的框架</strong>。我们的架构融合了一个<strong>条件控制分支</strong>以及<strong>可学习的面部/身体标记(tokens)</strong>，以保持帧间一致性以及细粒度细节。我们引入了<strong>Complex-Human-Videos (CHV)</strong> 数据集，该数据集包含多样且富有挑战性的人体动作，旨在<strong>评估I2V系统的鲁棒性</strong>。我们还引入了两个新的<strong>评估指标</strong>，用于衡量生成视频与其真实视频(ground truth)在<strong>光流(flow)</strong> 和<strong>轮廓(silhouette)一致性</strong>上的差异。实验结果表明，LatentMove<strong>显著提升了人体动画质量</strong>——尤其是在处理快速、复杂动作时——从而<strong>推动了I2V生成的边界</strong>。相关代码、CHV数据集以及评估指标将在 &lt;https://github.com/-- &gt;开放。     </p>
<div style="break-before: page; page-break-before: always;"></div><p>UniMoGen: Universal Motion Generation</p>
<p>动作生成是计算机图形学、动画、游戏和机器人技术的基石，能够实现逼真且多样化的角色动作创作。现有方法的一个显著局限是它们依赖于特定的骨骼结构，这限制了它们在不同角色间的通用性。</p>
<p>为克服此局限，我们提出了 <strong>UniMoGen</strong>，一种新颖的、基于 UNet 架构的扩散模型，专为<strong>骨架无关</strong>的动作生成而设计。UniMoGen 可训练来自不同角色(如人类和动物)的动作数据，且无需预定义最大关节数量。通过仅动态处理每个角色必需的关节，我们的模型同时实现了<strong>骨架无关性</strong>和<strong>计算高效性</strong>。</p>
<p>UniMoGen 的关键特性包括：通过风格和轨迹输入实现<strong>可控性</strong>，以及从过往帧<strong>延续动作</strong>的能力。我们在 100style 数据集上验证了 UniMoGen 的有效性，其在多样化角色动作生成方面优于最先进的方法。此外，当在使用了不同骨骼的 100style 和 LAFAN1 数据集上联合训练时，UniMoGen 在两种骨骼上都实现了<strong>高性能</strong>和<strong>更高的效率</strong>。</p>
<p>这些结果突显了 UniMoGen 的潜力：通过为广泛的角色动画提供<strong>灵活、高效且可控</strong>的解决方案，推动动作生成技术的进步。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>Diffusion Model-based Activity Completion for AI Motion Capture from Videos</p>
<p>基于人工智能(AI)的动作捕捉是一项新兴技术，为传统动作捕捉系统提供了一种更具成本效益的替代方案。然而，当前的AI动作捕捉方法完全依赖于观测到的视频序列，这与传统动作捕捉类似。这意味着所有人类动作都必须预先定义，无法产生超出观测序列范围的动作。</p>
<p>为了解决这一局限，我们致力于将AI动作捕捉应用于<strong>虚拟人</strong>领域，因为虚拟人需要能够执行超出观测序列的灵活动作。我们假设训练数据中存在许多动作片段，但它们之间的<strong>过渡</strong>可能缺失。为了弥合这些间隙，我们提出了一种<strong>基于扩散模型的动作补全技术</strong>，该技术能够生成<strong>互补的人体运动序列</strong>，从而确保动作的流畅性和连续性。</p>
<p>通过引入<strong>门控模块</strong>(gate module)和<strong>位置-时间嵌入模块</strong>(position-time embedding module)，我们的方法在Human3.6M数据集上取得了<strong>具有竞争力的结果</strong>。我们的实验结果表明：<br />
(1) MDC-Net在ADE(平均位移误差)、FDE(最终位移误差)和MMADE(多模态ADE)指标上优于现有方法，但在MMFDE(多模态FDE)上精度略低；<br />
(2) MDC-Net的模型大小(16.84M)小于HumanMAC(28.40M)；<br />
(3) MDC-Net能生成更自然、连贯的运动序列。</p>
<p>此外，我们还提出了一种从人体运动序列中提取传感器数据(包括加速度和角速度)的方法。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control</p>
<p>我们通过3D虚拟人控制探索大语言模型（LLMs）的人体运动知识。给定运动指令时，我们引导LLMs首先生成分步的高层运动计划（高层规划），随后在每步中指定身体部位位置（低层规划），通过线性插值生成虚拟人动画，为人类评估者提供直观的验证视角。基于精心设计的20个代表性运动指令——全面覆盖基础运动单元并平衡身体部位使用——我们展开综合评估：包括对生成动画和高层运动计划的人工评估，以及低层规划中与标准位置的自动比对。研究发现：</p>
<ol>
<li>LLMs擅长解析高层身体运动，但精确定位身体部位存在困难</li>
<li>将运动查询拆解为原子单元可提升规划性能，但LLMs难以处理涉及高自由度身体部位的多步运动</li>
<li>LLMs对通用空间描述能提供合理近似，但无法处理文本中的精确空间规范，以及虚拟人控制所需的精确时空参数</li>
<li>显著优势体现在：LLMs在概念化创意运动及识别文化特定运动模式方面展现潜力   </li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><p>IKMo: Image-Keyframed Motion Generation with Trajectory-Pose Conditioned Motion Diffusion Model</p>
<p><strong>现有基于轨迹与姿态输入的人体运动生成方法</strong>通常对这两种模态进行全局处理，导致输出结果次优。本文提出 <strong>IKMo</strong>（基于图像关键帧的运动生成方法），这是一种基于扩散模型的运动生成方法，其核心在于<strong>解耦轨迹与姿态输入</strong>。轨迹和姿态输入经过一个<strong>两阶段条件化框架</strong>进行处理：</p>
<ol>
<li><strong>第一阶段</strong>：应用专门的优化模块对输入进行精细化处理。</li>
<li><strong>第二阶段</strong>：轨迹和姿态<strong>并行</strong>通过一个<strong>轨迹编码器（Trajectory Encoder）<strong>和一个</strong>姿态编码器（Pose Encoder）<strong>进行编码。随后，一个</strong>运动控制网络（ControlNet）<strong>处理融合后的轨迹与姿态数据，引导生成具有</strong>高空间保真度与语义保真度</strong>的运动。</li>
</ol>
<p>基于 HumanML3D 和 KIT-ML 数据集的实验结果表明，在轨迹-关键帧约束下，所提方法在所有指标上均优于现有最优方法。此外，我们实现了<strong>基于多模态大语言模型（MLLM）的智能体</strong>来预处理模型输入。给定用户提供的文本和关键帧图像，这些智能体能够提取运动描述、关键帧姿态和轨迹作为优化后的输入，输入到运动生成模型中。我们进行了包含 10 名参与者的用户研究，实验结果证明，基于 MLLM 的智能体预处理使生成的运动更符合用户预期。我们相信，所提出的方法显著提升了扩散模型在运动生成方面的<strong>保真度与可控性</strong>。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>AniCrafter: Customizing Realistic Human-Centric Animation via Avatar-Background Conditioning in Video Diffusion Models</p>
<p>视频扩散模型的最新进展显著提升了角色动画技术。然而，当前方法依赖于诸如 DWPose 或 SMPL-X 等基础结构条件来驱动角色图像动画，这限制了它们在具有动态背景或复杂人体姿态的开放域场景中的有效性。为此，我们提出 <strong>AniCrafter</strong>，这是一个基于扩散模型、以人物为中心的动画模型。它能够无缝地将给定角色集成并动画化到开放域的动态背景中，同时遵循给定的人体运动序列。我们的模型建立在尖端的图像到视频(I2V)扩散架构之上，并融入了一种创新的“角色-背景”联合条件控制机制。该机制将开放域以人物为中心的动画重新定义为一项<strong>修复任务</strong>，从而生成更稳定、更通用的动画输出。实验结果表明，我们的方法具有显著优势。代码将在 <a href="https://github.com/MyNiuuu/AniCrafter">https://github.com/MyNiuuu/AniCrafter</a> 公开。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>ParticleGS: Particle-Based Dynamics Modeling of 3D Gaussians for Prior-free Motion Extrapolation</p>
<p><strong>本文旨在根据视觉观测数据建模三维高斯分布的动力学特性，以支持时间外推。</strong></p>
<p>现有的动态三维重建方法通常难以有效学习潜在的动力学规律，或者严重依赖手动定义的物理先验，这限制了它们的外推能力。为解决这一问题，我们提出了一种新颖的、基于粒子动力学系统的、无需先验的动态三维高斯溅射运动外推框架。</p>
<p>我们方法的核心优势在于其能够学习描述三维高斯分布动力学的微分方程，并在未来帧外推过程中遵循这些方程。与简单地拟合观测到的视觉帧序列不同，我们的目标是更有效地建模高斯粒子动力学系统。为此，我们在标准高斯核中引入了一个<strong>动力学隐状态向量</strong>，并设计了一个<strong>动力学隐空间编码器</strong>来提取初始状态。随后，我们引入了一个<strong>基于神经常微分方程（Neural ODEs）的动力学模块</strong>，该模块在动力学隐空间中建模高斯分布的时间演化。最后，使用一个<strong>高斯核空间解码器</strong>将特定时间步的隐状态解码为形变。</p>
<p>实验结果表明，所提出的方法在重建任务中达到了与现有方法相当的渲染质量，并在未来帧外推方面显著优于现有方法。我们的代码公开在：<a href="https://github.com/QuanJinSheng/ParticleGS">https://github.com/QuanJinSheng/ParticleGS</a>。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>MotionPro: A Precise Motion Controller for Image-to-Video Generation</p>
<p><strong>通过交互式运动控制实现图像动画</strong>在图像到视频（I2V）生成领域日益受到欢迎。现代方法通常依赖<strong>大型高斯核</strong>来扩展运动轨迹作为条件，而<strong>未明确定义运动区域</strong>，这导致<strong>运动控制粗糙</strong>，且<strong>无法区分物体运动与相机运动</strong>。</p>
<p>为了缓解这些问题，我们提出了 <strong>MotionPro</strong>，一个<strong>精确的运动控制器</strong>。它创新性地利用<strong>区域级轨迹（region-wise trajectory）</strong> 和<strong>运动掩码（motion mask）</strong> 来分别<strong>调控细粒度的运动合成</strong>和<strong>识别目标运动的类别</strong>（即物体运动或相机运动）。</p>
<p>在技术上，MotionPro 首先通过一个<strong>跟踪模型</strong>估计每个训练视频的<strong>光流图（flow maps）</strong>，然后对<strong>区域级轨迹进行采样</strong>以模拟推理场景。与使用大型高斯核扩展光流的方法不同，我们的<strong>区域级轨迹方法</strong>通过直接利用<strong>局部区域内的轨迹</strong>，实现了更精确的控制，从而有效地刻画了<strong>细粒度的运动</strong>。同时，我们从预测的光流图中<strong>推导出运动掩码</strong>，以捕捉运动区域的<strong>整体运动动态</strong>。</p>
<p>为了追求<strong>自然的运动控制</strong>，MotionPro 进一步通过<strong>特征调制（feature modulation）</strong> 整合<strong>区域级轨迹</strong>和<strong>运动掩码</strong>，从而<strong>增强了视频去噪效果</strong>。</p>
<p>更值得一提的是，我们精心构建了一个名为 <strong>MC-Bench</strong> 的<strong>基准测试集</strong>，其中包含 <strong>1.1K 个用户标注的图像-轨迹对</strong>，用于评估<strong>细粒度的</strong>和<strong>物体级的</strong> I2V 运动控制效果。在 WebVid-10M 和 MC-Bench 上进行的广泛实验证明了 MotionPro 的有效性。更多结果请参考我们的项目页面：<a href="https://zhw-zhang.github.io/MotionPro-page/">https://zhw-zhang.github.io/MotionPro-page/</a>。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>DIPO: Dual-State Images Controlled Articulated Object Generation Powered by Diverse Data</p>
<p>我们提出了 DIPO，这是一个用于从一对图像可控生成铰接式三维物体的创新框架：一幅图像描绘物体在静止状态，另一幅描绘其在铰接状态。与单图像输入方法相比，我们的双图像输入仅需付出适度的数据收集开销，但同时提供了重要的运动信息，这些信息为预测部件间的运动学关系提供了可靠的指导。具体而言，我们提出了一种双图像扩散模型，该模型捕获图像对之间的关系，以生成部件布局和关节参数。此外，我们引入了一个基于思维链（CoT）的图推理器，用于显式推断部件间的连接关系。</p>
<p>为了进一步提高对复杂铰接物体的鲁棒性和泛化能力，我们开发了一个全自动数据集扩展流程（名为 <strong>LEGO-Art</strong>），它丰富了 PartNet-Mobility 数据集的多样性和复杂性。我们提出了 <strong>PM-X</strong>，一个包含复杂铰接三维物体的大规模数据集，附带渲染图像、URDF（统一机器人描述格式）标注和文本描述。</p>
<p>大量实验表明，无论是在静止状态还是铰接状态，DIPO 都显著优于现有基线方法，同时所提出的 PM-X 数据集进一步增强了对多样化和结构复杂的铰接物体的泛化能力。我们的代码和数据集将在论文发表后向社区开源。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>Force Prompting: Video Generation Models Can Learn and Generalize Physics-based Control Signals</p>
<p>近期视频生成模型的进展激发了对能够模拟逼真环境的世界模型的兴趣。虽然导航功能已被充分探索，但模仿真实世界力量的、具有物理意义的交互在很大程度上仍未得到充分研究。</p>
<p>在这项工作中，我们研究了将物理力作为视频生成的控制信号，并提出了“力提示”。这使得用户能够通过局部点力（例如戳植物）和全局风力场（例如风吹动布料）与图像进行交互。我们证明，通过利用原始预训练模型中的视觉和运动先验，这些力提示能够使视频对物理控制信号做出逼真的响应，并且在推理过程中无需使用任何3D资源或物理模拟器。</p>
<p>力提示的主要挑战在于难以获得高质量配对的力-视频训练数据。这在现实世界中是由于获取力信号的困难，而在合成数据中则是由于物理模拟器在视觉质量和领域多样性方面的局限性。</p>
<p><strong>我们的关键发现是：当视频生成模型经过调整以适应Blender合成视频中的物理力条件时，即使仅使用少量物体的有限演示样本，它们也能表现出卓越的泛化能力。我们的方法可以生成模拟各种几何形状、场景和材质受力的视频。</strong></p>
<p><strong>我们还试图理解这种泛化能力的来源，并通过消融实验揭示了两个关键要素：视觉多样性以及在训练过程中使用特定的文本关键词。</strong></p>
<p><strong>我们的方法仅在约1.5万个训练样本上，使用四块A100 GPU训练一天，就在力遵循度和物理真实性方面超越了现有方法，使世界模型更接近真实世界的物理交互。</strong></p>
<p><strong>我们在项目主页上发布了所有数据集、代码、权重和交互式视频演示。</strong>   </p>
<div style="break-before: page; page-break-before: always;"></div><p>PAMD: Plausibility-Aware Motion Diffusion Model for Long Dance Generation</p>
<p>计算舞蹈生成在艺术、人机交互、虚拟现实和数字娱乐等众多领域至关重要，尤其是在生成连贯且富有表现力的长舞蹈序列方面。基于扩散模型的音乐到舞蹈生成已取得显著进展，但现有方法仍难以生成物理上合理的运动。为解决此问题，我们提出了<strong>合理性感知运动扩散模型 (PAMD)</strong>，这是一个用于生成既与音乐对齐又物理逼真的舞蹈框架。</p>
<p>PAMD的核心在于<strong>合理运动约束 (PMC)</strong>，它利用<strong>神经距离场 (NDFs)</strong> 来建模真实的姿态流形，并将生成的运动引导至物理上有效的姿态流形。为了在生成过程中提供更有效的引导，我们引入了<strong>先验运动引导 (PMG)</strong>，它将站立姿态作为辅助条件，与音乐特征一起使用。为了进一步增强复杂运动的真实感，我们提出了<strong>基于足地接触的运动优化 (MRFC)</strong> 模块，该模块通过弥合关节线性位置空间中的优化目标与非线性旋转空间中的数据表示之间的差距，来解决脚滑伪影问题。</p>
<p>大量实验表明，PAMD显著提高了音乐对齐度，并增强了生成运动的物理合理性。本项目页面地址为：<a href="https://mucunzhuzhu.github.io/PAMD-page/">https://mucunzhuzhu.github.io/PAMD-page/</a>。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>Absolute Coordinates Make Motion Generation Easy</p>
<p>当前最先进的文本到动作生成模型依赖于由 HumanML3D 推广的运动学感知、局部相对动作表示方法。该方法通过相对于骨盆和前一帧对动作进行编码，并带有内置冗余。虽然这种设计简化了早期生成模型的训练，但它却给扩散模型带来了关键限制，并阻碍了其在下游任务中的应用。</p>
<p>在这项工作中，我们重新审视了动作表示方法，并为文本到动作生成提出了一种经过彻底简化且被长期弃用的替代方案：全局空间中的<strong>绝对关节坐标</strong>。通过对设计选择的系统分析，我们发现，即使仅使用简单的 Transformer 主干网络且无需辅助的运动学感知损失函数，这种表述方式也能实现<strong>显著更高的动作保真度、改进的文本对齐能力以及强大的可扩展性</strong>。</p>
<p>此外，我们的方法天然支持下游任务，例如文本驱动的动作控制以及时间/空间编辑，无需额外的任务特定工程改造，也无需根据控制信号进行成本高昂的分类器引导生成。</p>
<p>最后，我们展示了其良好的泛化能力，能够直接从文本生成运动中的 SMPL-H 网格顶点，为未来的研究和动作相关应用奠定了坚实基础。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>Flow Matching for Geometric Trajectory Simulation</p>
<p>N体系统的模拟是一个基础性问题，在分子动力学、生物化学和行人动力学等诸多领域具有广泛应用。机器学习已成为扩展基于物理的模拟器及直接从实验数据开发模型的重要工具。特别是基于深度生成建模和几何深度学习的最新进展，通过建模轨迹的复杂概率分布，同时遵循N体系统核心的置换对称性，实现了概率化模拟。然而，为生成真实轨迹，现有方法必须从无信息噪声开始学习复杂变换，且无法利用领域知识先验。</p>
<p>本研究提出STFlow方法以突破此局限。通过融合流匹配技术和数据依赖耦合机制，STFlow在不牺牲模型表达能力或可扩展性的前提下，实现了基于物理知识的几何轨迹模拟。我们在N体动力学系统、分子动力学和行人动力学基准测试中的评估表明：STFlow在实现更高效推理的同时，显著降低了预测误差，这凸显了在概率化几何轨迹建模中引入物理信息先验分布的优势。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Pose Splatter: A 3D Gaussian Splatting Model for Quantifying Animal Pose and Appearance</p>
<p>对动物姿态与外观进行精确且可扩展的量化，对于行为研究至关重要。当前的三维姿态估计技术，例如基于关键点和网格的技术，常常面临诸多挑战，包括表征细节有限、标注工作量大以及逐帧优化成本高昂。这些限制阻碍了对细微动作的研究，并可能使大规模分析变得不切实际。</p>
<p>我们提出了 <strong>姿态泼溅 (Pose Splatter)</strong>，这是一个新颖的框架，它利用<strong>形状雕刻 (shape carving)</strong> 和 <strong>三维高斯泼溅 (3D Gaussian splatting)</strong> 技术，在无需动物几何先验知识、无需逐帧优化或人工标注的情况下，对实验动物的完整姿态和外观进行建模。我们还提出了一种新颖的<strong>旋转不变视觉嵌入 (rotation-invariant visual embedding)</strong> 技术，用于编码姿态和外观信息，该技术旨在作为下游行为分析中三维关键点数据的<strong>即插即用替代方案 (plug-in replacement)</strong>。</p>
<p>在小鼠、大鼠和斑胸草雀数据集上的实验表明，姿态泼溅能够学习到精确的三维动物几何结构。值得注意的是，该框架能够表征姿态的细微变化，其生成的低维姿态嵌入在人类评估中优于当前最先进技术，并且能泛化到未见过的数据。通过消除标注和逐帧优化的瓶颈，姿态泼溅使得分析大规模、长期的行为成为可能，从而得以以前所未有的分辨率绘制基因型、神经活动与微行为之间的关系图。</p>
<div style="break-before: page; page-break-before: always;"></div><p>CGS-GAN: 3D Consistent Gaussian Splatting GANs for High Resolution Human Head Synthesis</p>
<p><strong>最近，基于3D高斯泼溅（3D Gaussian Splatting）的3D GAN被提出，用于高质量合成人头像。然而，现有方法通过将随机潜在向量以当前相机位置为条件（view-conditioning），来稳定训练并从陡峭视角提升渲染质量。这种做法破坏了3D一致性，因为我们观察到，每次相机移动后重新合成3D头像时，身份特征会发生显著变化。相反，将相机固定到单一视角，虽然能在该视角下生成高质量渲染结果，但在新视角下表现不佳。移除视角条件控制通常会破坏GAN训练的稳定性，常导致训练崩溃。</strong></p>
<p><strong>针对这些挑战，我们提出了CGS-GAN，这是一种新颖的3D高斯泼溅GAN框架。它能够在无需依赖视角条件控制的情况下，实现稳定的训练和高质量、3D一致的人头像合成。为确保训练稳定性，我们引入了一种多视角正则化技术，该技术能以最小的计算开销增强生成器的收敛性。此外，我们改进了现有3D高斯泼溅GAN中使用的条件损失函数，并提出了一种生成器架构。该架构不仅旨在稳定训练，还能促进高效渲染和直接扩展，支持高达 \(2048^2\) 的输出分辨率。</strong></p>
<p><strong>为评估CGS-GAN的能力，我们从FFHQ数据集整理了一个新数据集。该数据集支持超高分辨率，聚焦于人头像的更大区域，减少了视角相关的伪影以提升3D一致性，并排除了主体被手或其他物体遮挡的图像。因此，我们的方法在具有竞争力的FID分数支持下，实现了极高的渲染质量，同时确保了3D场景生成的一致性。请访问我们的项目页面：<a href="https://fraunhoferhhi.github.io/cgs-gan/">https://fraunhoferhhi.github.io/cgs-gan/</a></strong>      </p>
<div style="break-before: page; page-break-before: always;"></div><p>Multi-Person Interaction Generation from Two-Person Motion Priors</p>
<p>利用高层级控制生成逼真人体运动，对于社会行为理解、机器人技术和动画制作至关重要。随着高质量动作捕捉(MOCAP)数据的日益普及，各类数据驱动方法相继被提出。然而，多人交互运动建模领域的研究仍相对匮乏。本文提出图驱动交互采样法(Graph-driven Interaction Sampling)，该方法能够利用现有双人运动扩散模型作为运动先验，生成逼真且多样化的多人交互动作。我们的核心思路是：无需专门训练多人交互合成模型，而是将复杂的多人交互在时空维度分解为双人交互的图结构(称为成对交互图 Pairwise Interaction Graph)。由此将生成任务解耦为同步进行的、以他人动作为条件的单人运动生成。此外，为减少生成结果中的身体部位互穿等伪影，我们在扩散采样方案中引入了两项图相关引导机制。与现有方法不同，本方案能在避免个体动作重复的前提下，生成多样化的高质量多人交互。大量实验证明，在生成各类双人及多人交互时，本方法在减少伪影方面持续优于现有方案。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>DanceTogether! Identity-Preserving Multi-Person Interactive Video Generation</p>
<p><strong>可控视频生成（CVG）</strong> 技术发展迅速，然而当需要在嘈杂的控制信号下让多个角色移动、互动并交换位置时，现有系统却表现不佳。我们推出了 <strong>DanceTogether</strong> 来解决这一不足，它是首个端到端的扩散框架，能够将<strong>单张参考图像</strong>加上<strong>独立的姿态掩膜流</strong>转化为<strong>长时、逼真的视频</strong>，同时<strong>严格保持每个角色的身份特征</strong>。</p>
<p>我们提出了一种新颖的 <strong>MaskPoseAdapter</strong>，它在每一步去噪过程中，通过将鲁棒的跟踪掩膜与语义丰富但含噪的姿态热图相融合，从而绑定“谁”（身份）和“如何”（动作），有效消除了困扰逐帧处理流程的<strong>身份漂移</strong>和<strong>外观渗色</strong>问题。</p>
<p>为了进行大规模训练和评估，我们引入了：<br />
(i) <strong>PairFS-4K</strong>：包含26小时的双人滑冰视频素材，涵盖超过7,000个不同身份；<br />
(ii) <strong>HumanRob-300</strong>：一小时的类人机器人互动数据集，用于快速跨领域迁移；<br />
(iii) <strong>TogetherVideoBench</strong>：一个包含三个测试轨道的基准平台，核心是 <strong>DanceTogEval-100</strong> 测试集，涵盖舞蹈、拳击、摔跤、瑜伽和花样滑冰。</p>
<p>在 <strong>TogetherVideoBench</strong> 上，<strong>DanceTogether</strong> 的性能显著优于现有技术。此外，我们证明只需<strong>一小时的微调训练</strong>，即可生成令人信服的人机互动视频，突显了其在具身人工智能（embodied-AI）和人机交互（HRI）任务上强大的泛化能力。大量的消融实验证实，<strong>持续的身份-动作绑定</strong>是实现这些提升的关键。</p>
<p>总而言之，我们的模型、数据集和基准平台共同将CVG从<strong>单主体编排</strong>提升到了<strong>组合可控的多主体互动</strong>水平，为数字制作、模拟仿真和具身智能开辟了新途径。我们的视频演示和代码已开放：<a href="https://DanceTog.github.io/">https://DanceTog.github.io/</a>。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Temporal Differential Fields for 4D Motion Modeling via Image-to-Video Synthesis</p>
<p>对规律性呼吸运动进行时域建模对于图像引导的临床应用至关重要。现有方法无法模拟时域运动，除非同时存在包含起始帧和结束帧的高剂量成像扫描。然而，在术前数据采集阶段，患者的轻微移动可能导致一个呼吸周期内首尾帧之间出现动态背景变化。这种额外的偏差很难通过图像配准去除，从而影响时域建模。</p>
<p>为了解决这一局限性，我们开创性地通过图像到视频(I2V)合成框架来模拟规律的运动过程，该框架利用起始帧进行动态化处理，以预测给定长度的未来帧序列。此外，为了提升生成视频的时序一致性，我们设计了时域差分扩散模型(Temporal Differential Diffusion Model)来生成时域差分场(temporal differential fields)，这些差分场量化了相邻帧之间的相对差异表示。</p>
<p>我们设计了提示注意力层(prompt attention layer)以生成细粒度的差分场，并采用场增强层(field augmented layer)来更好地让这些差分场与I2V框架进行交互，从而提升合成视频更精确的时序变化。</p>
<p>在ACDC心脏数据集和4D肺数据集上的大量实验结果表明，我们的方法能够沿着固有的运动轨迹模拟4D视频，在感知相似性和时序一致性方面媲美其他先进方法。代码即将开源。</p>
<div style="break-before: page; page-break-before: always;"></div><p>WonderPlay: Dynamic 3D Scene Generation from a Single Image and Actions</p>
<p>WonderPlay是一种创新框架，将物理模拟与视频生成技术相结合，能够基于单张图像生成动作条件化的动态三维场景。先前的研究局限于刚体或简单弹性动力学，而WonderPlay采用混合生成模拟器，可合成多样化的三维动态效果。该混合生成模拟器首先通过物理求解器模拟粗糙的三维动态，随后引导视频生成器产出具有更精细、更逼真运动的视频。生成的视频继而用于更新模拟的动态三维场景，从而形成物理求解器与视频生成器之间的闭环。这种方法既融合了基于物理模拟器的精确动力学特性，又结合了扩散式视频生成器的强大表现力，同时支持直观的用户操控。实验结果表明，WonderPlay支持用户通过单张图像输入，与布料、沙粒、积雪、液体、烟雾、弹性体及刚体等多元场景进行交互。代码即将开源。项目主页：<a href="https://kyleleey.github.io/WonderPlay/">https://kyleleey.github.io/WonderPlay/</a></p>
<div style="break-before: page; page-break-before: always;"></div><p>Motion Matters: Compact Gaussian Streaming for Free-Viewpoint Video Reconstruction</p>
<p><strong>3D高斯点云渲染（3DGS）</strong> 已成为在线自由视点视频（FVV）重建的一种高保真且高效的范式，可为观众提供快速的响应性和沉浸式体验。然而，现有的在线方法面临存储需求过高的挑战，这主要源于其逐点建模方式未能有效利用运动特性。</p>
<p>为解决这一局限，我们提出了一种新颖的<strong>紧凑高斯流式传输（ComGS）框架</strong>。该框架利用动态场景中运动的<strong>局部性</strong>和<strong>一致性</strong>，通过<strong>关键点驱动的运动表示</strong>来建模具有<strong>物体一致性的高斯点运动</strong>。通过仅传输关键点属性，该框架提供了一种更节省存储空间的解决方案。</p>
<p>具体而言，我们首先使用<strong>视图空间梯度差策略</strong>，在运动区域内定位并识别出一组稀疏的<strong>运动敏感关键点</strong>。基于这些关键点，我们提出了一种<strong>自适应运动驱动机制</strong>，该机制预测一个<strong>空间影响场</strong>，用于将关键点运动传播给具有相似运动的邻近高斯点。</p>
<p>此外，ComGS在关键帧重建中采用了一种<strong>错误感知校正策略</strong>，该策略<strong>有选择性地精修错误区域</strong>，并在不引入不必要开销的情况下<strong>减轻误差累积</strong>。</p>
<p>总体而言，与3DGStream相比，ComGS实现了超过<strong>159倍</strong>的显著存储缩减；与当前最先进（SOTA）方法QUEEN相比，缩减了<strong>14倍</strong>，同时保持了有竞争力的视觉保真度和渲染速度。我们的代码将开源。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>Pursuing Temporal-Consistent Video Virtual Try-On via Dynamic Pose Interaction</p>
<p><strong>视频虚拟试穿技术</strong>旨在将特定服装无缝贴合到视频人物的身体上。其核心挑战在于保持服装视觉真实性的同时，动态适配人物的姿态与体型。现有方法主要聚焦于基于图像的虚拟试穿，若直接应用于视频常导致时序不一致问题。当前多数视频虚拟试穿方案通过引入时序模块缓解此问题，但仍忽视人物与服装间关键的时空姿态交互关系。</p>
<p>有效的视频姿态交互需同时兼顾两方面：单帧内人物与服装姿态的空间对齐，以及整个视频中人体姿态的时序动态变化。基于此动机，我们提出<strong>动态姿态交互扩散模型（DPIDM）</strong> 这一新型框架，利用扩散模型深入研究视频虚拟试穿中的动态姿态交互。</p>
<p>技术实现上，DPIDM创新性地：</p>
<ol>
<li>采用<strong>基于骨骼的姿态适配器</strong>，将同步化处理的人物/服装姿态输入去噪网络</li>
<li>设计<strong>分层注意力模块</strong>，通过姿态感知的：
<ul>
<li>空间注意力机制建模帧内人-衣姿态交互</li>
<li>时序注意力机制捕捉跨帧的长期人体姿态动态</li>
</ul>
</li>
<li>引入<strong>时序正则化注意力损失函数</strong>，在连续帧间增强时序一致性</li>
</ol>
<p>在VITON-HD、VVT和ViViD数据集上的大量实验证明DPIDM的优越性。特别值得注意的是，在VVT数据集上DPIDM达到0.506的VFID分数，较当前最优方法GPD-VVTO实现60.5%的性能提升。</p>
<div style="break-before: page; page-break-before: always;"></div><p>SHaDe: Compact and Consistent Dynamic 3D Reconstruction via Tri-Plane Deformation and Latent Diffusion</p>
<p>我们提出了一种新颖的动态3D场景重建框架，该框架集成了三个关键组件：一个显式的三平面形变场、一个带有球谐函数（SH）注意力的视角条件化标准辐射场，以及一个具有时间感知能力的隐空间扩散先验模型。</p>
<p><strong>我们的方法使用三个随时间演化的正交二维特征平面来编码4D场景，实现了高效且紧凑的时空表示。这些特征通过一个形变偏移场被显式地扭曲到标准空间，从而无需基于多层感知器（MLP）的运动建模。</strong></p>
<p><strong>在标准空间中，我们摒弃了传统的MLP解码器，采用了一个基于球谐函数的结构化渲染头。该渲染头通过对学习到的频带施加注意力来合成视角依赖的颜色，从而同时提高了可解释性和渲染效率。</strong></p>
<p><strong>为了进一步提升保真度和时间一致性，我们引入了一个由Transformer引导的隐空间扩散模块。该模块在压缩的隐空间中精炼三平面和形变特征。这一生成式模块能在模糊或分布外（OOD）运动情况下对场景表示进行去噪，从而增强了泛化能力。</strong></p>
<p><strong>我们的模型采用两阶段训练策略：首先独立预训练扩散模块，然后使用图像重建损失、扩散去噪损失和时间一致性损失的组合，与整个流程联合微调。</strong></p>
<p><strong>我们在合成基准测试上展示了最先进的结果，在视觉质量、时间连贯性以及对稀疏视角动态输入的鲁棒性方面均超越了近期的方法，如HexPlane和4D高斯溅射法（4D Gaussian Splatting）。</strong>   </p>
<div style="break-before: page; page-break-before: always;"></div><p>MEgoHand: Multimodal Egocentric Hand-Object Interaction Motion Generation</p>
<p><strong>以自我为中心的手-物体运动生成</strong>对于沉浸式AR/VR和机器人模仿至关重要，但由于不稳定的视角、自我遮挡、透视畸变以及嘈杂的自身运动，这仍然是一项挑战。现有方法依赖于预定义的3D物体先验知识，限制了对新物体的泛化能力（这限制了它们对新物体的泛化能力）。同时，近期的多模态方法存在以下问题：从抽象文本提示生成结果模糊不清、用于建模3D手-物体关联的流程过于复杂、以及在开环预测中误差不断累积。</p>
<p>我们提出了<strong>MEgoHand</strong>，一个多模态框架，能够从以自我为中心的RGB图像、文本和初始手部姿态合成物理合理的手-物体交互动作。MEgoHand引入了一个<strong>双层架构</strong>：高层“大脑”（cerebrum）利用<strong>视觉语言模型（VLM）<strong>从视觉-文本上下文中推断运动先验知识，并使用</strong>单目深度估计器</strong>进行与物体无关的空间推理；而低层则基于<strong>DiT（扩散变换器）的流匹配策略</strong>生成细粒度轨迹，并采用<strong>时序正交滤波</strong>来增强稳定性。</p>
<p>为了解决数据集不一致的问题，我们设计了一个<strong>数据集构建范式</strong>，包含一个<strong>逆向MANO重定向网络</strong>和一个<strong>虚拟RGB-D渲染器</strong>，构建了一个包含335万帧RGB-D图像、2.4万次交互和1200个物体的统一数据集。在<strong>五个域内数据集和两个跨域数据集</strong>上进行的大量实验证明了MEgoHand的有效性，实现了手腕平移误差（降低86.9%）和关节旋转误差（降低34.1%）的大幅减少，突显了其精确建模细粒度手部关节结构以及鲁棒泛化的能力。    </p>
<div style="break-before: page; page-break-before: always;"></div><p>MAGIC: Motion-Aware Generative Inference via Confidence-Guided LLM</p>
<p><strong>静态3D生成的最新进展</strong>加剧了对物理一致性<strong>动态3D内容</strong>的需求。然而，现有的视频生成模型，包括基于扩散的方法，通常优先考虑<strong>视觉真实感</strong>，却忽视了<strong>物理合理性</strong>，导致生成的对象动态效果<strong>不符合物理规律</strong>。</p>
<p>先前实现<strong>物理感知动态生成</strong>的方法通常依赖于<strong>大规模标注数据集</strong>或<strong>广泛的模型微调</strong>，这带来了巨大的计算和数据收集负担，并限制了其在<strong>不同场景下的可扩展性</strong>。</p>
<p>为了应对这些挑战，我们提出了<strong>MAGIC</strong>框架——一个<strong>免训练</strong>的、用于<strong>单图像物理属性推断</strong>与<strong>动态生成</strong>的框架。该框架将<strong>预训练的图生视频扩散模型</strong>与<strong>基于大语言模型（LLM）的迭代推理</strong>相结合。MAGIC能够从单张静态图像生成<strong>运动丰富的视频</strong>，并通过一个<strong>置信度驱动的大语言模型反馈循环</strong>弥合<strong>视觉与物理之间的差距</strong>，该循环能够自适应地引导扩散模型生成<strong>符合物理规律的运动</strong>。</p>
<p>为了将<strong>视觉动态转化为可控的物理行为</strong>，我们进一步引入了一个<strong>可微分物质点法（MPM）模拟器</strong>。该模拟器直接操作于从单张图像重建出的<strong>3D高斯模型</strong>之上，无需任何监督或模型调整，即可产生<strong>基于物理原理、可直接用于仿真的输出结果</strong>。</p>
<p>实验表明，MAGIC在<strong>推断准确性</strong>上超越了现有的物理感知生成方法，并且在<strong>时间一致性</strong>方面也优于最先进的视频扩散模型。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>GS2E: Gaussian Splatting is an Effective Data Generator for Event Stream Generation</p>
<p>我们提出了GS2E(高斯泼溅到事件转换)，一个为高保真事件视觉任务构建的大规模合成事件数据集，其数据源自真实世界稀疏多视角RGB图像的重建。现有事件数据集通常从密集RGB视频合成，这类方法普遍存在视角多样性不足、几何一致性缺失的问题，或依赖于昂贵且难以扩展的硬件系统。GS2E通过三维高斯泼溅技术首先生成逼真静态场景重建，继而采用创新的物理信息事件模拟流程，成功突破了这些局限。该流程创造性融合了自适应轨迹插值与物理一致的事件对比度阈值建模方法，能够在多样化运动模式和光照条件下生成时间密度高、几何一致性好的事件流，同时确保与底层场景结构的精准对齐。基于事件的3D重建实验表明，GS2E展现出卓越的泛化能力，作为推进事件视觉研究的基准数据集具有重要实用价值。     </p>
<div style="break-before: page; page-break-before: always;"></div><p>EVA: Expressive Virtual Avatars from Multi-view Videos</p>
<p>随着神经渲染和动作捕捉算法的快速发展，逼真人体化身建模技术取得了显著突破，在虚拟现实、增强现实、远程通信以及游戏、影视、医疗等行业释放出巨大潜力。然而，现有方法由于采用面部表情与肢体动作的纠缠式表征，无法实现对虚拟化身的完整、忠实且富有表现力的控制。本研究提出了富有表现力的虚拟化身(Expressive Virtual Avatars，EVA) —— 一种演员专用、完全可控且表现力丰富的数字人框架，该系统在实现高保真实时渲染的同时，能够对表情、肢体动作和手势进行独立控制。具体而言，我们创新性地将人体化身建模为双层架构：表现力模板几何层与三维高斯外观层。首先，我们开发了一种表现力模板追踪算法，通过由粗到精的优化策略，从多视角视频中精准重建肢体运动、面部表情及非刚性形变参数。其次，我们提出了一种新颖的解耦式三维高斯外观模型，旨在有效分离躯体与面部的视觉表征。与传统的统一高斯估计方法不同，本方法采用两个专门化独立模块分别建模躯体和面部。实验结果表明，EVA在渲染质量与表现力方面均超越现有最优方法，验证了其在构建全身虚拟化身方面的有效性。该研究标志着向全驱动数字人模型迈出了重要一步，为创建逼真再现人体几何特征与外观特征的数字分身提供了技术支撑。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>Large-Scale Multi-Character Interaction Synthesis</p>
<p>生成大规模多角色交互是角色动画领域中一项具有挑战性且重要的任务。多角色交互不仅需要自然的互动动作，还要求角色之间能够协调配合完成状态转换。例如在舞蹈场景中，既包含角色与舞伴共舞的互动，也需要角色基于时空观察协调转换到新舞伴。我们将这种状态转换定义为协调式交互，并将其分解为交互合成与过渡规划两个阶段。现有方法中，单角色动画技术未考虑多角色间至关重要的互动关系；基于深度学习的交互合成方法通常仅关注双角色场景且缺乏过渡规划；基于优化的交互合成方法依赖人工设计目标函数，其泛化能力受限；而人群模拟技术虽涉及更多角色，但其交互呈现稀疏性和被动性。我们指出多角色交互合成面临两大挑战：数据匮乏问题及紧密高频交互的过渡规划问题。现有数据集或缺乏多角色场景，或缺少紧密高频的交互内容。而针对多角色紧密高频交互的过渡规划需兼顾空间与时间双重考量。为此，我们提出了一个条件式生成框架，包含用于交互合成的可协调多角色交互空间和专门处理协调关系的过渡规划网络。实验证明我们的框架在多角色交互合成方面具有显著优势，基于该方法实现的应用案例也验证了其良好的扩展性和迁移能力。    </p>
<div style="break-before: page; page-break-before: always;"></div><p>Hunyuan-Game: Industrial-grade Intelligent Game Creation Model</p>
<p>智能游戏创作是游戏开发领域的一项变革性突破，它利用生成式人工智能动态生成和增强游戏内容。尽管生成模型已取得显著进展，但高质量游戏资产(包括图像和视频)的全面合成仍是一个充满挑战的领域。为了打造既契合玩家偏好又能大幅提升设计师效率的高保真度游戏内容，我们推出了Hunyuan-Game —— 一个旨在彻底革新智能游戏生产的创新项目。</p>
<p>Hunyuan-Game包含两大核心分支：图像生成与视频生成。图像生成模块基于包含数十亿游戏图像的大型数据集，研发出一系列专为游戏场景定制的图像生成模型：(1)通用文生图；(2)游戏特效生成，包含文生特效与基于参考图的游戏特效生成；(3)针对角色、场景及游戏特效的透明图像生成；(4)基于草图、黑白图和白模的游戏角色生成。视频生成模块依托数百万游戏及动漫视频的海量数据集，开发出五类核心算法模型，每类模型均直击游戏开发痛点，并对多样化的游戏视频场景具备强适应性：(1)图生视频生成；(2)360度A/T姿态数字人视频合成；(3)动态插画生成；(4)生成式视频超分；(5)交互式游戏视频生成。这些图像与视频生成模型不仅展现出高水准的美学表达，还深度融合领域专业知识，建立起对多种游戏及动漫美术风格的体系化理解。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>Vid2World: Crafting Video Diffusion Models to Interactive World Models</p>
<p>世界模型能够根据历史观察和动作序列预测状态转移，在提升序列决策的数据效率方面展现出巨大潜力。然而，现有世界模型通常需要大量领域特定训练，且生成的预测结果保真度低、粒度粗糙，限制了其在复杂环境中的应用。相比之下，基于海量互联网数据集训练的视频扩散模型，已展现出生成高质量视频的卓越能力，能够捕捉多样化的真实世界动态特征。本文提出Vid2World，这是一种将预训练视频扩散模型迁移应用于交互式世界模型的通用方法。为弥合模型差异，Vid2World通过对预训练视频扩散模型进行架构改造和训练目标优化，实现自回归生成能力，完成模型的因果化重构。此外，该方法创新性地引入因果动作引导机制，显著提升了交互式世界模型的动作可控性。在机器人操作和游戏模拟领域的广泛实验表明，本方法为将高性能视频扩散模型转化为交互式世界模型提供了可扩展且高效的解决方案。    </p>
<div style="break-before: page; page-break-before: always;"></div><p>MGStream: Motion-aware 3D Gaussian for Streamable Dynamic Scene Reconstruction</p>
<p>3D高斯溅射(3DGS)因其逼真的渲染能力和计算效率，在可流式传输的动态新视角合成(DNVS)领域受到广泛关注。尽管在提升渲染质量和优化策略方面取得了诸多进展，但基于3DGS的可流式传输动态场景重建仍存在闪烁伪影、存储效率低下以及难以建模新兴物体的问题。为此，我们提出MGStream方法：利用运动相关的3D高斯模型(3DGs)重建动态场景，而静态场景则使用普通3D高斯模型。运动相关3DGs通过运动掩码和基于聚类的凸包算法实现，对其施加刚性形变以建模动态场景，同时通过基于注意力的优化实现新兴物体的重建。由于形变和优化仅作用于运动相关3DGs，MGStream有效避免了闪烁伪影并提升了存储效率。在N3DV和MeetRoom等真实数据集上的大量实验表明，MGStream在渲染质量、训练/存储效率和时间一致性方面均优于现有基于流式传输的3DGS方法。代码已开源：<a href="https://github.com/pcl3dv/MGStream">https://github.com/pcl3dv/MGStream</a>。</p>
<div style="break-before: page; page-break-before: always;"></div><p>LMP: Leveraging Motion Prior in Zero-Shot Video Generation with Diffusion Transformer</p>
<p>近年来，大规模预训练扩散变换器模型在视频生成领域取得了显著进展。尽管当前DiT模型能够生成高清、高帧率且高度多样化的视频，但对视频内容的细粒度控制仍存在不足。仅通过提示语控制视频中主体的运动具有挑战性，尤其是在描述复杂运动时。此外，现有方法在图像到视频生成中无法有效控制运动，因为参考图像中的主体与参考视频中的主体在初始位置、尺寸和形状方面往往存在差异。为此，我们提出了利用运动先验(Leveraging Motion Prior, LMP)的零样本视频生成框架。该框架通过利用预训练扩散变换器的强大生成能力，使生成的视频在文本到视频和图像到视频生成中都能参考用户提供的运动视频。具体而言，我们首先引入前景-背景解耦模块来区分参考视频中的运动主体和背景，避免对目标视频生成产生干扰。设计了重加权运动迁移模块，使目标视频能够参考源视频的运动特征。为避免参考视频主体对目标视频的干扰，我们提出外观分离模块来抑制参考主体在目标视频中的外观表现。我们为DAVIS数据集标注了详细的提示语用于实验，并设计了评估指标验证方法的有效性。大量实验表明，我们的方法在生成质量、提示-视频一致性及控制能力方面均达到了最先进水平。项目主页详见<a href="https://vpx-ecnu.github.io/LMP-Website/">https://vpx-ecnu.github.io/LMP-Website/</a>。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Video-GPT via Next Clip Diffusion</p>
<p>GPT在自然语言处理领域展现了非凡成就。然而，语言序列难以充分描述视觉世界中的时空细节，而视频序列恰好擅长捕捉此类信息。基于此观察，本文提出了一种简洁的Video-GPT框架，将视频视为描述视觉世界的新型&quot;语言&quot;。受GPT中下一词元预测的启发，我们针对视频GPT的预训练提出了新颖的下一片段扩散范式。与现有方法不同，这种独特范式通过基于历史干净片段对噪声片段进行自回归去噪，使Video-GPT能同时处理短期生成与长期预测任务。大量实验表明，我们的Video-GPT在视频预测任务上达到了最先进性能(Physics-IQ基准测试：Video-GPT 34.97 vs. Kling 23.64 vs. Wan 20.89)，这正是实现世界建模的关键指标。此外，该模型能良好适配视频生成与理解领域的6个主流任务，展现出优异的下游泛化能力。项目主页请访问<a href="https://Video-GPT.github.io">https://Video-GPT.github.io</a>。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>MonoMobility: Zero-Shot 3D Mobility Analysis from Monocular Videos</p>
<p>准确分析动态环境中的运动部件及其运动属性对于推动具身智能等关键领域的发展至关重要。针对现有方法依赖密集多视角图像或详细部件级标注的局限性，我们提出了一种创新框架，能够以零样本方式从单目视频中分析三维运动特性。该框架仅需单目视频即可精确解析运动部件及其运动属性，完全无需任何标注训练数据。具体而言，我们的方法首先通过深度估计、光流分析和点云配准技术构建场景几何结构并初步分析运动部件及其初始运动属性，继而采用二维高斯泼溅进行场景表征。在此基础上，我们针对铰接式物体专门设计了一种端到端动态场景优化算法，通过迭代优化逐步细化初始分析结果，从而确保系统能够处理&quot;旋转&quot;、&quot;平移&quot;乃至复杂运动(&quot;旋转+平移&quot;)，展现出高度的灵活性和普适性。为验证方法的鲁棒性和广泛适用性，我们构建了包含仿真场景和真实场景的综合性数据集。实验结果表明，该框架能够在无标注条件下有效解析铰接物体运动，展现了其在未来具身智能应用中的重要潜力。</p>
<div style="break-before: page; page-break-before: always;"></div><p>RoPECraft: Training-Free Motion Transfer with Trajectory-Guided RoPE Optimization on Diffusion Transformers</p>
<p>我们提出了RoPECraft，一种面向扩散变换器的免训练视频运动迁移方法，其核心机制仅通过修改旋转位置嵌入(RoPE)实现。该方法首先从参考视频中提取密集光流，利用获得的运动偏移量对RoPE的复指数张量进行变形处理，从而将运动信息有效编码至生成过程中。在去噪阶段，我们通过流匹配目标函数对预测速度与目标速度进行轨迹对齐，从而持续优化这些嵌入表示。为确保输出结果与文本提示保持语义一致并避免生成重复内容，我们引入了基于参考视频傅里叶变换相位分量的正则化项，通过将相位角投影至平滑流形来抑制高频伪影。基准测试实验表明，RoPECraft在定性与定量评估中均优于近期发表的所有同类方法。</p>
<div style="break-before: page; page-break-before: always;"></div><p>UniHM: Universal Human Motion Generation with Object Interactions in Indoor Scenes</p>
<p>复杂场景中的人体运动合成是一个基础性挑战，其要求整合静态环境、可移动物体、自然语言提示和空间路径点等多模态信息，这超越了传统文本驱动运动生成任务的范畴。现有的基于语言条件的运动模型由于运动标记化的局限性(导致信息丢失且无法捕捉三维人体运动的连续上下文依赖性)，在场景感知运动生成方面常常面临困难。为解决这些问题，我们提出了统一运动语言模型UniHM，该模型利用基于扩散的生成方法实现场景感知人体运动合成。UniHM是首个支持复杂三维场景中文本驱动运动生成与文本驱动人物-物体交互(HOI)的统一框架。本研究的核心创新包含三个方面：(1)融合连续六自由度运动与离散局部运动标记的混合运动表征，提升运动真实感；(2)创新的无查找量化变分自编码器(LFQ-VAE)，在重建精度和生成性能上均超越传统VQ-VAE；(3)增强版Lingo数据集(HumanML3D注释增强版)，为场景特定运动学习提供更强化监督。实验结果表明，UniHM在OMOMO基准测试中实现了与当前最优方法相当的文本驱动HOI合成性能，同时在HumanML3D数据集上获得了具有竞争力的通用文本条件运动生成效果。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>PoseBench3D: A Cross-Dataset Analysis Framework for 3D Human Pose Estimation</p>
<p>可靠的三维人体姿态估计在现实应用中正变得越来越重要，但先前的研究大多仅关注单个数据集内的性能表现。然而在实际场景中，系统必须适应与训练数据差异显著的多样化视角、环境条件和相机配置——这正是现实应用中的典型挑战。为了应对这些问题，我们构建了一个标准化测试环境：通过在多数据集上统一评估各类方法，确保跨数据集比较的一致性与公平性，从而支持对模型在新数据上的表现分析。</p>
<p>基于此，我们提出了PoseBench3D统一框架。该框架系统性地重新评估了当前和未来模型在四大主流人体姿态估计数据集上的表现，并具备兼容新兴数据集的可扩展性。通过标准化接口，我们将数据集以预配置且可灵活调整的格式提供给使用者，确保与各类模型架构的兼容性。我们对18种方法进行了重新评估(包括新训练模型和已有文献成果)，采用平均关节位置误差(MPJPE)和Procrustes对齐平均关节位置误差(PA-MPJPE)双指标进行评测，最终获得超过100项创新的跨数据集评估结果。此外，我们还分析了不同预处理技术和数据集参数对性能的影响，为模型泛化能力研究提供了新的洞见。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Robust Photo-Realistic Hand Gesture Generation: from Single View to Multiple View</p>
<p>高保真手势生成在以人为本的生成任务中面临重大挑战。现有方法通常采用单视角3D MANO网格渲染图像作为先验来提升手势生成质量。然而，由于手部运动的复杂性和单视角渲染的固有局限性，这种方法难以捕捉完整的三维手部信息，尤其当手指存在遮挡时。其根本矛盾在于：二维投影会导致三维拓扑关系的丢失，而单视角表征又存在空间覆盖不全的缺陷。</p>
<p>与单视角先验方法不同，我们提出了一种基于多模态UNet特征编码器(MUFEN)的多视角先验框架，通过引导扩散模型学习完整的三维手部信息。具体而言，我们将传统的正视图渲染扩展至包含后视、左视、右视、俯视和仰视的多视角系统，并选择信息量最丰富的视角组合作为训练先验以解决遮挡补全问题。这种配备专用双流编码器的多视角先验机制显著提升了模型对完整手部特征的理解能力。此外，我们设计了边界框特征融合模块，能够将手势定位特征与手势多模态特征相融合，从而增强MUFEN特征对手势相关特征的位置感知能力。实验表明，我们的方法在定量指标和定性评估中均达到了业界领先水平。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>Infinigen-Sim: Procedural Generation of Articulated Simulation Assets</p>
<p>我们推出Infinigen-Sim工具包，该工具使用户能够创建多样化和真实的铰接物体程序生成器。该工具包包含用于在Blender中创建铰接资源的高级实用工具，以及将生成的资源集成到常见机器人模拟器中的导出管道。我们通过为5个常见铰接物体类别创建程序生成器来展示系统性能。实验表明，从这些生成器中采样的资源可用于可移动物体分割、训练可泛化的强化学习策略，以及模仿学习策略的仿真到真实迁移。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Locality Sensitive Avatars From Video</p>
<p>我们提出局部敏感化身(Locality-Sensitive Avatar)，这是一种基于神经辐射场(NeRF)的网络，用于从单目视频中学习人体运动。为此，我们通过从观测空间到规范空间的非线性映射，估计视频不同帧之间的规范表示，并将其分解为骨骼刚性运动和非刚性对应部分。我们的核心贡献在于：通过图神经网络(GNN)对非刚性部分进行建模，将姿态信息约束在相邻身体部位的局部范围内，从而保留细粒度细节。与先前仅在整个形状坐标空间操作的规范表示方法相比，我们的局部敏感运动建模能够同时还原逼真的形状轮廓与生动的细节。我们在ZJU-MoCap、SynWild、ActorsHQ、MVHumanNet及多种户外视频数据集上进行评估。实验表明，通过对规范特征空间进行局部敏感形变建模，我们首次实现了在新视角合成、新姿势动画和三维形状重建任务中均达到最先进水平。代码已开源：<a href="https://github.com/ChunjinSong/lsavatar">https://github.com/ChunjinSong/lsavatar</a>。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Dyadic Mamba: Long-term Dyadic Human Motion Synthesis</p>
<p>从文本描述生成逼真的双人交互人体运动面临重大挑战，尤其是在处理超出常规训练序列长度的长时交互场景时。尽管近期基于Transformer的方法在短时双人运动合成中展现出良好效果，但由于位置编码方案的内在限制，这些方法在生成长序列时表现欠佳。本文提出Dyadic Mamba，一种利用状态空间模型(SSMs)生成任意长度高质量双人人体运动的新方法。我们的方法采用简洁有效的架构设计，通过序列拼接实现个体运动序列间的信息流动，无需复杂的交叉注意力机制。实验表明，Dyadic Mamba在标准短时基准测试中取得了具有竞争力的性能，同时在长序列生成上显著优于基于Transformer的方法。此外，我们提出了评估长时运动合成质量的新基准测试，为未来研究提供标准化框架。研究结果表明，基于SSM的架构为解决文本驱动长时双人人体运动合成这一难题提供了新的研究方向。</p>
<div style="break-before: page; page-break-before: always;"></div><p>MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation</p>
<p>人体图像动画技术因在虚拟数字人领域的广泛应用而备受关注并快速发展。然而，现有方法主要依赖二维渲染姿态图像进行运动引导，这限制了泛化能力并丢失了开放世界动画所需的关键三维信息。为解决这一问题，我们提出了MTVCrafter(运动标记化视频生成器)，这是首个直接建模原始三维运动序列(即4D运动)的人体图像动画框架。具体而言，我们引入4DMoT(4D运动标记器)将三维运动序列量化为4D运动标记。相较于二维渲染姿态图像，4D运动标记能提供更稳健的时空线索，避免了姿态图像与角色之间严格的像素级对齐要求，从而实现了更灵活的解耦控制。接着我们提出MV-DiT(运动感知视频扩散变换器)，通过设计独特的4D位置编码运动注意力机制，MV-DiT能有效利用运动标记作为4D紧凑但富有表现力的上下文信息，在复杂三维世界中实现人体图像动画。这标志着该领域的重要进步，并为姿态引导的人类视频生成开辟了新方向。实验表明，我们的MTVCrafter以6.98的FID-VID指标达到当前最佳性能，较次优方法提升65%。凭借强大的运动标记，MTVCrafter还能良好泛化到各种风格和场景下的多样化开放世界角色(单体/群体、全身/半身)。视频演示和代码详见补充材料及匿名GitHub仓库：<a href="https://anonymous.4open.science/r/MTVCrafter-1B13">https://anonymous.4open.science/r/MTVCrafter-1B13</a>。</p>
<div style="break-before: page; page-break-before: always;"></div><p>TexTailor: Customized Text-aligned Texturing via Effective Resampling</p>
<p>我们提出了TexTailor，这是一种从文本描述生成一致性物体纹理的新方法。现有的文本到纹理合成方法利用深度感知扩散模型，通过预定义的多视角逐步生成图像并合成纹理。然而，这些方法会导致跨视角纹理属性逐渐偏移，主要原因在于：(1) 扩散过程中每个视角对先前合成纹理的信息整合不足；(2) 纹理合成过程的自回归特性。此外，预定义的相机位置选择未考虑物体几何形状，限制了不同视角合成纹理信息的有效利用，最终影响整体纹理一致性。</p>
<p>TexTailor通过以下创新解决这些问题：(1) 采用重采样方案，在扩散过程中反复整合来自先前合成纹理的信息；(2) 基于这些重采样纹理对深度感知扩散模型进行微调。在此过程中，我们发现仅使用少量训练图像会限制模型生成与条件对齐的高保真图像的能力，因此提出了性能保持损失函数来缓解该问题。同时，我们通过基于物体几何形状自适应调整相机位置，改进了视角一致性纹理的合成。在Objaverse数据集子集和ShapeNet汽车数据集上的实验表明，TexTailor在合成视角一致性纹理方面优于现有最先进方法。</p>
<div style="break-before: page; page-break-before: always;"></div><p>CameraCtrl: Enabling Camera Control for Video Diffusion Models</p>
<p>可控性在视频生成中起着至关重要的作用，它能让用户更精确地创建和编辑内容。然而，现有模型缺乏对相机位姿的控制——这种控制作为电影语言，能够表达更深层次的叙事细微差别。为了缓解这一问题，我们提出了\method，旨在为视频扩散模型实现精准的相机位姿控制。我们的方法探索了有效的相机轨迹参数化方法，并结合一个即插即用的相机位姿控制模块，该模块在视频扩散模型的基础上进行训练，而基础模型的其他模块保持不变。此外，我们对多种训练数据集的影响进行了全面研究，结果表明具有多样化相机分布且与基础模型外观相似的视频确实能增强可控性和泛化能力。实验结果证明了\method在使用不同视频生成模型时实现精确相机控制的有效性，这标志着通过文本和相机位姿输入实现动态且定制化视频叙事的探索又向前迈进了一步。</p>
<div style="break-before: page; page-break-before: always;"></div><p>ACT-R: Adaptive Camera Trajectories for 3D Reconstruction from Single Image</p>
<p>我们在多视角合成中引入自适应视图规划策略，旨在提升单视角三维重建的遮挡区域揭示能力和三维一致性。与独立或同步生成无序视角集合的传统方法不同，我们通过生成具有时序一致性的视角序列来增强三维连贯性。最关键的是，我们的视角序列并非基于预设的相机参数确定，而是通过计算自适应相机轨迹(ACT)——具体来说是一个环绕目标物体的相机轨道——该轨迹能够最大化待重建三维物体遮挡区域的可见性。确定最优轨道后，我们将其输入视频扩散模型以生成轨道周围的新视角，继而通过多视角三维重建模型获得最终重建结果。由于无需运行时训练或优化，仅需调用预训练模型进行遮挡分析和多视角合成的前向推理，我们的多视角合成流程具有显著效率优势。实验表明，本方法预测的相机轨迹能有效揭示遮挡区域并生成一致性新视角，在未见过的GSO数据集上，无论是定量还是定性评估，我们的方法均显著优于当前最佳的三维重建方法。  </p>
<div style="break-before: page; page-break-before: always;"></div><p>M3G: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human Motion Synthesis</p>
<p>在虚拟化身创建中，从音频生成包含面部、身体、手部和整体运动的全身人体姿态是一项重要但具有挑战性的任务。以往的系统主要关注于逐帧对人体姿态进行标记化处理，并通过输入音频预测每帧的标记。然而，我们发现一个现象：一个完整的富有表现力的人体姿态所需的帧数(定义为粒度)在不同姿态模式中存在差异。由于现有系统的姿态标记采用固定粒度，它们无法有效建模这些多样化的姿态模式。</p>
<p>为解决这一问题，我们提出了一种名为多粒度姿态生成器(Multi-Granular Gesture Generator, M3G)的创新框架，用于音频驱动的全身姿态生成。在M3G中，我们首先提出新型多粒度向量量化变分自编码器(Multi-Granular VQ-VAE, MGVQ-VAE)，用于从不同时间粒度对运动模式进行标记化处理并重建运动序列。随后，我们设计了多粒度标记预测器，能够从音频中提取多粒度信息并预测相应的运动标记。最终，M3G通过MGVQ-VAE从预测标记中重建人体姿态。主客观实验结果表明，我们提出的M3G框架在生成自然且富有表现力的全身人体姿态方面优于现有最先进方法。</p>
<div style="break-before: page; page-break-before: always;"></div><p>SplineGS: Learning Smooth Trajectories in Gaussian Splatting for Dynamic Scene Reconstruction</p>
<p>针对动态变形物体的复杂场景重建以实现新视角合成是一项极具挑战性的任务。近期研究通过引入三维高斯泼溅技术(3D Gaussian Splatting)，在静态场景的高质量快速重建方面取得了显著进展，其核心在于为高斯斑点的形变设计了专用模块。然而，如何设计一个能够有效融入适当时空归纳偏置的形变模块仍是一个亟待解决的问题。为此，本文提出SplineGS方法，通过采用非均匀有理B样条(NURBS)——B样条的扩展形式——来表征时间维度上的平滑形变。该方法基于NURBS学习一组代表性轨迹，并通过这些轨迹的线性组合来表达各高斯斑点的个体运动轨迹，从而确保空间平滑性。组合权重的优化则依托于以高斯斑点位置为键值的多分辨率哈希表和MLP网络协同完成。得益于这种创新设计，本方法无需对运动轨迹施加正则化约束即可实现高效训练。实验结果表明，所提方法在保持训练时间大幅缩短的同时，其性能较现有方法具有显著竞争力。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>CyberHost: A One-stage Diffusion Framework for Audio-driven Talking Body Generation</p>
<p>基于扩散的视频生成技术已取得显著进展，推动了人体动画研究的蓬勃发展。尽管通过多种模态驱动人像动画已实现突破，但目前人体动画解决方案仍主要聚焦于视频驱动方法，使得音频驱动的说话身体生成研究相对较少。本文提出CyberHost——一种单阶段音频驱动的说话身体生成框架，有效解决了半身动画中常见的合成退化问题，包括手部完整性、身份一致性和动作自然性。CyberHost的核心设计体现在两个创新点：首先，区域注意力模块(RAM)通过维护一组可学习的隐式身份无关潜在特征，并将其与特定身份局部视觉特征相结合，显著提升了关键区域(如手部)的合成质量；其次，人体先验引导条件通过引入更多人体结构先验知识，有效降低了生成动作模式的不确定性，从而提升了生成视频的稳定性。据我们所知，CyberHost是首个能够实现人体零样本视频生成的单阶段音频驱动扩散模型。大量实验表明，CyberHost在定量评估和定性分析方面均优于现有方法。此外，该框架可扩展应用于视频驱动及音视频混合驱动场景，同样能获得令人满意的生成效果。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Sketch2Anim: Towards Transferring Sketch Storyboards into 3D Animation</p>
<p>分镜脚本被广泛用于创建3D动画。动画师通过反复试错的过程，将分镜中的2D草图作为参考来制作理想的3D动画。传统方法需要极高的专业素养，且人力密集、耗时冗长。因此，业界对能直接将2D分镜草图转化为3D动画的自动化方法有着迫切需求。该任务迄今尚未得到充分探索，受运动扩散模型重大进展的启发，我们提出从条件运动合成的角度来解决这一问题。为此，我们开发了Sketch2Anim系统，该系统包含草图约束理解和运动生成两大核心模块。</p>
<p>具体而言，鉴于2D草图与3D运动之间存在显著领域差异，我们摒弃直接基于2D输入的传统思路，转而设计了一个3D条件运动生成器。该生成器通过同时利用3D关键姿势、关节轨迹和动作词汇，实现了精确细粒度的运动控制。此外，我们创新性地开发了神经映射器，专门用于在共享嵌入空间中对齐用户提供的2D草图与其对应的3D关键姿势和轨迹，这首次实现了对运动生成的直接2D控制。得益于多条件运动生成器的灵活性，我们的方法不仅能成功将分镜脚本转化为高质量3D运动，还天然支持直接进行3D动画编辑。通过全面的实验评估和用户感知研究，我们验证了该方法的有效性。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured 3D Assets</p>
<p>尽管生成式人工智能在文本、图像、音频和视频领域取得了显著进展，但3D生成技术由于数据稀缺性、算法局限性及生态系统碎片化等基础性挑战，仍处于相对欠发展阶段。为此，我们推出Step1X-3D开放框架，通过以下创新方案应对这些挑战：(1)构建严格的数据处理管线，对超过500万项数字资产进行筛选，最终形成包含200万项具备标准化几何与纹理属性的高质量数据集；(2)开发两阶段原生3D架构，整合混合式VAE-DiT几何生成器与基于扩散模型的纹理合成模块；(3)实现模型、训练代码及适配模块的全面开源。在几何生成方面，混合VAE-DiT组件通过采用基于感知器的潜在编码技术，配合锐利边缘采样策略以保留细节特征，最终输出TSDF(截断符号距离函数)表征。基于扩散模型的纹理合成模块则通过几何条件约束与潜在空间同步机制，确保跨视角纹理一致性。基准测试结果表明，该框架不仅以显著优势超越现有开源方法，更在生成质量上与商业闭源解决方案形成有力竞争。值得关注的是，该框架首次实现了2D与3D生成范式的贯通，支持将2D控制技术(如LoRA)直接迁移应用于3D合成场景。通过同步提升数据质量、算法保真度与可复现性，Step1X-3D旨在为可控式3D资产生成的开放研究树立新标杆。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Link to the Past: Temporal Propagation for Fast 3D Human Reconstruction from Monocular Video</p>
<p>从单目视频实现快速3D穿衣人体重建仍然是计算机视觉领域的重要挑战，尤其是在计算效率与重建质量之间取得平衡方面。现有方法要么专注于静态图像重建但计算量过大，要么通过逐视频优化实现高质量重建却需要数分钟至数小时的运算时间，这使得它们无法满足实时应用需求。为此，我们提出了TemPoFast3D这一创新方法，通过利用人体外观的时间连贯性来减少冗余计算，同时保持重建质量。我们的方法是一种&quot;即插即用&quot;解决方案，通过高效的坐标映射维护并优化规范外观表示，独特地将像素对齐的重建网络转化为适用于连续视频流处理的系统。大量实验表明，TemPoFast3D在各项标准指标上达到或超越了当前最先进方法，能够在不同姿态和外观条件下提供高质量的纹理重建，最高处理速度可达12帧/秒。</p>
<div style="break-before: page; page-break-before: always;"></div><p>ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models</p>
<p>目前基于扩散模型的文本到视频生成方法仅限于生成单镜头的短视频片段，缺乏生成带有离散转场的多镜头视频的能力（即同一角色在相同或不同背景下执行不同活动的场景）。为了突破这一限制，我们提出了一个包含数据集收集流程和视频扩散模型架构扩展的框架，以实现文本到多镜头视频生成。我们的方法能够生成将所有镜头视为单一视频的多镜头视频，通过对所有镜头所有帧的全局注意力机制确保角色与背景的一致性，同时允许用户通过镜头特定条件控制镜头的数量、时长及内容。这一效果通过以下创新实现：(1) 在文本到视频模型中引入转场标记(transition token)，用于控制新镜头的起始帧；(2) 采用局部注意力掩码策略(local attention masking strategy)，既控制转场标记的作用范围，又支持镜头特定提示。为了获取训练数据，我们设计了一个新颖的数据收集流程，能够从现有单镜头视频数据集构建多镜头视频数据集。大量实验表明，仅需对预训练文本到视频模型进行数千次迭代的微调，模型即可具备生成具备镜头级控制能力的多镜头视频，其性能显著超越基线方法。更多技术细节请访问项目网站：<a href="https://shotadapter.github.io/">https://shotadapter.github.io/</a></p>
<div style="break-before: page; page-break-before: always;"></div><p>Human Motion Prediction via Test-domain-aware Adaptation with Easily-available Human Motions Estimated from Videos</p>
<p>在三维人体运动预测(HMP)领域，传统方法依赖成本高昂的动作捕捉数据训练模型。然而，此类运动捕捉数据的高采集成本限制了数据多样性，导致模型对未见过的运动模式或人体对象的泛化能力不足。针对这一问题，本文提出通过利用易获取视频中估计的姿态数据进行附加学习来增强HMP模型。具体而言，我们从单目视频中提取的二维姿态经过精心设计的处理流程，转化为类运动捕捉的三维动作序列。通过对获得的三维动作进行补充训练，HMP模型能够有效适配测试域特性。实验结果表明，该方法在定量指标和视觉效果上均展现出显著提升。</p>
<div style="break-before: page; page-break-before: always;"></div><p>MAGE:A Multi-stage Avatar Generator with Sparse Observations</p>
<p>从头戴式设备推断全身姿态：一种基于多阶段渐进预测的运动补全方法</p>
<p>仅能获取头部和手腕三个关节观测数据的头戴式设备，要实现全身姿态推断是一项具有重要AR/VR应用价值的挑战性任务。现有方法主要聚焦于学习单阶段运动映射关系，但由于未观测身体关节运动的推断空间过大，往往导致下肢预测效果欠佳、时间一致性不足，进而产生不真实或连贯性差的运动序列。为此，我们提出了一种强大的多阶段虚拟化身生成器MAGE，通过渐进式预测策略对单阶段直接运动映射学习进行分解。具体而言，给定初始的三关节运动数据，MAGE从六部位基础身体表征出发，逐步细化至22个关节，在不同抽象粒度层级上推断多尺度身体部位姿态。随着抽象层级逐步降低，MAGE通过引入前序预测阶段提供的运动上下文先验知识，在更丰富的约束条件和更小的模糊性下提升运动补全的真实性。基于大规模数据集的大量实验表明，MAGE在动作准确性和连续性方面显著优于现有最优方法。</p>
<div style="break-before: page; page-break-before: always;"></div><p>DiffLocks: Generating 3D Hair from a Single Image using Diffusion Models</p>
<p>我们致力于解决从单张图像生成三维头发几何结构的任务，这一任务由于发型多样性及缺乏成对的图像-三维头发数据而极具挑战性。先前的方法主要依赖合成数据进行训练，并通过使用低维中间表征(如引导发丝和头皮层级嵌入)应对数据量不足的问题，但这些表征需要后处理步骤进行解码、上采样和真实性增强。这些方法无法重建细节丰富的发型，在处理卷发时效果欠佳，或仅能处理有限数量的发型。</p>
<p>为突破这些限制，我们提出了DiffLocks这一创新框架，能够直接从单张图像实现多种复杂发型的细节重建。首先，我们通过自动化流程创建了迄今为止最大的合成头发数据集(包含4万种发型)，解决了三维头发数据匮乏的问题。其次，我们利用该合成数据集训练了一个图像条件化的扩散变换器模型，使其能够通过单张正面图像生成精确的三维发丝。得益于预训练的图像主干网络，我们的方法虽然仅使用合成数据训练，却能泛化至真实场景图像。该扩散模型预测的头皮纹理图中，任意位置均包含单根发丝的潜在编码。这些编码可直接解码为三维发丝，无需任何后处理技术。</p>
<p>通过直接表征单根发丝而非引导发丝，变换器能够建模复杂发型的精细空间结构。基于此，DiffLocks首次实现了从单张图像重建高度卷曲发型(如非洲式发型)的能力。相关数据和代码已发布于<a href="https://radualexandru.github.io/difflocks/">https://radualexandru.github.io/difflocks/</a>。</p>
<div style="break-before: page; page-break-before: always;"></div><p>TeGA: Texture Space Gaussian Avatars for High-Resolution Dynamic Head Modeling</p>
<p>通过3D高斯溅射实现的稀疏体积重建与渲染技术，近期使得可动画的3D头部虚拟形象能够以任意视角呈现令人惊叹的逼真效果。如今，这种超写实虚拟化身被视为远程呈现、扩展现实和娱乐领域新兴应用的核心组件。构建此类逼真化身需要从输入视频图像中估计不同面部组件的复杂非刚性运动；由于运动估计不精确，可动画模型相较于基于单一面部表情构建的不可动画模型，通常存在保真度和细节的损失。此外，当前最先进的模型常受限于内存容量，导致用于建模的3D高斯数量减少，进而影响细节表现和渲染质量。</p>
<p>针对这些问题，我们提出了一种新型高细节3D头部虚拟化身模型，在现有技术基础上实现了显著提升：大幅增加3D高斯数量并优化建模质量，支持4K分辨率渲染。我们的高质量模型通过多视角输入视频重建，并基于网格化3D形变模型构建，该模型为头部提供了粗粒度变形层。超写实外观通过嵌入网格连续UVD切线空间的3D高斯进行建模，能够在最需要细节的区域实现更高效的致密化。此外，这些高斯元素通过新型UVD变形场进行扭曲，以捕捉细微的局部运动。</p>
<p>我们的核心创新在于新颖的可变形高斯编码方案及整体拟合流程，这使得头部模型能够在保持外观细节的同时，准确捕捉面部运动和其他瞬态高频特征(如皮肤褶皱)。该技术突破主要体现在两个方面：首先通过UVD空间优化高斯分布，显著提升了局部区域的几何精度；其次引入的动态变形机制有效平衡了全局形变与局部细节保留，为高保真虚拟化身建模开辟了新路径。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Anymate: A Dataset and Baselines for Learning 3D Object Rigging</p>
<p>绑定(rigging)与蒙皮(skinning)是创建逼真3D动画的关键步骤，通常需要大量专业知识和人工操作。传统自动化这些流程的尝试主要依赖几何启发式方法，但在处理复杂几何物体时往往面临困难。近年来数据驱动方法展现出更好的泛化潜力，但常受限于训练数据规模不足。我们提出了Anymate数据集——一个包含23万个3D资源的大型数据集，每个资源均配有专家制作的绑定与蒙皮信息，其规模是现有数据集的70倍。基于该数据集，我们提出了基于学习的自动绑定框架，包含关节预测、连接性预测和蒙皮权重预测三个连续模块。我们系统性地设计与实验了多种架构作为各模块的基准，并通过全面评估比较其性能。我们的模型显著优于现有方法，为未来自动绑定与蒙皮方法的比较奠定了基础。代码与数据集可通过<a href="https://anymate3d.github.io/%E8%8E%B7%E5%8F%96">https://anymate3d.github.io/获取</a>。</p>
<div style="break-before: page; page-break-before: always;"></div><p>ReactDance: Progressive-Granular Representation for Long-Term Coherent Reactive Dance Generation</p>
<p>反应式舞蹈生成(Reactive Dance Generation, RDG)通过结合引导舞者动作和音乐输入来生成跟随者动作，同时确保空间协调性和时间连贯性。然而现有方法过度强调全局约束和优化，忽视了细粒度空间交互、局部时间上下文等局部信息。为此，我们提出了ReactDance——一个基于扩散模型的新型框架，能够实现具有长期连贯性和多尺度可控性的高保真RDG。针对双人舞合成中存在的交互保真度、同步性和时间一致性等挑战，我们提出两大核心创新：(1)组残差有限标量量化(GRFSQ)，这种多尺度解耦动作表征技术能够从粗粒度的身体律动到细粒度的关节动力学全面捕捉交互语义；(2)分块局部上下文(BLC)，该采样策略通过局部块因果掩码和周期性位置编码，有效消除长序列生成中的误差累积。基于解耦的多尺度GRFSQ表征，我们构建了配备层解耦无分类器指导(LDCFG)的扩散模型，实现跨尺度运动语义的精细化控制。在标准基准上的大量实验表明，ReactDance超越了现有方法，达到了最先进的性能水平。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Apply Hierarchical-Chain-of-Generation to Complex Attributes Text-to-3D Generation</p>
<p>当前文本到3D模型虽能生成高质量资产，但在处理复杂属性对象时仍存在挑战。关键障碍包括：(1) 现有方法主要通过文本编码器继承文本到图像模型的语义理解能力，但文本编码器对长描述的解析能力有限，导致跨注意力焦点偏移，进而引发生成结果中的属性绑定错误；(2) 被遮挡部件需要严格的生成顺序和显式解耦机制。尽管已有工作引入人工干预缓解上述问题，但其质量不稳定且高度依赖人工输入。</p>
<p>为此，我们提出自动化分层生成链(HCoG)方法。该方法利用大语言模型将长描述解构为表示不同部件的信息块，并依据遮挡关系从内向外排序，形成层级生成链。在模块内部：先粗粒度创建组件，再通过目标区域定位与对应的3D高斯核优化实现精确属性绑定；在模块间：通过高斯扩展与标签消除技术，以扩展新高斯核、重分配语义标签、清理冗余核的方式无缝衔接部件生成，确保新增部件不干扰已优化部分。实验证明HCoG能生成结构连贯、属性精确的复杂属性3D对象。代码已开源：<a href="https://github.com/Wakals/GASCOL">https://github.com/Wakals/GASCOL</a>。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Humanise: Language-conditioned human motion generation in 3d scenes</p>
<p>由于现有人-场景交互（HSI）数据集在规模和语义上的局限性，学习生成多样化、场景感知且目标导向的3D人体运动仍面临挑战。这些数据集普遍存在规模有限、质量平庸且语义信息匮乏的问题。为填补这一空白，我们通过将捕捉到的人体运动序列与多样化的3D室内场景对齐，提出了一个大规模且富含语义的合成HSI数据集HUMANISE。我们采用自动化方法为对齐后的运动数据标注语言描述，这些描述不仅包含动作信息，还明确指向具体交互物体（例如&quot;坐在书桌旁的扶手椅上&quot;）。由此，HUMANISE开启了一个全新的生成任务 —— 基于语言条件的3D场景人体运动生成。该任务的挑战性在于需要同时对3D场景、人体运动和自然语言进行联合建模。为此，我们提出了一种创新的场景-语言条件生成模型，能够生成与指定物体进行目标交互的3D人体运动。实验结果表明，该模型可在3D场景中生成多样化且语义一致的人体运动。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Synthesizing Diverse Human Motions in 3D Indoor Scenes</p>
<p>我们提出了一种在三维室内场景中生成虚拟人类的新方法，这些虚拟人能够在环境中自主导航并以逼真的方式与物体互动。现有方法依赖于包含捕捉人体动作和对应交互三维场景的高质量训练序列，然而这类交互数据获取成本高昂、采集困难，且难以覆盖复杂室内环境中所有合理的人-场景交互可能性。为解决这些挑战，我们提出了一种基于强化学习的方法，通过习得的运动控制策略驱动虚拟人在三维场景中实现自主导航和逼真的物体交互。该运动控制策略采用隐式运动动作空间，这些空间对应真实的运动基元，并通过强大的生成式运动模型从大规模动作捕捉数据中学习获得。</p>
<p>针对三维环境中的导航问题，我们设计了一种场景感知策略，其创新性的状态和奖励机制能有效规避碰撞。结合基于导航网格的路径规划算法生成中间航点，我们的方法能够合成虚拟人在三维室内场景中多样化避障导航的运动轨迹。为实现细粒度的人-物体交互，我们采用基于标记点的身体表征精心设计交互目标指引，并利用基于有向距离场（SDF）的特征编码人-场景邻近关系。即使面对物体形状、朝向、初始身体位置和姿态各异的分布外测试场景，我们的方法仍能合成逼真多样的人-物体交互（如坐在椅子上后起身）。实验结果表明，该方法在运动自然性和多样性方面均优于现有最先进的人-场景交互合成方法。相关代码、模型及演示视频已开源发布于：<a href="https://zkf1997.github.io/DIMOS">https://zkf1997.github.io/DIMOS</a>。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Move as You Say, Interact as You Can:Language-guided Human Motion Generation with Scene Affordance</p>
<p>尽管文本到动作合成技术取得了显著进展，但在3D环境中生成语言引导的人体动作仍面临重大挑战。这些挑战主要源于两个核心问题：(i)目前缺乏能够联合建模自然语言、3D场景和人体动作的强效生成模型；(ii)生成模型对数据的高度需求与当前缺乏全面、高质量的语言-场景-动作数据集之间的矛盾。为解决这些问题，我们提出了一种创新的两阶段框架，通过引入场景功能（scene affordance）作为中间表征，有效桥接了3D场景基础与条件动作生成。该框架包含两个核心组件：用于预测显式功能图的功能扩散模型（ADM），以及用于生成合理人体动作的功能到动作扩散模型（AMDM）。通过场景功能图的中间转换，我们的方法成功解决了在多模态条件信号下生成人体动作的难题，尤其在缺乏大量语言-场景-动作配对数据的有限训练条件下表现优异。大量实验表明，我们的方法在HumanML3D和HUMANISE等基准测试中持续优于所有基线模型。此外，我们通过专门构建的评估集验证了模型的卓越泛化能力，该评估集包含模型训练时未接触过的新描述和新场景。</p>
<div style="break-before: page; page-break-before: always;"></div><p>3D Scene Generation: A Survey</p>
<p>三维场景生成旨在合成具有空间结构、语义信息丰富且逼真的虚拟环境，其应用涵盖沉浸式媒体、机器人、自动驾驶和具身智能等领域。早期基于程序化规则的方法虽具备可扩展性，但生成多样性受限。随着深度生成模型(如生成对抗网络、扩散模型)和三维表征技术(如神经辐射场、三维高斯分布)的突破性进展，研究者得以学习真实场景分布规律，显著提升了生成结果的真实感、多样性和视角一致性。以扩散模型为代表的创新方法通过将三维场景生成重构为图像或视频合成问题，有效弥合了场景合成与真实感渲染之间的鸿沟。本文系统梳理了该领域前沿进展，将现有方法归纳为四大范式：程序化生成、基于神经三维的生成、基于图像的生成和基于视频的生成，深入剖析了各范式的技术原理、优劣权衡及代表性成果。同时，本文详细评述了常用数据集、评估体系及下游应用，并针对生成能力、三维表征、数据标注和评估方法等核心挑战展开讨论，提出未来发展方向包括：提升生成质量、开发物理感知与交互式生成技术、构建感知-生成一体化模型等。本综述系统整合了三维场景生成领域的最新突破，并展望了生成式人工智能、三维视觉与具身智能交叉融合的创新方向。为追踪动态发展，我们维护了最新项目页面：<a href="https://github.com/hzxie/Awesome-3D-Scene-Generation">https://github.com/hzxie/Awesome-3D-Scene-Generation</a>。</p>
<div style="break-before: page; page-break-before: always;"></div><p>SVAD: From Single Image to 3D Avatar via Synthetic Data Generation with Video Diffusion and Data Augmentation</p>
<p>从单张图像创建高质量可驱动的3D人体化身仍然是计算机视觉领域的重要挑战，这源于从单一视角重建完整3D信息的内在困难。现有方法面临明显局限：3D高斯泼溅(3DGS)方法虽能生成高质量结果，但需要多视角或视频序列输入；而视频扩散模型虽然能从单张图像生成动画，但在保持一致性和身份特征方面存在不足。我们提出SVAD方法，通过整合现有技术的互补优势突破这些限制。该方法通过视频扩散生成合成训练数据，采用身份特征保留和图像修复模块进行增强，并利用优化后的数据训练3DGS化身。综合评估表明，SVAD在保持身份一致性和跨新姿态/视角的细节呈现方面优于当前最先进的单图方法，同时具备实时渲染能力。通过构建数据增强流程，我们克服了传统3DGS方法对密集单目或多视图训练数据的依赖。大量定量与定性对比实验显示，本方法在多项指标上均超越基线模型。通过有效结合扩散模型的生成能力与3DGS的高质量输出及渲染效率，我们的工作为单图输入的高保真化身生成建立了新范式。</p>
<div style="break-before: page; page-break-before: always;"></div><p>ReAlign: Bilingual Text-to-Motion Generation via Step-Aware Reward-Guided Alignment</p>
<p>双语文本到运动生成通过双语文本输入合成3D人体运动，在游戏、影视和机器人等跨语言应用领域具有巨大潜力。然而，该任务面临两大关键挑战：缺乏双语运动-语言数据集，以及扩散模型中文本与运动分布的不对齐问题，导致生成运动存在语义不一致或质量低下的情况。为应对这些挑战，我们提出了BiHumanML3D双语人体运动数据集，为双语文本到运动生成模型建立了重要基准。此外，我们开发了双语运动扩散模型(BiMD)，通过跨语言对齐表征捕捉语义信息，实现统一的双语建模。在此基础上，我们提出了奖励引导的采样对齐方法(ReAlign)，包含用于评估采样过程中对齐质量的步态感知奖励模型，以及引导扩散过程向最优对齐分布演进的奖励引导策略。该奖励模型融合步态感知标记，结合保证语义一致性的文本对齐模块和提升真实性的运动对齐模块，通过在每一步优化噪声运动，平衡概率密度与对齐效果。实验表明，相较于现有最优方法，我们的方法在文本-运动对齐质量和运动生成质量上均有显著提升。项目主页：<a href="https://wengwanjiang.github.io/ReAlign-page/">https://wengwanjiang.github.io/ReAlign-page/</a>。</p>
<div style="break-before: page; page-break-before: always;"></div><p>MeshGen: Generating PBR Textured Mesh with Render-Enhanced Auto-Encoder and Generative Data Augmentation</p>
<p>在本论文中，我们提出了MeshGen这一先进的图像到三维生成流程，能够生成具有精细几何结构和基于物理渲染(PBR)纹理的高质量三维网格。针对现有三维原生扩散模型面临的挑战——包括自编码器性能欠佳、可控性有限、泛化能力弱以及基于图像的PBR纹理不一致等问题，MeshGen通过多项关键创新突破这些限制。我们首创了渲染增强的点到形状自动编码器，通过设计基于光线正则化的感知优化方法，将网格压缩至紧凑的潜在空间，确保三维形状的精准表征与重建，有效保留潜在空间内的几何细节。</p>
<p>为解决数据稀缺和图像-形状错位问题，我们进一步提出几何增强和生成式渲染增强技术，显著提升了模型的可控性和泛化能力，使其在有限公共数据集条件下仍能保持优异表现。在纹理生成方面，MeshGen采用基于参考注意力的多视角ControlNet实现一致性外观合成，并辅以多视角PBR分解器估算PBR分量，以及UV修复器填充不可见区域，确保三维网格实现无缝衔接的连贯纹理。</p>
<p>大量实验表明，MeshGen在形状和纹理生成方面均显著超越现有方法，为基于PBR纹理的三维网格生成质量树立了新标杆。代码详见<a href="https://github.com/heheyas/MeshGen">https://github.com/heheyas/MeshGen</a>，项目页面请访问<a href="https://heheyas.github.io/MeshGen">https://heheyas.github.io/MeshGen</a>。</p>
<div style="break-before: page; page-break-before: always;"></div><p>TokenHSI: Unified Synthesis of Physical Human-Scene Interactions through Task Tokenization</p>
<p>合成多样化且物理合理的人-场景交互(HSI)对计算机动画和具身人工智能都至关重要。尽管已有可喜进展，但现有方法主要集中于开发独立控制器，每个控制器仅专精于特定交互任务。这严重限制了应对需要多技能整合的复杂HSI任务的能力，例如在携带物体时完成坐下动作。针对这一问题，我们提出了TokenHSI——一个基于Transformer的统一策略框架，能够实现多技能统一与灵活适应。其核心洞见在于：将人体本体感知建模为独立共享令牌，并通过掩码机制与不同任务令牌相结合。这种统一策略实现了跨技能的有效知识共享，从而促进多任务训练。此外，我们的策略架构支持可变长度输入，使习得技能能灵活适应新场景。通过训练额外任务令牌生成器，我们不仅能修改交互目标的几何形态，还能协调多项技能以应对复杂任务。实验表明，该方法能显著提升各类HSI任务中的多功能性、适应性和可扩展性。项目网站：<a href="https://liangpan99.github.io/TokenHSI/">https://liangpan99.github.io/TokenHSI/</a></p>
<div style="break-before: page; page-break-before: always;"></div><p>ELGAR: Expressive Cello Performance Motion Generation for Audio Rendition</p>
<p>乐器演奏艺术是人类创造力与情感的生动体现。然而，生成乐器演奏动作是一项极具挑战性的任务，因其不仅需要捕捉复杂肢体运动，还需重构演奏者与乐器之间相互作用的动态关系。现有研究主要集中于局部肢体运动的建模，为此我们提出ELGAR(基于音频的富有表现力大提琴演奏动作生成系统)，这是一种基于扩散模型的先进框架，能够仅凭音频信号即可生成全身细粒度的乐器演奏动作。为强调乐器演奏的交互本质，我们创新性地引入手部交互接触损失(HICL)和琴弓交互接触损失(BICL)，有效保证了交互过程的真实性。此外，为更好评估生成动作与音乐音频语义的契合度，我们专门针对弦乐器演奏动作生成设计了新型评价指标，包括指板接触距离、弓弦距离及运弓评分。通过大量评估与消融实验验证了所提方法的有效性。同时，我们构建了动作生成数据集SPD-GEN，该数据集整理并规范化自动作捕捉数据集SPD。实验表明，ELGAR在生成具有复杂快速交互特征的乐器演奏动作方面展现出巨大潜力，将有力推动动画制作、音乐教育、交互艺术创作等领域的进一步发展。</p>
<div style="break-before: page; page-break-before: always;"></div><p>PARC: Physics-based Augmentation with Reinforcement Learning for Character Controllers</p>
<p>人类在利用敏捷运动技能驾驭多样化复杂环境方面表现出色，跑酷运动员便是典型例证——他们能够完成攀越墙体、跨越沟壑等高动态动作。然而在虚拟角色中复现这类敏捷动作仍面临挑战，部分原因在于敏捷地形穿越行为的运动捕捉数据稀缺且获取成本高昂。本研究提出PARC(基于物理模拟增强与强化学习的角色控制框架)，该框架通过机器学习与物理仿真技术迭代增强运动数据集，持续拓展地形穿越控制器的能力。</p>
<p>PARC的实施流程分为三个阶段：首先基于核心地形穿越技能的小型数据集训练运动生成器；随后利用该生成器为新地形场景生成合成运动数据。然而这些生成动作常存在接触点错误或动作不连贯等异常现象。为修正这些问题，我们训练基于物理原理的追踪控制器以在仿真中模仿生成动作。校正后的动作将被纳入数据集，用于下一轮迭代中运动生成器的持续训练。通过这种迭代机制，PARC实现了运动生成器与追踪控制器的协同进化，最终构建出能够与复杂环境进行敏捷交互的通用模型。该框架有效弥合了运动数据稀缺与通用角色控制器需求之间的鸿沟，为开发敏捷地形穿越控制器提供了创新解决方案。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Person-In-Situ: Scene-Consistent Human Image Insertion with Occlusion-Aware Pose Control</p>
<p>将人体图像合成到场景图像中在娱乐和广告等领域具有广泛的应用前景。然而，现有方法通常无法处理前景物体对插入人物的遮挡问题，且往往将人物简单地放置在最前景层。此外，这些方法对人物姿态的控制能力也较为有限。为解决这些挑战，我们提出了两种创新方法。这两种方法均通过3D人体模型实现显式的姿态控制，并利用潜在扩散模型将人物合成到上下文语境中恰当的深度位置，无需依赖遮挡蒙版即可自然地处理遮挡关系。</p>
<p>第一种是两阶段方法：模型首先通过监督学习生成包含人物的场景深度图，然后基于该深度图合成人物图像。第二种方法则通过隐式学习遮挡关系，无需显式的深度监督即可直接从输入数据中生成人物图像。定量和定性评估表明，两种方法在准确反映遮挡关系和用户指定姿态的同时，能够更好地保持场景一致性，其综合性能优于现有方法。</p>
<div style="break-before: page; page-break-before: always;"></div><p>PrimitiveAnything: Human-Crafted 3D Primitive Assembly Generation with Auto-Regressive Transformer</p>
<p>形状基元抽象通过将复杂三维形状分解为简单几何元素，在人类视觉认知中起着关键作用，并在计算机视觉和图形学领域具有广泛应用。尽管三维内容生成技术近年来取得了显著进展，但现有的基元抽象方法要么依赖有限语义理解的几何优化，要么通过小规模、特定类别的数据集进行学习，难以泛化到多样化的形状类别。我们提出了PrimitiveAnything框架，将形状基元抽象重新定义为基元装配生成任务。该框架包含一个基于形状条件约束的基元Transformer模块用于自回归生成，以及一个无歧义参数化方案来统一表示多种类型的几何基元。通过从大规模人工标注的抽象基元中直接学习装配过程，该框架能够准确捕捉人类分解复杂形状的认知模式。大量实验表明，PrimitiveAnyting生成的基元装配在保持跨类别几何保真度的同时，能够更好地与人类感知对齐。该方法可赋能多种三维应用，并展现出在游戏领域支持基于基元的用户生成内容（UGC）的潜力。项目主页：<a href="https://primitiveanything.github.io">https://primitiveanything.github.io</a></p>
<div style="break-before: page; page-break-before: always;"></div><p>Bridging Geometry-Coherent Text-to-3D Generation with Multi-View Diffusion Priors and Gaussian Splatting</p>
<p>分数蒸馏采样(Score Distillation Sampling, SDS)通过利用预训练的二维扩散模型推动了文本到三维生成技术的发展，但其忽视了多视角关联性，导致生成的三维内容容易出现几何不一致性和多面伪影问题。本研究提出耦合分数蒸馏(Coupled Score Distillation, CSD)框架，通过耦合多视角联合分布先验，在确保三维生成几何一致性的同时，实现对三维高斯溅射(3D Gaussian Splatting)的稳定直接优化。具体而言，通过将优化问题重构为多视角联合优化任务，我们推导出有效的优化规则，该规则通过耦合多视角先验有效指导不同视角间的协同优化，同时保持生成三维资产的多样性。此外，我们提出了直接优化随机初始化的三维高斯溅射(3D-GS)来生成几何一致三维内容的框架，并采用从3D-GS初始化、通过CSD细化的可变形四面体网格，最终产出高质量精修网格。定量与定性实验结果表明，本方法在生成效率与质量方面均展现出竞争优势。</p>
<div style="break-before: page; page-break-before: always;"></div><p>FlexiAct: Towards Flexible Action Control in Heterogeneous Scenarios</p>
<p>动作定制旨在根据输入的控制信号生成目标主体执行相应动作的视频。当前方法主要采用姿态引导或全局运动定制技术，但由于对空间结构(如布局、骨架和视角一致性)的严格限制，其适应不同主体和场景的能力有所下降。为了突破这些限制，我们提出了FlexiAct框架，能够将参考视频中的动作迁移至任意目标图像。与现有方法不同，FlexiAct允许参考视频主体与目标图像在布局、视角和骨骼结构上存在差异，同时保持身份一致性。</p>
<p>实现这一目标需要解决三个关键挑战：精确的动作控制、空间结构适配和一致性保持。为此，我们设计了RefAdapter——一种轻量级的图像条件适配器。该模块在空间适配和一致性保持方面表现优异，在平衡外观一致性与结构灵活性方面超越了现有方法。此外，我们通过观察发现，在去噪过程中，不同时间步对运动(低频)和外观细节(高频)的关注程度不同。基于此洞见，我们提出了频率感知动作提取(FAE)模块。与依赖独立时空架构的传统方法不同，FAE直接在去噪过程中实现动作提取。</p>
<p>实验表明，我们的方法能有效实现跨布局、跨骨架、跨视角的动作迁移。为促进相关研究，我们已在https://shiyi-zh0408.github.io/projectpages/FlexiAct/公开代码和模型权重。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Real-Time Person Image Synthesis Using a Flow Matching Model</p>
<p>姿势引导人物图像合成(Pose-Guided Person Image Synthesis，PGPIS)是指根据目标姿势和源图像生成逼真人物图像的技术。该任务在手语视频生成、AR/VR、游戏和直播等实际应用中发挥着关键作用。在这些场景中，实时PGPIS技术对于提供即时视觉反馈和维持用户沉浸感至关重要。然而，由于从多样化和动态的人体姿势合成高保真图像的复杂性，实现实时性能仍然是一个重大挑战。近年来基于扩散模型的方法在PGPIS中展现出令人印象深刻的图像质量，但其缓慢的采样速度阻碍了在时间敏感场景中的部署。这种延迟在需要快速图像更新的任务(如直播中生成手语视频)中尤为明显。因此，开发快速可靠的PGPIS模型是实现实时交互系统的关键步骤。</p>
<p>为应对这一挑战，我们提出了一种基于流匹配(Flow Matching，FM)的生成模型。该方法能够实现更快、更稳定、更高效的训练和采样过程。此外，所提出的模型支持条件生成并可在潜在空间运行，使其特别适用于对速度和质量都有严格要求的实时PGPIS应用。我们在广泛使用的DeepFashion数据集上对提出的流匹配实时人物图像合成模型(Real-Time Person Image Synthesis Using a Flow Matching Model，RPFM)进行了评估。实验结果表明，RPFM在保持与最先进模型相当性能的同时，实现了接近实时的采样速度。该方法通过略微但可接受的生成图像准确性降低，换取了生成速度两倍以上的提升，从而确保了实时性能。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Polar Coordinate-Based 2D Pose Prior with Neural Distance Field</p>
<p>人体姿态捕捉技术对于运动分析至关重要，能够实现对运动员动作的精准评估。尽管基于深度学习的人体姿态估计(HPE)模型在RGB视频公开数据集上取得了令人瞩目的性能，但在实际体育场景中，其有效性常受限于运动模糊、遮挡以及不同姿态表征方式之间的领域偏移问题。通过模型微调可部分缓解这些挑战，但通常需要大规模标注数据，且仍难以适应多样化运动环境的泛化需求。</p>
<p>为解决上述限制，我们提出了一种基于神经距离场(NDF)的二维姿态先验引导优化方法。与现有仅依赖人体姿态角度表征的方法不同，我们创新性地引入基于极坐标的表征方式，显式融合关节连接长度信息，从而实现对错误姿态估计的更精准校正。此外，我们定义了一种新型非测地线距离度量方法，通过分离角度差异与径向差异，验证了该度量相较于传统测地线距离更适用于极坐标表征。为缓解数据稀缺问题，我们开发了基于梯度的批量投影增强策略，通过迭代优化合成具有真实性的姿态样本。</p>
<p>本研究在跳远数据集上进行了验证，结果表明该方法能够提升跨多种姿态表征方式的二维姿态估计性能，展现出良好的跨领域鲁棒性。实验数据显示，我们的方法在仅需少量训练数据的情况下即可显著提升姿态合理性。项目代码已开源：<a href="https://github.com/QGAN2019/polar-NDF">https://github.com/QGAN2019/polar-NDF</a>。</p>
<div style="break-before: page; page-break-before: always;"></div><p>PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model</p>
<p>音频驱动的人体动画技术在人机交互领域得到广泛应用，而扩散模型的出现进一步推动了该技术的发展。当前大多数方法依赖多阶段生成和中间表示，导致推理时间较长，并在特定前景区域和音频-动作一致性方面存在生成质量问题。这些缺陷主要源于缺乏局部细粒度监督指导。为解决上述挑战，我们提出了PAHA——首个端到端的扩散模型驱动的音频驱动上半身人体动画框架。我们引入两个核心方法：部件感知重加权(PAR)和部件一致性增强(PCE)。PAR基于姿态置信度分数动态调整区域训练损失权重，有效提升视觉质量；PCE通过构建并训练基于扩散模型的区域视听分类器，改善动作与伴随语音的一致性。我们进一步为前述分类器设计了两种创新推理指导方法——序列指导(SG)和差分指导(DG)，分别实现效率与质量的平衡。此外，我们构建了首个公开的中文新闻主播语音数据集CNAS，以推动该领域的研究与验证。大量实验结果和用户研究表明，PAHA在音频-动作对齐和视频相关评估指标上显著优于现有方法。代码和CNAS数据集将在论文录用后开源。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>GUAVA: Generalizable Upper Body 3D Gaussian Avatar</p>
<p>从单张图像重建具有丰富面部和手部动作的高质量可动画三维人体化身技术，因其广泛的应用潜力而备受关注。传统三维人体化身重建通常需要多视角视频或单目视频，并要求针对每个个体进行独立训练，这一过程既复杂又耗时。此外，受限于SMPLX模型的表达能力，现有方法往往侧重于身体动作而难以捕捉细腻的面部表情。针对这些挑战，我们首先提出了富有表现力的人体模型(EHM)来增强面部表情建模能力，并开发了精确的追踪方法。基于该模板模型，我们推出了GUAVA框架——首个支持快速构建可动画上半身三维高斯化身的解决方案。通过逆纹理映射和投影采样技术，我们实现了从单张图像推断上半身(Ubody)高斯特征。渲染图像经过神经优化器进行精细化处理。实验结果表明，GUAVA在渲染质量上显著超越现有方法，同时实现了突破性的速度优化：重建过程仅需亚秒级时间(0.1秒)，并支持实时动画与渲染。</p>
<div style="break-before: page; page-break-before: always;"></div><p>DualReal: Adaptive Joint Training for Lossless Identity-Motion Fusion in Video Customization</p>
<p>基于预训练大规模模型的定制化文生视频技术近年来通过聚焦身份与运动一致性获得了广泛关注。现有方法通常采用孤立的定制范式，即单独定制主体身份或运动动态。然而，这种范式完全忽视了身份与运动之间固有的相互约束和协同依存关系，导致生成过程中身份与运动冲突不断积累并系统性劣化。为此，我们提出了DualReal创新框架，通过自适应联合训练在双维度间协作构建相互依赖关系。具体而言，DualReal包含两个核心单元：(1)双重感知适配单元通过动态选择训练阶段(身份或运动)，在冻结维度的先验指导下学习当前信息，并采用正则化策略避免知识泄露；(2)阶段混合控制器利用去噪阶段和扩散Transformer深度的对应关系，以自适应粒度指导不同维度，在多个阶段规避冲突，最终实现身份与运动模式的无损融合。我们构建了比现有方法更全面的评测基准，实验结果表明DualReal在CLIP-I和DINO-I指标上平均提升21.7%和31.8%，在几乎所有运动质量指标上均达到最优性能。</p>
<div style="break-before: page; page-break-before: always;"></div><p>SignSplat: Rendering Sign Language via Gaussian Splatting</p>
<p>通过高斯泼溅实现条件性人体渲染的现有最先进方法通常侧重于从多视角捕捉简单身体动作(如舞蹈或行走)。然而对于更复杂的用例(如手语)，我们更关注手部和面部细微且复杂的运动，而非大幅度的身体动作。由于捕捉手语多视角数据的复杂性，构建高保真模型的问题进一步加剧。解决方案在于更好地利用序列数据，通过挖掘时间变化性来克服有限视角信息带来的挑战。但基于序列数据的学习需要极其精确且一致的模型拟合，以确保复杂运动中外观的一致性。我们重点研究如何通过约束网格参数，构建能够从少量视角建模人体细微运动的高斯泼溅框架。我们对高斯参数施加正则化技术以缓解过拟合和渲染伪影问题。此外，提出了一种新的自适应控制方法来致密化高斯分布并修剪网格表面上的泼溅点。为验证方法的准确性，我们基于神经机器翻译方法构建手语拼接技术，渲染出新颖的手语视频序列。在基准数据集上，我们的方法达到业界领先水平；对于高度关节化和复杂的手语动作，其表现显著优于现有方法。</p>
<div style="break-before: page; page-break-before: always;"></div><p>MVHumanNet++: A Large-scale Dataset of Multi-view Daily Dressing Human Captures with Richer Annotations for 3D Human Digitization </p>
<p>在这个时代，大语言模型和文生图模型的成功得益于大规模数据集的驱动。然而在3D视觉领域，虽然通过Objaverse和MVImgNet等大规模数据集在以物体为中心的任务上取得了显著进展，但以人为中心的任务发展却相对受限，这主要源于缺乏可比拟的大规模人体数据集。为了填补这一空白，我们推出了MVHumanNet++数据集 —— 该数据集包含4500位人物身份的多视角人体动作序列。本项工作的核心在于通过多视角人体捕捉系统，采集具有大规模身份多样性和日常服饰特征的人类数据，这种采集方式具备高度的可扩展性。我们的数据集涵盖9000套日常服装、6万组动作序列和6.45亿帧画面，并提供丰富的标注信息，包括人体遮罩、相机参数、2D/3D关键点、SMPL/SMPLX参数及对应文本描述。此外，我们对MVHumanNet++数据集进行了增强处理，新增法线贴图和深度图标注，显著提升了其在先进人体中心研究中的适用性与实用价值。为了探索该数据集在2D/3D视觉任务中的潜力，我们开展了多项先导研究，充分展现了MVHumanNet++的规模优势所带来的性能提升与创新应用。作为当前最大规模的3D人体数据集，我们希望通过开放MVHumanNet++及其标注信息，推动大规模3D人体中心任务领域的创新发展。该数据集已通过<a href="https://kevinlee09.github.io/research/MVHumanNet++/">https://kevinlee09.github.io/research/MVHumanNet++/</a> 公开发布。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Co$^{3}$Gesture: Towards Coherent Concurrent Co-speech 3D Gesture Generation with Interactive Diffusion</p>
<p>从人类语音生成手势在虚拟化身动画领域取得了巨大进展。现有方法虽然能够合成单人独白场景下的协同手势，却忽视了双人交互对话场景下同步手势建模的实用性。此外，缺乏高质量的同步伴随语音手势数据集也限制了这一问题的解决。为实现这一目标，我们首先构建了大规模同步伴随语音手势数据集GES-Inter，包含超过700万帧多样化的双人交互姿势序列。在此基础上，我们提出新型框架Co³Gesture，能够生成包含双人交互动作的连贯同步伴随语音手势。考虑到对话双方身体动态的非对称性，我们的框架基于两个以分离说话者音频为条件的协作生成分支构建。具体而言，为增强对话过程中人体姿势与对应说话者语音的协调性，我们设计了时序交互模块（TIM）。该模块能够有效建模双说话者手势序列间的时序关联表征作为交互指导，并将其融入同步手势生成。此外，我们开发了互注意力机制，从整体上增强交互同步动作的依赖关系学习，从而实现生动连贯的手势生成。大量实验表明，我们的方法在新构建的GES-Inter数据集上优于当前最先进模型。数据集与源代码已发布于&lt;\href{https://mattie-e.github.io/Co3/}{\textit{https://mattie-e.github.io/Co3/}}。&gt;</p>
<div style="break-before: page; page-break-before: always;"></div><p>Efficient 3D Full-Body Motion Generation from Sparse Tracking Inputs with Temporal Windows</p>
<p>要在沉浸式增强现实/虚拟现实(AR/VR)应用中实现无缝用户体验，高效神经网络(NN)模型的重要性不言而喻。这是因为，在虚拟环境中进行完整的3D全身重建时，需要通过这些模型生成因传感器数量有限而无法捕捉的身体缺失部位。然而，当前最先进的神经网络模型通常计算成本高昂，且依赖更长的稀疏跟踪输入序列来捕获时间上下文以生成全身动作。这种对长序列的依赖不可避免地增加了计算开销，并在长期时间依赖关系中引入噪声，从而对生成性能产生负面影响。</p>
<p>本文提出了一种基于多层感知机(MLP)的创新方法，在平衡计算成本与内存开销的同时，显著提升了3D全身动作生成的综合性能。具体而言，我们设计了一种神经网络机制——将长输入序列分割为更小的时间窗口，随后通过潜在表示将当前动作与这些时间窗口的信息融合，从而利用历史上下文进行生成。实验表明，相较于现有最优方法，该机制使生成准确率得到显著提升，同时大幅降低计算成本和内存开销，使本方法适用于资源受限的设备。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Model See Model Do: Speech-Driven Facial Animation with Style Control</p>
<p>语音驱动的三维面部动画在虚拟化身、游戏和数字内容创作等应用中发挥着关键作用。尽管现有方法在实现精确唇形同步和生成基础情感表情方面取得了显著进展，但它们往往难以捕捉并有效传递细腻的表演风格。我们提出了一种新颖的基于示例的生成框架，通过将隐扩散模型与参考风格片段进行条件约束，可生成高表现力且时序连贯的面部动画。为解决精确遵循风格参考的挑战，我们引入了一种称为&quot;风格基底&quot;的创新条件调节机制：该机制从参考片段中提取关键姿势，以叠加方式引导扩散生成过程适配目标风格，同时保持唇形同步质量。这种方法使模型能够捕捉细微的风格线索，同时确保生成动画与输入语音高度契合。通过定性、定量和感知评估的全面验证，我们的方法在忠实复现目标风格的同时，实现了跨多种语音场景的卓越唇形同步效果。</p>
<div style="break-before: page; page-break-before: always;"></div><p>3D Human Pose Estimation via Spatial Graph Order Attention and Temporal Body Aware Transformer</p>
<p>当前，Transformer和图卷积网络(GCN)是三维人体姿态估计的主流技术。然而，基于Transformer的方法在使用骨架表征时要么忽略了关节之间的空间邻域关系，要么在骨架序列建模中忽视了局部关节运动的时序模式；而基于GCN的方法则常常未能构建姿态特异性表征。为解决这些问题，我们提出一种新方法：利用GCN的图建模能力，通过不同阶数的多图结构表征每个骨架，并引入新型图阶注意力模块动态强化各关节最具代表性的阶数特征。通过提出的时序身体感知Transformer对序列空间特征进行进一步处理，该模块在建模序列全局身体特征依赖关系时，能够感知关节间跨骨架的局部特征关联。鉴于我们的三维姿态输出与序列中心二维姿态对齐，我们改进了自注意力机制使其聚焦中心姿态特征，并沿序列首尾方向逐步弱化关注强度。在Human3.6m、MPI-INF-3DHP和HumanEva-I数据集上的大量实验验证了方法的有效性。代码和模型已在Github开源。</p>
<div style="break-before: page; page-break-before: always;"></div><p>JointDiT: Enhancing RGB-Depth Joint Modeling with Diffusion Transformers</p>
<p>我们提出了JointDiT —— 一种对RGB图像与深度图进行联合分布建模的扩散变换器。通过利用最先进的扩散变换器架构优势及其卓越的图像先验知识，JointDiT不仅能生成高保真度的图像，还能生成几何合理且精确的深度图。这种稳健的联合分布建模是通过我们提出的两项简单而有效的技术实现的：根据各模态噪声水平动态调整的自适应调度权重，以及非平衡时间步采样策略。借助这些技术，我们实现了模型在所有噪声水平下的全模态训练，使JointDiT仅通过控制各分支的时间步，即可自然处理多种组合生成任务，包括联合生成、深度估计和深度条件图像生成。JointDiT展现出卓越的联合生成性能。此外，在深度估计和深度条件图像生成方面也取得了可比肩的结果，表明联合分布建模可以作为条件生成的有效替代方案。项目页面详见https://byungki-k.github.io/JointDiT/。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Eye2Eye: A Simple Approach for Monocular-to-Stereo Video Synthesis</p>
<p>沉浸式视觉体验的日益普及，推动了对立体3D视频生成技术的关注。尽管视频合成领域已取得重大进展，但由于3D视频数据的相对稀缺，创建3D视频仍面临挑战。我们提出了一种将文本到视频生成器转化为视频到立体生成器的简单方法。给定输入视频，我们的框架能自动生成视点偏移后的视频帧，从而实现引人入胜的3D效果。现有及同期方法通常采用多阶段处理流程：首先估计视频视差或深度，随后对视频进行形变处理以生成第二视角，最后对暴露区域进行修复。当场景涉及镜面反射表面或透明物体时，这种方法的固有缺陷就会显现。在此类情况下，单层视差估计无法满足需求，导致形变过程中出现伪影和像素位移错误。我们的研究通过直接合成新视点突破了这些限制，完全规避了中间处理步骤。这一突破的实现得益于对预训练视频模型在几何结构、物体材质、光学特性和语义理解等方面先验知识的有效利用，而无需依赖外部几何模型或人为解耦合成过程中的几何要素。我们在包含多种物体材质与复杂组合的真实场景中验证了该方法的优势。视频演示请访问：https://video-eye2eye.github.io</p>
<div style="break-before: page; page-break-before: always;"></div><p>T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation</p>
<p>文生视频生成模型近年来取得了显著进展，能够生成高质量视频，在视觉美感和指令遵循准确性方面表现优异，已成为数字艺术创作和在线用户互动的核心工具。然而，尽管技术进步明显，这些模型对基本物理定律的遵循能力仍未得到充分验证：大量生成结果仍存在违反刚体碰撞、能量守恒和引力动力学等基本物理约束的现象，导致内容缺乏真实感甚至产生误导。现有物理评估基准通常依赖基于简单生活场景提示的自动像素级指标，因而忽视了人类判断和第一性原理物理规律。为填补这一空白，我们提出\textbf{T2VPhysBench} —— 首个基于第一性原理的评估基准，系统检验开源与商业领域最先进的文生视频系统是否遵循十二项核心物理定律，涵盖牛顿力学、守恒定律和现象学效应等维度。本基准采用严谨的人类评估协议，包含三项针对性研究：(1) 整体合规性评估显示，所有模型在每类物理定律上的平均得分均低于0.60；(2) 提示词消融实验表明，即使添加详尽的定律特化提示仍无法修正物理违规；(3) 反事实稳健性测试揭示，当明确要求时，模型常生成明显违反物理规则的视频。研究结果揭示了当前架构的持续缺陷，并为引导未来研究走向真正具备物理感知能力的视频生成提供了具体洞见。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Direct Motion Models for Assessing Generated Videos</p>
<p>当前视频生成模型的一个主要局限在于，它们虽然能生成看似逼真的帧画面，但运动效果却十分拙劣——这一问题无法被FVD等主流生成视频评估方法有效捕捉。本研究通过开发一种能更好衡量合理物体交互与运动质量的指标，实现了对FVD的突破性改进。我们的创新方法基于点轨迹自动编码技术，提取的运动特征不仅可用于视频分布的对比(少至单个生成视频与真实视频，多至两个数据集)，还能评估单个视频的运动质量。实验证明，相较于像素重建或动作识别特征，采用点轨迹特征构建的指标对合成数据中的时序失真表现出显著更高的敏感性，且在预测开源模型生成视频的时间连贯性与真实感方面，其人类评估结果优于多种替代方法。此外，点轨迹表征技术使我们能在时空维度精准定位生成视频中的不一致性，相比先前研究，为生成视频错误提供了更强的可解释性。完整结果概览及代码链接请访问项目页面：http://trajan-paper.github.io。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Real-Time Animatable 2DGS-Avatars with Detail Enhancement from Monocular Videos</p>
<p>从单目视频中重建高质量、可动画化的三维人体化身，在降低对复杂硬件依赖方面展现出巨大潜力，这使其在游戏开发、增强现实和社交媒体应用中极具实用价值。然而，现有方法在捕捉精细几何细节和保持动画稳定性方面仍面临重大挑战，特别是在动态或复杂姿态下表现尤为明显。为解决这些问题，我们提出了一种基于二维高斯泼溅(2DGS)的新型实时可动画人体化身重建框架。通过结合2DGS与全局SMPL姿态参数，我们的框架不仅能够校正位置和旋转偏差，还能实现重建化身的鲁棒且自然的姿态驱动动画。此外，我们创新性地引入了旋转补偿网络(RCN)，该网络通过融合局部几何特征与全局姿态参数来学习旋转残差。该网络显著改善了非刚性形变的处理能力，并确保动画过程中姿态转换的平滑性和无伪影特性。实验结果表明，我们的方法成功实现了从单目视频重建逼真且高度可动画的人体化身，在保持精细细节的同时确保姿态变化的稳定性和自然性。在公共基准测试中，本方法在重建质量和动画鲁棒性方面均超越了当前最先进的方法。</p>
<div style="break-before: page; page-break-before: always;"></div><p>CoCoDiff: Diversifying Skeleton Action Features via Coarse-Fine Text-Co-Guided Latent Diffusion</p>
<p>在动作识别任务中，特征多样性对提升模型泛化能力和性能至关重要。现有方法通常通过在样本空间中扩充训练数据来增强特征多样性，但这种方式往往效率低下且存在语义不一致问题。为克服这些局限，我们提出了一种新颖的粗细粒度文本协同引导扩散模型(CoCoDiff)。该模型通过扩散过程和多粒度文本引导，在潜在空间中生成多样化且语义一致的特征表示。具体而言，我们的方法将骨骼序列提取的时空特征输入潜在扩散模型，生成多样化的动作表征。同时，我们创新性地引入粗细粒度文本协同引导策略，利用大语言模型(LLMs)提供的文本信息，确保生成特征与原始输入的语义一致性。值得注意的是，CoCoDiff在训练过程中作为即插即用辅助模块运行，无需增加推理阶段的计算开销。大量实验表明，CoCoDiff在基于骨骼数据的动作识别基准测试(包括NTU RGB+D、NTU RGB+D 120和Kinetics-Skeleton)中取得了最先进的性能表现。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Common3D: Self-Supervised Learning of 3D Morphable Models for Common Objects in Neural Feature Space</p>
<p>三维可变形模型(3DMMs)是一种强大的工具，能够表示物体类别的潜在形状与外观。给定单张测试图像时，3DMMs可用于解决多种任务，例如预测物体的三维形状、姿态、语义对应关系及实例分割。然而，目前3DMMs仅适用于极少数特定关注的对象类别(如人脸或人体)，因为它们需要复杂的3D数据采集和针对特定类别的训练流程。相比之下，我们提出了一种新方法Common3D——通过从以物体为中心的视频集合中完全自监督的方式学习常见物体的3D可变形模型。</p>
<p>为实现这一目标，我们的模型将物体表征为一个学习得到的3D模板网格和一个以图像为条件的神经网络参数化的变形场。与先前工作不同，Common3D使用神经特征而非RGB颜色描述物体外观，这种对像素强度的抽象使学习到的表征具有更强的泛化性。值得注意的是，我们通过利用可变形模板网格定义的对应关系，采用对比目标训练外观特征。相较于相关研究，该方法产生了更高质量的对应特征，并显著提升了模型在估计三维物体姿态和语义对应任务中的性能。Common3D是首个完全自监督且能以零样本方式解决多种视觉任务的方法。</p>
<div style="break-before: page; page-break-before: always;"></div><p>HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation</p>
<p>扩散模型的快速发展为VR和AR技术的应用革新带来曙光，这类技术通常需要场景级4D资产来支撑用户体验。然而，现有扩散模型主要聚焦于静态3D场景或物体级动态建模，限制了其提供真正沉浸式体验的能力。为解决这一问题，我们提出HoloTime框架：通过整合视频扩散模型实现单提示词或参考图像生成全景视频，并开发360度4D场景重建方法，将生成的全景视频无缝转化为4D资产，为用户创造完全沉浸的4D体验。具体而言，为驯化视频扩散模型生成高保真全景视频，我们构建了首个适用于下游4D场景重建任务的全景视频数据集360World。基于此数据集，我们提出全景动画生成器 —— 一种两阶段图像到视频扩散模型，可将全景图像转化为高质量全景视频。随后开发的全景时空重建技术，通过时空深度估计方法将生成视频转化为4D点云，进而优化整体4D高斯溅射表征，重建时空一致的4D场景。通过与现有方法的对比实验验证，本方法在全景视频生成与4D场景重建方面均展现显著优势，证明其能创建更具吸引力和真实感的沉浸式环境，从而提升VR/AR应用中的用户体验。</p>
<div style="break-before: page; page-break-before: always;"></div><p>MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance</p>
<p>在本文中，我们提出了一种将三维人脸参数化模型融入隐式扩散框架的视频人脸驱动方法，旨在提升现有视频人脸生成方法中的形状一致性和运动控制能力。该方法采用FLAME（基于关节模型与表情学习的人脸模型）作为三维人脸参数化表征，为面部表情与头部姿态建模提供了统一框架，能够从驱动视频中精确提取细粒度的人脸几何与运动特征。具体而言，我们通过融合FLAME序列生成的深度图、法线贴图和渲染图，将丰富的三维表情与姿态细节信息注入隐式扩散模型。采用集成自注意力机制的多层面部运动融合模块，在空间域内结合身份特征与运动潜特征。借助三维人脸参数化模型的运动指导，我们的方法实现了参考图像与驱动视频捕获动作之间人脸身份的参数量化对齐。在基准数据集上的实验结果表明，该方法在高质量人脸动画生成方面具有显著优势，能够精准建模表情变化与头部姿态调整。此外，该方法在域外图像上展现出优秀的泛化性能。项目代码已开源：https://github.com/weimengting/MagicPortrait。</p>
<div style="break-before: page; page-break-before: always;"></div><p>ReVision: High-Quality, Low-Cost Video Generation with Explicit 3D Physics Modeling for Complex Motion and Interaction</p>
<p>近年来，视频生成技术取得了显著进展，但在生成复杂运动和交互方面仍存在挑战。为此，我们提出了ReVision框架——一种即插即用的解决方案，通过将参数化的3D物理知识显式整合到预训练条件视频生成模型中，显著提升了生成具有复杂运动与交互的高质量视频的能力。具体而言，ReVision包含三个阶段：首先使用视频扩散模型生成粗粒度视频；接着从粗粒度视频中提取2D和3D特征集构建以物体为中心的3D表征，并通过我们提出的参数化物理先验模型进行优化，生成精确的3D运动序列；最后将优化后的运动序列作为附加条件反馈至原视频扩散模型，使其即使在涉及复杂动作与交互的场景下，也能生成运动一致的视频。我们在Stable Video Diffusion平台上验证了该方法的有效性，ReVision显著提升了运动保真度与连贯性。值得注意的是，该框架仅需15亿参数，在复杂视频生成任务上的表现即大幅超越拥有130亿参数的业界领先视频生成模型。实验结果表明，通过融入3D物理知识，即使相对较小的视频扩散模型也能以更高的真实性与可控性生成复杂运动和交互，为物理合理的视频生成提供了创新解决方案。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Efficient Listener: Dyadic Facial Motion Synthesis via Action Diffusion</p>
<p>在双向对话中生成逼真的听者面部动作仍然具有挑战性，这主要源于高维动作空间和时序依赖性的要求。现有方法通常考虑提取3D可变形模型(3DMM)系数并在3DMM空间进行建模。然而，这种做法使得3DMM的计算速度成为性能瓶颈，难以实现实时交互响应。为解决这一问题，我们提出了面部动作扩散方法(Facial Action Diffusion, FAD)，该方法将图像生成领域的扩散方法引入到面部动作生成中，以实现高效的面部动作生成。我们进一步构建了专为接收说话者视觉和听觉信息的双模态输入而设计的高效听者网络(Efficient Listener Network, ELNet)。结合FAD和ELNet，所提出的方法能有效学习听者面部运动表征，在性能上超越了现有最优方法的同时，将计算时间减少了99%。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting </p>
<p>个性化3D数字人编辑技术因其用户友好性以及在AR/VR和虚拟试穿等领域的应用潜力而备受关注。现有研究虽然探索了3D编辑的可行性，但受限于复杂重建场景中几何与纹理混合优化的不稳定性表征学习，往往难以生成理想的视觉效果。本文致力于为普通用户提供便捷的解决方案，使其能够创建具备精准区域定位、几何自适应性及高保真渲染效果的可编辑3D数字人。为此，我们提出了一种创新框架，通过混合式四面体约束高斯溅射(TetGS)作为底层表征，将编辑过程解耦为局部空间适配与真实感外观学习两个维度。该TetGS技术融合了四面体网格的可控显式结构与3D高斯溅射的高精度渲染优势，并采用分阶段优化策略：首先基于单目视频实现3D数字人实例化以获取TetGS初始化先验；其次通过显式划分的四面体引导高斯核重分布的局部空间适配；最终采用由粗到细的激活策略实现基于几何的外观生成。定性与定量实验结果表明，我们的方法在生成可编辑高真实感3D数字人方面具有显著优势。</p>
<div style="break-before: page; page-break-before: always;"></div><p>SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from Gameplay Recordings</p>
<p>本论文介绍了SoccerDiffusion —— 一种基于Transformer架构的扩散模型，旨在直接从真实足球比赛录像中学习人形机器人足球的端到端控制策略。利用从RoboCup比赛收集的数据，该模型能够根据多模态传感器输入(包括视觉、本体感觉和比赛状态)预测关节控制轨迹。我们采用蒸馏技术将多步扩散过程简化为单步推理，从而实现在嵌入式平台上的实时运算。实验结果表明，该模型在仿真环境和实体机器人上均能复现行走、踢球、跌倒恢复等复杂运动行为。尽管在高层战术行为层面仍存在局限，但本研究为后续强化学习或偏好优化方法奠定了坚实基础。相关数据集、预训练模型及代码已发布于：<a href="https://bit-bots.github.io/SoccerDiffusion">https://bit-bots.github.io/SoccerDiffusion</a></p>
<div style="break-before: page; page-break-before: always;"></div><p>TesserAct: Learning 4D Embodied World Models</p>
<p>本文提出了一种有效的学习新型4D具身世界模型的方法。该模型能够预测3D场景随具身智能体动作产生的动态演变，同时保持时空一致性。我们提出通过RGB-DN(RGB、深度与法线)视频训练来学习4D世界模型，这种方法不仅通过将形状细节、配置状态和时间变化纳入预测而超越了传统2D模型，还能有效学习具身智能体的精确逆动态模型。具体而言，我们首先利用现成模型扩展现有机器人操作视频数据集，补充深度和法线信息；然后在此标注数据集上对视频生成模型进行微调，使其能够联合预测每帧的RGB-DN信息；最后提出一种算法，直接将生成的RGB、深度和法线视频转换为高质量的4D世界场景。该方法在具身场景的4D场景预测中确保了时间与空间一致性，实现了具身环境的新视角合成，并支持策略学习——实验表明由此获得的策略性能显著优于基于传统视频世界模型的方案。</p>
<div style="break-before: page; page-break-before: always;"></div><p>EfficientHuman: Efficient Training and Reconstruction of Moving Human using Articulated 2D Gaussian</p>
<p>三维高斯溅射(3D Gaussian Splatting, 3DGS)作为场景重建与新视角合成的开创性技术，近期在人体三维重建领域的应用尝试通过引入人体姿态先验信息来提升渲染质量与训练速度。然而，由于多视角不一致性和高斯冗余问题，现有方法在动态表面拟合方面仍面临挑战。这种不一致性源于高斯椭球体难以精确表征动态物体表面，从而阻碍了动态人体的快速重建。与此同时，高斯冗余的普遍存在意味着这些方法的训练耗时仍无法满足动态人体的快速拟合需求。针对上述问题，我们提出EfficientHuman模型——通过引入铰接式2D高斯面元(Articulated 2D Gaussian surfels)，在保证高质量渲染的同时快速实现动态人体重建。该模型的核心创新在于将高斯溅射体编码为规范空间下的铰接式2D高斯面元，并通过线性混合蒙皮(LBS)变换至姿态空间以实现高效姿态迁移。相较于传统3D高斯表征，铰接式2D高斯面元能够在保证视角一致几何表征的同时快速适配动态人体。此外，我们引入姿态校准模块和LBS优化模块以实现动态人体姿态的精准拟合，进一步提升模型性能。在ZJU-MoCap数据集上的大量实验表明，EfficientHuman平均仅需不到1分钟即可完成三维动态人体重建，较现有最优方法提速20秒，同时有效降低了高斯冗余数量。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Joint Optimization of Neural Radiance Fields and Continuous Camera Motion from a Monocular Video</p>
<p>神经辐射场(Neural Radiance Fields, NeRF)已展现出其在三维几何表征方面的卓越能力，但其训练过程依赖于预先精确计算的相机位姿。为了降低这一限制，现有方法通常通过联合优化相机位姿与NeRF来实现，但往往需要良好的位姿初始化或深度先验。然而，由于这些方法将每个相机映射至统一的世界坐标系，在面对大幅旋转等挑战性场景时表现受限。本文提出了一种创新方法，通过将连续相机运动建模为时间依赖的角速度与线速度，从而消除对先验条件的依赖。我们首先通过学习相机间的相对运动(通过速度积分实现)，随后通过聚合这些相对运动至视频某一时间步定义的世界坐标系中，最终获得相机位姿。具体而言，我们通过时间依赖的NeRF模型学习精确的连续相机运动——该模型通过在每个时间步利用相邻帧进行训练，以捕捉局部场景几何与运动特征。学习到的运动信息进一步用于微调NeRF，使其能够表征完整场景几何结构。在Co3D和Scannet数据集上的实验表明，相较于现有最优方法，我们的方法在相机位姿估计、深度估计精度方面表现更优，同时在新视角合成任务中达到可比性能。代码已开源：<a href="https://github.com/HoangChuongNguyen/cope-nerf">https://github.com/HoangChuongNguyen/cope-nerf</a>   </p>
<div style="break-before: page; page-break-before: always;"></div><p>Sketch2Anim: Towards Transferring Sketch Storyboards into 3D Animation</p>
<p>分镜脚本（故事板）被广泛应用于三维动画创作。动画师通常以分镜中的二维草图为参考，通过反复试错来制作理想的三维动画。传统方法需要极高的专业素养，且费时费力。因此，业界迫切需要能够直接将二维故事板草图转化为三维动画的自动化方法。该任务目前尚未得到充分探索，受运动扩散模型重大进展的启发，我们尝试从条件运动合成的角度切入这一领域。为此，我们提出了Sketch2Anim系统，其核心由草图约束理解与运动生成两大模块构成。具体而言，鉴于二维草图与三维运动之间存在显著的领域差异，我们并未直接以二维输入为条件，而是设计了一个三维条件运动生成器，通过同时利用三维关键姿势、关节轨迹和动作描述词，实现精准细粒度的运动控制。此外，我们创新性地开发了神经映射器，专门用于将用户提供的二维草图与其对应的三维关键姿势及轨迹在共享嵌入空间中对齐，从而首次实现了对运动生成的直接二维控制。得益于多条件运动生成器的灵活性，我们的方法不仅能将分镜脚本成功转化为高质量三维运动，还天然支持直接的三维动画编辑。通过全面的实验评估以及用户感知研究，我们验证了该方法的有效性。</p>
<div style="break-before: page; page-break-before: always;"></div><p>HumMorph: Generalized Dynamic Human Neural Fields from Few Views</p>
<p>我们推出了HumMorph——一种具有显式姿态控制能力的动态人体自由视角渲染新型通用方法。该方法能够在给定任意姿态下少量观测视角（最低仅需一个）的条件下，渲染出目标人物处于任意指定姿态的图像。通过完全依赖模型的前馈推理过程，我们的方法实现了快速生成。首先，我们在标准T姿态下构建人物的粗略表征，该表征通过融合来自不同局部观测视角的视觉特征，并利用学习获得的先验知识填补缺失信息。为了增强细节表现，我们进一步从原始观测视图中直接提取细粒度的像素对齐特征，以提供高分辨率的外观信息。实验表明，当仅使用单视角输入时，HumMorph已具备与现有最佳方法竞争的实力；而在仅需两个单目观测的条件下，我们的方法能实现视觉效果显著提升的渲染效果。值得注意的是，此前通用方法通常依赖于通过多相机同步系统获取精确的人体形态与姿态参数。相较之下，我们着眼于更实际的应用场景——直接从观测视图中进行存在噪声的体态参数估计。实验结果表明，我们的架构对参数噪声具有更强的鲁棒性，在该设定下明显超越了现有技术标杆。</p>
<div style="break-before: page; page-break-before: always;"></div><p>AnimateAnywhere: Rouse the Background in Human Image Animation</p>
<p>人体图像动画生成旨在根据给定的人物角色和背景生成符合指定姿态序列的人类视频。然而，现有方法更多关注人物动作而忽视了背景的动态生成，这通常会导致静态结果或背景运动不协调的问题。学界虽已探索过相机位姿引导的动画生成任务，但为大多数娱乐应用和普通用户准备相机运动轨迹并不现实。为此，我们提出了AnimateAnywhere框架，无需依赖相机轨迹即可在人体动画中激发背景动态。基于&quot;人体运动往往反映背景动态&quot;这一关键发现，我们设计了背景运动学习器（BML），从人体姿态序列中学习背景运动规律。为了增强模型对跨帧对应关系的精确学习，我们进一步在三维注意力图上施加极线约束：通过将极线掩膜与当前三维注意力图相结合，精心构建出抑制几何不合理注意力的掩膜系统。大量实验证明，我们的AnimateAnywhere能有效从人体姿态序列中学习背景运动规律，在生成具有生动逼真背景的人类动画效果方面达到业界领先水平。源代码和模型将在<a href="https://github.com/liuxiaoyu1104/AnimateAnywhere">https://github.com/liuxiaoyu1104/AnimateAnywhere</a> 发布。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Generative AI for Character Animation: A Comprehensive Survey of Techniques, Applications, and Future Directions</p>
<p>生成式人工智能正在重塑艺术、游戏以及最引人注目的动画领域。基础模型和扩散模型的最新突破，显著降低了动画内容的制作时间和成本。作为动画的核心要素，角色涉及动作、情绪、手势和面部表情等多个维度。近月来该领域技术发展的速度与广度令人应接不暇，亟需一份整合性综述来梳理脉络。不同于早期仅孤立探讨虚拟化身、手势或面部动画的综述，本次调研首次以统一视角全面解析角色动画中所有主要生成式AI应用。</p>
<p>我们首先系统考察了以下方向的最新技术进展：面部动画、表情渲染、图像合成、虚拟化身创建、手势建模、动作合成、物体生成以及纹理合成。针对每个领域，我们重点梳理了前沿研究、实际应用、常用数据集及新兴趋势。为帮助新入行者，我们还设置了完整的背景知识章节，深入解读基础模型和评估指标，为读者奠定领域知识基础。通过剖析开放挑战并规划未来研究方向，我们绘制了推进AI驱动角色动画技术的发展蓝图。</p>
<p>本调研旨在为进入生成式AI动画及其相关领域的研究者与开发者提供知识图谱。相关资源已整合于：<a href="https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey">https://github.com/llm-lab-org/Generative-AI-for-Character-Animation-Survey</a></p>
<div style="break-before: page; page-break-before: always;"></div><p>ActionArt: Advancing Multimodal Large Models for Fine-Grained Human-Centric Video Understanding</p>
<p>对视频中人类动作和姿态的细粒度理解对于以人为中心的人工智能应用至关重要。本研究提出ActionArt数据集——一个旨在推动以人为中心的多模态理解研究的细粒度视频描述数据集。我们的数据集包含数千个涵盖广泛人类动作、人-物交互和多样化场景的视频片段，每个视频均配有详细标注，精确记录了每个肢体动作的细微变化。我们设计了八项子任务来评估现有大型多模态模型在不同维度的细粒度理解能力。实验结果表明，虽然当前主流大型多模态模型在多项任务上表现优异，但在细粒度理解层面仍存在明显不足。我们分析这种局限主要源于高质量标注数据的稀缺性，这类数据的标注既成本高昂又难以进行规模化人工标注。鉴于人工标注的高成本和规模化难题，我们提出通过代理任务来增强模型在时空维度上的感知能力。这些代理任务经过精心设计，可以通过现有MLLMs自动生成的数据进行驱动，从而降低对昂贵人工标注的依赖。实验结果显示，所提出的代理任务显著缩小了与人工标注细粒度数据训练的性能差距。</p>
<div style="break-before: page; page-break-before: always;"></div><p>SmallGS: Gaussian Splatting-based Camera Pose Estimation for Small-Baseline Videos</p>
<p>小基线运动的动态视频在日常生活中无处不在，尤其是在社交媒体上。然而，由于特征模糊、漂移累积以及三角测量约束不足，这些视频对现有的姿态估计框架提出了挑战。高斯泼溅(Gaussian splatting)通过对场景保持显式表示，在视角变化较小时能够提供可靠的新视图栅格化。受此启发，我们提出了SmallGS，这是一个专为小基线视频设计的相机姿态估计框架。该方法利用高斯泼溅优化序列相机位姿：首先在每个视频片段的首帧重建场景，为后续帧提供稳定参考。高斯泼溅在有限视角差异下的时间一致性特性，降低了传统相机姿态估计对深度剧烈变化的依赖。我们进一步将预训练的鲁棒视觉特征(如DINOv2)融入高斯泼溅，通过高维特征图渲染增强相机姿态估计的鲁棒性。通过冻结高斯泼溅模型并基于栅格化特征优化相机视角，SmallGS无需显式特征对应或强视差运动即可有效学习相机位姿。我们在TUM-Dynamics序列的小基线视频中验证了该方法的有效性：相较于MonST3R和DORID-SLAM，SmallGS在动态场景的小基线视频中实现了更精确的相机姿态估计。项目主页详见：<a href="https://yuxinyao620.github.io/SmallGS">https://yuxinyao620.github.io/SmallGS</a></p>
<div style="break-before: page; page-break-before: always;"></div><p>STP4D: Spatio-Temporal-Prompt Consistent Modeling for Text-to-4D Gaussian Splatting</p>
<p>文本到4D生成技术正在快速发展并广泛应用于多种场景。然而，现有方法往往难以在统一框架内实现充分的时空建模与提示对齐，导致生成内容存在时序不一致、几何畸变或质量低下等问题，且结果常偏离输入文本描述。为此，我们提出创新性方法STP4D，通过整合全面的时空-提示一致性建模，致力于实现高质量的文本到4D生成。</p>
<p>该方法特别设计了三个协同工作的核心模块：时变提示嵌入模块通过动态调整文本语义表达增强时序一致性，几何信息增强模块利用多尺度特征保持几何结构完整性，时序扩展形变模块则通过物理模拟优化动态形变过程。此外，STP4D率先将扩散模型与4D高斯表征（4DGS）相结合，既保留了4DGS的细粒度建模能力和实时渲染优势，又继承了扩散模型的快速推理特性（单个资产生成仅需约4.6秒）。</p>
<p>实验验证表明，STP4D生成的4D内容在保真度方面显著优于现有方法，同时保持着极高的生成效率，在质量和速度两个维度均实现突破。这项技术为影视特效、虚拟现实等领域的动态三维内容创作提供了新的解决方案。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Unify3D: An Augmented Holistic End-to-end Monocular 3D Human Reconstruction via Anatomy Shaping and Twins Negotiating</p>
<p>单目三维穿衣人体重建旨在从单张图像中生成完整的三维数字人。为解决单张 RGB 图像中人体几何信息缺失的问题，现有方法通常依赖前置模型来获取显式的几何表征。在重建过程中，研究重点往往同时放在模型本身和输入图像上。这种范式受限于前置模型，且忽视了重建任务的内在完整性。针对这一局限，本文提出了一种创新范式，将人体重建视为整体过程，采用端到端网络直接从二维图像预测三维数字人，摒弃任何显式的中间几何表征。基于此，我们进一步提出了一个包含两大核心模块的新型重建框架：解剖形态提取模块（ASE），其能捕捉考虑人体解剖学特性的隐式形态特征；双胞胎协商重建 U-Net（TNRU），通过跨模态双U-Net的特征交互提升重建效果。此外，我们提出了卡通数据增强策略并构建了包含15,000 + 三维人体扫描的数据集，以增强模型在复杂案例输入下的性能。通过在两个测试集及大量真实场景案例上的广泛实验，验证了本方法相较于最先进技术（SOTA）的优越性。项目演示详见：<a href="https://e2e3dgsrecon.github.io/e2e3dgsrecon/%E3%80%82">https://e2e3dgsrecon.github.io/e2e3dgsrecon/。</a></p>
<div style="break-before: page; page-break-before: always;"></div><p>SSD-Poser: Avatar Pose Estimation with State Space Duality from Sparse Observations</p>
<p>随着AR/VR应用场景的不断拓展，头戴式显示器（HMD）对实时全身姿态估计的需求日益增长。尽管HMD能够提供头部和手部的关节信号，但由于下半身自由度较高，全身姿态重建仍面临挑战。近年来，学界常采用传统神经网络与生成式模型（如Transformer和扩散模型）来提升该任务的性能表现。然而，这些方法在实现精确姿态重建与保持快速推理速度之间难以达到平衡。为此，我们设计了轻量高效的SSD-Poser模型，旨在通过稀疏观测实现稳健的全身运动估计。该模型通过精心设计的混合编码器——状态空间注意力编码器，将状态空间对偶性适配复杂运动姿态，从而实现实时逼真的姿态重建。此外，模型引入频率感知解码器以缓解变频运动信号引起的抖动现象，显著提升了运动流畅度。在AMASS数据集上的综合实验表明，SSD-Poser在计算效率与精度方面均达到优异水平，相比现有最优方法展现出突出的推理效能。</p>
<div style="break-before: page; page-break-before: always;"></div><p>PIN-WM: Learning Physics-INformed World Models for Non-Prehensile Manipulation</p>
<p>尽管非抓取式操作（如受控推挤/戳动）是机器人的基础技能，但由于其对涉及摩擦与恢复力的复杂物理交互具有高度敏感性，其学习过程仍面临挑战。为实现稳健的策略学习与泛化能力，我们选择学习非抓取操作中三维刚体动力学的世界模型，并采用基于模型的强化学习方法。本文提出物理信息世界模型 PIN-WM，该模型能够从视觉观测数据中实现三维刚体动力系统的端到端高效辨识。通过采用可微分物理仿真技术，PIN-WM 仅需少量与任务无关的物理交互轨迹即可完成学习。此外，该模型通过高斯溅射法产生的观测损失进行训练，无需依赖状态估计。为弥合仿真与现实差距，我们通过物理感知随机化技术将学习到的 PIN-WM 转化为一组数字表亲——通过扰动物理参数与渲染参数，生成 PIN-WM 的多样化有意义变体。大量仿真与真实世界测试表明，经物理感知数字表亲增强的PIN-WM能够促进具有仿真-现实迁移能力的稳健非抓取操作技能学习，其性能超越当前最先进的 Real2Sim2Real 方法。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Gaussian Splatting is an Effective Data Generator for 3D Object Detection</p>
<p>我们研究了自动驾驶中 3D 物体检测的数据增强方法。利用基于高斯溅射的 3D 重建技术最新进展，我们在驾驶场景中实现了 3D 物体的精准放置。与现有基于扩散的方法（通过鸟瞰图布局条件生成图像）不同，我们的方法直接将 3D 物体置入重建的三维空间，并显式施加几何变换。这既保证了物体放置的物理合理性，又提供了高精度的 3D 姿态与位置标注。实验表明，即使仅在真实场景中集成有限数量的外部 3D 物体，增强后的数据也能显著提升3D检测性能，并优于现有基于扩散的 3D 数据增强方法。在 nuScenes 数据集上的大量测试表明，相较于物体外观多样性，施加高几何多样性的物体布局对性能提升更为关键。此外，我们发现通过最大化检测损失或施加高视觉遮挡生成的困难样本，并不能有效提升基于摄像头的自动驾驶 3D 物体检测性能。  </p>
<div style="break-before: page; page-break-before: always;"></div><p>HUG: Hierarchical Urban Gaussian Splatting with Block-Based Reconstruction</p>
<p>随着城市三维场景日益复杂化及高质量渲染需求的提升，高效的场景重建与渲染技术变得至关重要。本文提出 HUG 方法 —— 一种基于 3D 高斯溅射的新型解决方案，旨在解决大规模城市场景处理和复杂细节渲染中的效率瓶颈。该方法通过引入分层神经高斯表示，对数据分区和重建流程进行系统性优化。我们采用改进的基于分块的重建流程，重点提升单个分块内的重建质量，同时减少对分块边界冗余训练区域的依赖。通过将神经高斯表示与分层架构相结合，本方法以较低计算成本实现了高质量场景渲染。我们在公开基准测试中取得了最先进的结果，充分证明该方法在大规模城市场景表征方面的有效性和优势。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Physically Consistent Humanoid Loco-Manipulation using Latent Diffusion Models</p>
<p>本文利用潜在扩散模型（Latent Diffusion Models, LDMs）的能力生成逼真的RGB人-物交互场景，以指导人形机器人的移动操作规划。具体而言，我们从生成图像中提取接触位置和机器人配置信息，并将其应用于全身轨迹优化（Whole-Body Trajectory Optimization, TO）框架，从而为人形机器人生成符合物理规律的运动轨迹。我们通过仿真在多种长周期移动操作场景下验证了完整流程，并对提出的接触点与机器人配置提取方法进行了深入分析。实验结果表明，利用从LDMs提取的信息，我们能够生成需要长期推理且符合物理约束的运动轨迹。</p>
<div style="break-before: page; page-break-before: always;"></div><p>PMG: Progressive Motion Generation via Sparse Anchor Postures Curriculum Learning</p>
<p>在计算机动画、游戏设计以及人机交互领域，如何生成符合用户意图的人体运动始终是一项重大挑战。现有方法存在显著局限性：基于文本描述的方法能够提供高层语义指导，但难以准确描述复杂动作；基于轨迹的技术虽然能直观控制全局运动方向，却常无法生成精确或定制化的角色动作；而锚点姿态引导的方法通常仅限于生成简单运动模式。</p>
<p>为实现更高控制精度和更精细的运动生成，我们提出<strong>ProMoGen（渐进式运动生成框架）</strong> —— 一种将轨迹引导与稀疏锚点动作控制相结合的全新框架。全局轨迹保证了空间方向与位移的连贯性，而稀疏锚点动作仅传递精确的动作指导（不含位移信息）。这种解耦设计使得两者能够独立优化，从而实现更高可控性、高保真度与复杂运动合成。ProMoGen在统一训练流程中支持双控制范式（轨迹+锚点）与单控制范式（仅轨迹或仅锚点）的灵活切换。</p>
<p>此外，针对稀疏动作直接学习存在固有不稳定性的问题，我们提出<strong>SAP-CL（稀疏锚姿态课程学习策略）</strong>。该课程学习方法通过渐进式调整引导锚点数量，实现了更精准稳定的收敛效果。大量实验表明，ProMoGen在预定义轨迹与任意锚点帧的引导下，能够生成生动多样的运动。我们的方法将个性化动作与结构化引导无缝融合，在多种控制场景下显著优于当前最先进的方法。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Unveiling Hidden Vulnerabilities in Digital Human Generation via Adversarial Attacks</p>
<p>富有表现力的人体姿态与形状估计（EHPS）对于数字人生成至关重要，尤其在直播等实时应用中。尽管现有研究主要集中于降低估计误差，但大多忽视了鲁棒性与安全性问题，导致这些系统容易受到对抗攻击。为应对这一重大挑战，我们提出了<strong>实体化攻击 (TBA)</strong> ——一个创新框架，旨在生成能有效攻破任何数字人生成模型的对抗样本。该方法通过<strong>双重异构噪声生成器 (DHNG)</strong>，结合变分自编码器（VAE）与 ControlNet 技术，根据原始图像特征生成多样化、目标导向的噪声。此外，我们设计了定制的<strong>对抗性损失函数</strong>来优化噪声，确保其兼具高可控性与强破坏性。通过从噪声和最先进的EHPS模型中获取多梯度信号进行迭代优化，TBA显著提升了对抗样本的攻击效能。大量实验证明TBA的优越性：在估计误差方面实现了41.0% 的显著增长，平均提升约 17.0% 。这些发现揭示了当前 EHPS 模型存在的重大安全漏洞，并凸显了在数字人生成系统中加强防御的迫切需求。   </p>
<div style="break-before: page; page-break-before: always;"></div><p>PICO: Reconstructing 3D People In Contact with Objects</p>
<p>从单张彩色图像中恢复 3D 人物-物体交互 (HOI) 面临多重挑战，包括深度模糊性、遮挡问题，以及物体形状和外观的巨大多样性。因此，以往研究需要依赖已知物体形状和接触点等受控条件，且仅能处理有限类别的物体。为应对真实场景的需求，我们需要能泛化到自然图像和新型物体类别的方法。本研究通过两大创新实现突破：</p>
<p>(1) 我们构建了 <strong>PICO-db</strong> 新型数据集，其独特之处在于将自然图像与人体和物体网格上的密集3D接触点进行配对。基于DAMON数据集(已包含标准3D人体接触标注)，我们通过以下方式实现双重接触标注：利用视觉基础模型从数据库检索适配的3D物体网格，并创新性地通过&quot;每接触区域仅需两次点击&quot;的投影方法，将 DAMON 的人体接触区域映射到物体表面。这种极简人工标注方式实现了人体与物体间丰富的接触对应关系。</p>
<p>(2) 我们提出 <strong>PICO-fit</strong> 渲染比较拟合方法，利用新型接触对应数据集重建交互中的 3D 人体和物体网格。该方法的工作流程包括：推断 SMPL-X 人体的接触信息 → 从 PICO-db 检索匹配的物体网格及接触数据 → 通过优化迭代拟合三维模型使其符合图像证据。PICO-fit 的突破性在于能处理现有方法无法应对的多种物体类别，这对实现真实场景的大规模HOI理解至关重要。</p>
<p>项目数据与代码已开源：<a href="https://pico.is.tue.mpg.de">https://pico.is.tue.mpg.de</a></p>
<div style="break-before: page; page-break-before: always;"></div><p>Bolt: Clothing Virtual Characters at Scale</p>
<p>为虚拟角色着装是一个耗时且通常需要手动操作的过程。一套服装可能由多件服饰组成，每件服饰都必须适配角色的独特体型。由于角色的体型差异可能非常显著，为众多角色适配服装会面临组合爆炸式增长的难题。我们推出了 Bolt 系统，该系统的设计目标是通过转移、悬垂和绑定三阶段流程，将原本在源身体模型上制作的服装适配到新体型上。首先，我们创新的服饰转移方法将每件服饰的三维网格位置转换到目标角色，同时优化服饰的二维缝纫图案，保留原始缝合线与边界的关键特征。其次，系统通过物理模拟逐步悬垂并解缠服装中的每件服饰。最后，所有服饰将被绑定到新角色骨骼上。整个流程完全自动化，无需人工干预即可实现大规模角色着装。完成着装的角色可立即应用于游戏、动画、数字人合成等多种场景。</p>
<div style="break-before: page; page-break-before: always;"></div><p>Dynamic Camera Poses and Where to Find Them</p>
<p>大规模标注动态互联网视频中的相机位姿对于推动逼真视频生成与仿真等领域的发展至关重要。然而，此类数据集的构建面临双重挑战：一方面，多数网络视频因拍摄条件限制并不适用于位姿估计；另一方面，即便采用最先进的算法，动态网络视频的位姿标注仍存在显著技术瓶颈。为此，本文提出 DynPose-100K —— 一个标注了精确相机位姿的大规模动态网络视频数据集。我们的数据采集框架通过精心整合任务专用模型与通用模型的级联筛选策略实现高效数据过滤。在位姿估计环节，我们创新性地融合了点追踪、动态遮罩和运动恢复结构 (SfM) 等前沿技术，显著提升了现有方法的精度。实验分析表明，DynPose-100K 不仅在规模上达到十万级别，更在关键属性维度展现出丰富的多样性，为各类下游应用的突破性进展提供了坚实基础。</p>
<div style="break-before: page; page-break-before: always;"></div><p>3DV-TON: Textured 3D-Guided Consistent Video Try-on via Diffusion Models</p>
<p>视频虚拟试穿技术旨在将视频中的服装替换为目标服饰。现有方法在处理复杂服装图案和多样化人体姿态时，往往难以生成高质量且时间一致的结果。我们提出 3DV-TON —— 一个基于扩散模型的新型框架，能够生成高保真且时序一致的视频试穿效果。该方法的创新之处在于采用可动画化的带纹理3D网格作为显式的帧级引导，通过直接参照视频序列中始终如一的服装纹理运动，有效解决了现有模型过度追求外观保真度而牺牲运动连贯性的问题。该框架包含自适应的动态 3D 引导生成流程：<br />
(1) 选取关键帧进行初始2D图像试穿；<br />
(2) 重建与原始视频姿态同步的带纹理3D网格并进行动画化处理。我们进一步提出鲁棒的矩形掩码策略，成功缓解了人体动态运动和服装位移过程中因服装信息泄露导致的伪影传播问题。为推动视频试穿领域研究，我们构建了 HR-VVT 高清基准数据集，包含 130 段涵盖多种服装类型和场景的视频。定量与定性实验均证明我们的方法显著优于现有技术。</p>
<p>项目主页详见：<a href="https://2y7c3.github.io/3DV-TON/">https://2y7c3.github.io/3DV-TON/</a></p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
            </nav>

        </div>

        <script type="text/javascript">
            window.playground_copyable = true;
        </script>
        <script src="elasticlunr.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="mark.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="searcher.js" type="text/javascript" charset="utf-8"></script>
        <script src="clipboard.min.js" type="text/javascript" charset="utf-8"></script>
        <script src="highlight.js" type="text/javascript" charset="utf-8"></script>
        <script src="book.js" type="text/javascript" charset="utf-8"></script>

        <!-- Custom JS scripts -->
        <script type="text/javascript" src="theme/pagetoc.js"></script>
        <script type="text/javascript">
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>
    </body>
</html>
